{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0d140e-d46d-455c-a828-bd36e21fbaf9",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Modell "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534403f2-c4a3-4b8e-8810-837c585e13ea",
   "metadata": {},
   "source": [
    "# Godot Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a907490-767a-4d9a-bc88-ef93e5b24bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 16 environments.\n",
      "Expecting Godot instances on ports: [5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015]\n",
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     80\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing device:\u001b[39m\u001b[33m\"\u001b[39m, device)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m q_net = \u001b[43mQNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSTATE_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_ACTIONS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m optimizer = optim.Adam(q_net.parameters(), lr=LEARNING_RATE)\n\u001b[32m     85\u001b[39m loss_fn = nn.MSELoss()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/torch/nn/modules/module.py:1371\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1369\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/torch/nn/modules/module.py:957\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/torch/nn/modules/module.py:1357\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1351\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1352\u001b[39m             device,\n\u001b[32m   1353\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1354\u001b[39m             non_blocking,\n\u001b[32m   1355\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1356\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/torch/cuda/__init__.py:403\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    400\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    401\u001b[39m     )\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    407\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import json\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import random\n",
    "from collections import deque\n",
    "import os\n",
    "import select\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ===========================\n",
    "#      ENVIRONMENT CONFIG\n",
    "# ===========================\n",
    "\n",
    "HOST = \"127.0.0.1\"\n",
    "\n",
    "# How many parallel Godot environments to train with\n",
    "N_ENVS = 16 \n",
    "\n",
    "# Port base for Godot instances\n",
    "BASE_PORT = 5000\n",
    "\n",
    "# Generate ports based on environments\n",
    "ENV_PORTS = [BASE_PORT + i for i in range(N_ENVS)]\n",
    "\n",
    "print(f\"Training with {N_ENVS} environments.\")\n",
    "print(f\"Expecting Godot instances on ports: {ENV_PORTS}\")\n",
    "\n",
    "# ===========================\n",
    "#  RL / model config\n",
    "# ===========================\n",
    "\n",
    "# State vector: tank_x, tank_y, goal_x, goal_y, dx, dy  (all normalized)\n",
    "STATE_SIZE = 8\n",
    "\n",
    "# Discrete actions: (turn, throttle) with values in {-1, 0, 1}\n",
    "# Godot will feed these into setTurn() and setDirection().\n",
    "ACTIONS: List[Dict[str, float]] = [\n",
    "    {\"turn\": -1.0, \"throttle\":  1.0},  # left + forward\n",
    "    {\"turn\":  0.0, \"throttle\":  1.0},  # straight + forward\n",
    "    {\"turn\":  1.0, \"throttle\":  1.0},  # right + forward\n",
    "    {\"turn\":  0.0, \"throttle\":  0.0},  # stop\n",
    "    {\"turn\":  0.0, \"throttle\": -1.0},  # straight + backward\n",
    "]\n",
    "N_ACTIONS = len(ACTIONS)\n",
    "\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 0.2\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_SIZE = 10_000\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "CHECKPOINT_PATH = \"tank_dqn_checkpoint.pt\"\n",
    "LOG_PATH = \"training_log.csv\"\n",
    "\n",
    "# ===========================\n",
    "#  Q-network\n",
    "# ===========================\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)  # [batch, output_dim] (Q-values for each action)\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "q_net = QNetwork(STATE_SIZE, N_ACTIONS).to(device)\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "ReplayEntry = Tuple[List[float], int, float, List[float], bool]\n",
    "replay_buffer: deque[ReplayEntry] = deque(maxlen=REPLAY_SIZE)\n",
    "\n",
    "# Globals for RL bookkeeping (shared across envs)\n",
    "epsilon = EPSILON_START\n",
    "total_steps = 0\n",
    "\n",
    "# Per-env bookkeeping\n",
    "num_envs = len(ENV_PORTS)\n",
    "buffers: List[str] = [\"\"] * num_envs\n",
    "prev_states: List[List[float] | None] = [None] * num_envs\n",
    "prev_actions: List[int | None] = [None] * num_envs\n",
    "episode_returns: List[float] = [0.0] * num_envs\n",
    "episode_idxs: List[int] = [0] * num_envs\n",
    "\n",
    "decoder = json.JSONDecoder()\n",
    "\n",
    "# ===========================\n",
    "#  Checkpoint + logging\n",
    "# ===========================\n",
    "\n",
    "def save_checkpoint() -> None:\n",
    "    state = {\n",
    "        \"q_net\": q_net.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"epsilon\": epsilon,\n",
    "        \"total_steps\": total_steps,\n",
    "        \"episode_idxs\": episode_idxs,\n",
    "    }\n",
    "    torch.save(state, CHECKPOINT_PATH)\n",
    "    print(f\"[CHECKPOINT] Saved to {CHECKPOINT_PATH}\")\n",
    "\n",
    "\n",
    "def load_checkpoint() -> None:\n",
    "    global epsilon, total_steps, episode_idxs\n",
    "    if not os.path.exists(CHECKPOINT_PATH):\n",
    "        print(\"[CHECKPOINT] No checkpoint found, starting fresh.\")\n",
    "        return\n",
    "    state = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "    q_net.load_state_dict(state[\"q_net\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer\"])\n",
    "    epsilon = state.get(\"epsilon\", epsilon)\n",
    "    total_steps = state.get(\"total_steps\", 0)\n",
    "    episode_idxs = state.get(\"episode_idxs\", episode_idxs)\n",
    "    print(f\"[CHECKPOINT] Loaded from {CHECKPOINT_PATH}, steps={total_steps}\")\n",
    "\n",
    "\n",
    "def log_episode_to_file(env_idx: int, ep_idx: int, ep_return: float, eps: float) -> None:\n",
    "    header_needed = not os.path.exists(LOG_PATH)\n",
    "    with open(LOG_PATH, \"a\") as f:\n",
    "        if header_needed:\n",
    "            f.write(\"env,episode,return,epsilon\\n\")\n",
    "        f.write(f\"{env_idx},{ep_idx},{ep_return},{eps}\\n\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Networking helpers\n",
    "# ===========================\n",
    "\n",
    "def send_json(sock: socket.socket, payload: Dict[str, Any]) -> None:\n",
    "    data = json.dumps(payload)\n",
    "    sock.sendall(data.encode(\"utf-8\"))\n",
    "\n",
    "\n",
    "def connect_all_envs() -> List[socket.socket]:\n",
    "    socks: List[socket.socket] = []\n",
    "    for p in ENV_PORTS:\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        s.connect((HOST, p))\n",
    "        s.setblocking(False)\n",
    "        print(f\"Connected to env on port {p}\")\n",
    "        socks.append(s)\n",
    "    return socks\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  State / RL helpers\n",
    "# ===========================\n",
    "\n",
    "def build_state_vector(message: Dict[str, Any]) -> List[float]:\n",
    "    arena = message.get(\"arena\", {})\n",
    "    tank = message.get(\"tank\", {})\n",
    "    goal = message.get(\"goal\", {})\n",
    "\n",
    "    arena_w = float(arena.get(\"width\", 1.0))\n",
    "    arena_h = float(arena.get(\"height\", 1.0))\n",
    "\n",
    "    tank_x = float(tank.get(\"x\", 0.0))\n",
    "    tank_y = float(tank.get(\"y\", 0.0))\n",
    "    goal_x = float(goal.get(\"x\", 0.0))\n",
    "    goal_y = float(goal.get(\"y\", 0.0))\n",
    "\n",
    "    # NEW: orientation\n",
    "    theta = float(tank.get(\"rot\", 0.0))  # radians from Godot\n",
    "    cos_theta = math.cos(theta)\n",
    "    sin_theta = math.sin(theta)\n",
    "\n",
    "    # Normalize positions to [0, 1]\n",
    "    tank_x_n = tank_x / arena_w\n",
    "    tank_y_n = tank_y / arena_h\n",
    "    goal_x_n = goal_x / arena_w\n",
    "    goal_y_n = goal_y / arena_h\n",
    "\n",
    "    # Relative position tank -> goal, also normalized\n",
    "    dx = (goal_x - tank_x) / arena_w\n",
    "    dy = (goal_y - tank_y) / arena_h\n",
    "\n",
    "    # State: position, relative goal, orientation (cos, sin)\n",
    "    return [tank_x_n, tank_y_n, goal_x_n, goal_y_n, dx, dy, cos_theta, sin_theta]\n",
    "\n",
    "\n",
    "def select_action(state: List[float]) -> int:\n",
    "    global epsilon\n",
    "\n",
    "    epsilon = max(EPSILON_END, epsilon - EPSILON_DECAY)\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(N_ACTIONS)\n",
    "\n",
    "    state_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_vals = q_net(state_t)\n",
    "    return int(torch.argmax(q_vals, dim=1).item())\n",
    "\n",
    "\n",
    "def store_transition(s: List[float], a: int, r: float, s_next: List[float], done: bool) -> None:\n",
    "    replay_buffer.append((s, a, r, s_next, done))\n",
    "\n",
    "\n",
    "def train_step() -> float | None:\n",
    "    global total_steps\n",
    "\n",
    "    if len(replay_buffer) < BATCH_SIZE:\n",
    "        return None\n",
    "\n",
    "    batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "\n",
    "    states = torch.tensor([b[0] for b in batch], dtype=torch.float32, device=device)\n",
    "    actions = torch.tensor([b[1] for b in batch], dtype=torch.int64, device=device)\n",
    "    rewards = torch.tensor([b[2] for b in batch], dtype=torch.float32, device=device)\n",
    "    next_states = torch.tensor([b[3] for b in batch], dtype=torch.float32, device=device)\n",
    "    dones = torch.tensor([b[4] for b in batch], dtype=torch.float32, device=device)\n",
    "\n",
    "    q_values = q_net(states)  # [B, N_ACTIONS]\n",
    "    q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)  # [B]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q = q_net(next_states)\n",
    "        max_next_q = next_q.max(dim=1)[0]\n",
    "        targets = rewards + GAMMA * max_next_q * (1.0 - dones)\n",
    "\n",
    "    loss = loss_fn(q_values, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_steps += 1\n",
    "    if total_steps % 1000 == 0:\n",
    "        save_checkpoint()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Per-env message handling\n",
    "# ===========================\n",
    "\n",
    "def on_message_multi_env(env_idx: int, sock: socket.socket, message: Dict[str, Any]) -> None:\n",
    "    state = build_state_vector(message)\n",
    "    reward = float(message.get(\"reward\", 0.0))\n",
    "    done = bool(message.get(\"done\", False))\n",
    "\n",
    "    episode_returns[env_idx] += reward\n",
    "\n",
    "    prev_s = prev_states[env_idx]\n",
    "    prev_a = prev_actions[env_idx]\n",
    "\n",
    "    if prev_s is not None and prev_a is not None:\n",
    "        store_transition(prev_s, prev_a, reward, state, done)\n",
    "        loss = train_step()\n",
    "    else:\n",
    "        loss = None\n",
    "\n",
    "    action_idx = select_action(state)\n",
    "    prev_states[env_idx] = state\n",
    "    prev_actions[env_idx] = action_idx\n",
    "\n",
    "    action = ACTIONS[action_idx]\n",
    "\n",
    "    state_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_vals = q_net(state_t)\n",
    "        chosen_q = float(q_vals[0, action_idx].item())\n",
    "\n",
    "    response = {\n",
    "        \"value\": chosen_q,\n",
    "        \"action\": {\n",
    "            \"turn\": action[\"turn\"],\n",
    "            \"throttle\": action[\"throttle\"],\n",
    "        },\n",
    "        \"debug\": {\n",
    "            \"env\": env_idx,\n",
    "            \"state\": state,\n",
    "            \"reward\": reward,\n",
    "            \"done\": done,\n",
    "            \"epsilon\": epsilon,\n",
    "            \"loss\": loss,\n",
    "            \"action_idx\": action_idx,\n",
    "            \"episode\": episode_idxs[env_idx],\n",
    "            \"episode_return\": episode_returns[env_idx],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    send_json(sock, response)\n",
    "\n",
    "    if done:\n",
    "        print(f\"[ENV {env_idx}] episode {episode_idxs[env_idx]} return = {episode_returns[env_idx]:.3f}, eps={epsilon:.3f}\")\n",
    "        log_episode_to_file(env_idx, episode_idxs[env_idx], episode_returns[env_idx], epsilon)\n",
    "\n",
    "        episode_idxs[env_idx] += 1\n",
    "        episode_returns[env_idx] = 0.0\n",
    "        prev_states[env_idx] = None\n",
    "        prev_actions[env_idx] = None\n",
    "\n",
    "\n",
    "def _process_buffer_for_env(env_idx: int, sock: socket.socket) -> None:\n",
    "    buf = buffers[env_idx]\n",
    "\n",
    "    while buf:\n",
    "        buf = buf.lstrip()\n",
    "        if not buf:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            obj, idx = decoder.raw_decode(buf)\n",
    "        except json.JSONDecodeError:\n",
    "            break\n",
    "\n",
    "        buf = buf[idx:]\n",
    "        try:\n",
    "            on_message_multi_env(env_idx, sock, obj)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in on_message for env {env_idx}: {e}\")\n",
    "\n",
    "    buffers[env_idx] = buf\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Multi-env receive loop\n",
    "# ===========================\n",
    "\n",
    "def receive_loop_multi_env() -> None:\n",
    "    socks = connect_all_envs()\n",
    "    print(\"All envs connected, starting training loop...\")\n",
    "\n",
    "    while True:\n",
    "        if not socks:\n",
    "            print(\"All envs disconnected, stopping.\")\n",
    "            break\n",
    "\n",
    "        readable, _, _ = select.select(socks, [], [], 0.1)\n",
    "\n",
    "        for s in readable:\n",
    "            env_idx = socks.index(s)\n",
    "            try:\n",
    "                chunk = s.recv(4096)\n",
    "            except BlockingIOError:\n",
    "                continue\n",
    "\n",
    "            if not chunk:\n",
    "                print(f\"Env {env_idx} disconnected.\")\n",
    "                socks.remove(s)\n",
    "                continue\n",
    "\n",
    "            buffers[env_idx] += chunk.decode(\"utf-8\")\n",
    "            _process_buffer_for_env(env_idx, s)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_checkpoint()\n",
    "    receive_loop_multi_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc9a52-a41b-40d9-9095-c8fae3a42f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
