{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0d140e-d46d-455c-a828-bd36e21fbaf9",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Modell "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534403f2-c4a3-4b8e-8810-837c585e13ea",
   "metadata": {},
   "source": [
    "# Godot Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a907490-767a-4d9a-bc88-ef93e5b24bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 20 environments.\n",
      "Expecting Godot instances on ports: [5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019]\n",
      "Using device: cpu\n",
      "[CHECKPOINT] Loaded from tank_dqn_checkpoint.pt, steps=192000\n",
      "Connected to env on port 5000\n",
      "Connected to env on port 5001\n",
      "Connected to env on port 5002\n",
      "Connected to env on port 5003\n",
      "Connected to env on port 5004\n",
      "Connected to env on port 5005\n",
      "Connected to env on port 5006\n",
      "Connected to env on port 5007\n",
      "Connected to env on port 5008\n",
      "Connected to env on port 5009\n",
      "Connected to env on port 5010\n",
      "Connected to env on port 5011\n",
      "Connected to env on port 5012\n",
      "Connected to env on port 5013\n",
      "Connected to env on port 5014\n",
      "Connected to env on port 5015\n",
      "Connected to env on port 5016\n",
      "Connected to env on port 5017\n",
      "Connected to env on port 5018\n",
      "Connected to env on port 5019\n",
      "All envs connected, starting training loop...\n",
      "[ENV 8] episode 48 return = 0.990, eps=0.010\n",
      "[ENV 5] episode 49 return = 10.984, eps=0.010\n",
      "[ENV 12] episode 54 return = 15.545, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 0] episode 43 return = 23.289, eps=0.010\n",
      "[ENV 5] episode 50 return = 27.763, eps=0.010\n",
      "[ENV 18] episode 49 return = 54.484, eps=0.010\n",
      "[ENV 10] episode 50 return = 42.396, eps=0.010\n",
      "[ENV 5] episode 51 return = 1.262, eps=0.010\n",
      "[ENV 2] episode 48 return = 58.085, eps=0.010\n",
      "[ENV 17] episode 50 return = 16.505, eps=0.010\n",
      "[ENV 15] episode 49 return = 55.470, eps=0.010\n",
      "[ENV 9] episode 53 return = 38.193, eps=0.010\n",
      "[ENV 12] episode 55 return = 27.204, eps=0.010\n",
      "[ENV 13] episode 49 return = 69.050, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 9] episode 54 return = 25.386, eps=0.010\n",
      "[ENV 4] episode 50 return = 107.490, eps=0.010\n",
      "[ENV 4] episode 51 return = 3.159, eps=0.010\n",
      "[ENV 15] episode 50 return = 20.372, eps=0.010\n",
      "[ENV 13] episode 50 return = 23.234, eps=0.010\n",
      "[ENV 17] episode 51 return = 56.181, eps=0.010\n",
      "[ENV 1] episode 59 return = 56.798, eps=0.010\n",
      "[ENV 6] episode 49 return = 42.767, eps=0.010\n",
      "[ENV 18] episode 50 return = 68.748, eps=0.010\n",
      "[ENV 16] episode 48 return = 132.621, eps=0.010\n",
      "[ENV 4] episode 52 return = 18.418, eps=0.010\n",
      "[ENV 7] episode 49 return = 79.128, eps=0.010\n",
      "[ENV 19] episode 46 return = 79.179, eps=0.010\n",
      "[ENV 14] episode 50 return = 137.707, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 11] episode 46 return = 5.208, eps=0.010\n",
      "[ENV 8] episode 49 return = 135.742, eps=0.010\n",
      "[ENV 5] episode 52 return = 92.123, eps=0.010\n",
      "[ENV 4] episode 53 return = 20.436, eps=0.010\n",
      "[ENV 9] episode 55 return = 38.251, eps=0.010\n",
      "[ENV 4] episode 54 return = 6.639, eps=0.010\n",
      "[ENV 2] episode 49 return = 90.260, eps=0.010\n",
      "[ENV 10] episode 51 return = 61.207, eps=0.010\n",
      "[ENV 19] episode 47 return = 34.788, eps=0.010\n",
      "[ENV 15] episode 51 return = 61.102, eps=0.010\n",
      "[ENV 5] episode 53 return = 16.231, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 17] episode 52 return = 96.477, eps=0.010\n",
      "[ENV 13] episode 51 return = 88.657, eps=0.010\n",
      "[ENV 8] episode 50 return = 58.902, eps=0.010\n",
      "[ENV 0] episode 44 return = 144.346, eps=0.010\n",
      "[ENV 11] episode 47 return = 2.581, eps=0.010\n",
      "[ENV 6] episode 50 return = 112.590, eps=0.010\n",
      "[ENV 10] episode 52 return = 57.100, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 19] episode 48 return = 74.997, eps=0.010\n",
      "[ENV 15] episode 52 return = 53.354, eps=0.010\n",
      "[ENV 17] episode 53 return = 25.917, eps=0.010\n",
      "[ENV 1] episode 60 return = 34.183, eps=0.010\n",
      "[ENV 3] episode 54 return = 8.411, eps=0.010\n",
      "[ENV 9] episode 56 return = 54.550, eps=0.010\n",
      "[ENV 16] episode 49 return = 114.564, eps=0.010\n",
      "[ENV 7] episode 50 return = 101.386, eps=0.010\n",
      "[ENV 14] episode 51 return = 123.310, eps=0.010\n",
      "[ENV 12] episode 56 return = 134.119, eps=0.010\n",
      "[ENV 6] episode 51 return = 35.896, eps=0.010\n",
      "[ENV 10] episode 53 return = 9.432, eps=0.010\n",
      "[ENV 5] episode 54 return = 47.343, eps=0.010\n",
      "[ENV 19] episode 49 return = 25.517, eps=0.010\n",
      "[ENV 7] episode 51 return = 12.919, eps=0.010\n",
      "[ENV 16] episode 50 return = 17.386, eps=0.010\n",
      "[ENV 13] episode 52 return = 41.900, eps=0.010\n",
      "[ENV 11] episode 48 return = 47.722, eps=0.010\n",
      "[ENV 1] episode 61 return = 32.049, eps=0.010\n",
      "[ENV 12] episode 57 return = 11.526, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 4] episode 55 return = 121.376, eps=0.010\n",
      "[ENV 13] episode 53 return = 10.318, eps=0.010\n",
      "[ENV 6] episode 52 return = 19.157, eps=0.010\n",
      "[ENV 3] episode 55 return = 51.592, eps=0.010\n",
      "[ENV 2] episode 50 return = 88.203, eps=0.010\n",
      "[ENV 10] episode 54 return = 34.374, eps=0.010\n",
      "[ENV 0] episode 45 return = 101.503, eps=0.010\n",
      "[ENV 8] episode 51 return = 110.843, eps=0.010\n",
      "[ENV 15] episode 53 return = 93.246, eps=0.010\n",
      "[ENV 2] episode 51 return = 15.739, eps=0.010\n",
      "[ENV 7] episode 52 return = 34.447, eps=0.010\n",
      "[ENV 11] episode 49 return = 44.967, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 9] episode 57 return = 86.842, eps=0.010\n",
      "[ENV 4] episode 56 return = 36.819, eps=0.010\n",
      "[ENV 10] episode 55 return = 51.506, eps=0.010\n",
      "[ENV 19] episode 50 return = 74.322, eps=0.010\n",
      "[ENV 7] episode 53 return = 32.087, eps=0.010\n",
      "[ENV 14] episode 52 return = 94.239, eps=0.010\n",
      "[ENV 3] episode 56 return = 34.112, eps=0.010\n",
      "[ENV 12] episode 58 return = 83.694, eps=0.010\n",
      "[ENV 17] episode 54 return = 71.953, eps=0.010\n",
      "[ENV 11] episode 50 return = 34.163, eps=0.010\n",
      "[ENV 9] episode 58 return = 33.393, eps=0.010\n",
      "[ENV 0] episode 46 return = 80.314, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 13] episode 54 return = 51.390, eps=0.010\n",
      "[ENV 11] episode 51 return = 23.110, eps=0.010\n",
      "[ENV 18] episode 51 return = 134.053, eps=0.010\n",
      "[ENV 15] episode 54 return = 40.668, eps=0.010\n",
      "[ENV 1] episode 62 return = 100.643, eps=0.010\n",
      "[ENV 16] episode 51 return = 131.302, eps=0.010\n",
      "[ENV 12] episode 59 return = 64.038, eps=0.010\n",
      "[ENV 0] episode 47 return = 37.412, eps=0.010\n",
      "[ENV 5] episode 55 return = 145.631, eps=0.010\n",
      "[ENV 6] episode 53 return = 115.060, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 13] episode 55 return = 33.492, eps=0.010\n",
      "[ENV 4] episode 57 return = 68.250, eps=0.010\n",
      "[ENV 6] episode 54 return = 25.187, eps=0.010\n",
      "[ENV 8] episode 52 return = 128.867, eps=0.010\n",
      "[ENV 9] episode 59 return = 83.301, eps=0.010\n",
      "[ENV 18] episode 52 return = 65.567, eps=0.010\n",
      "[ENV 14] episode 53 return = 109.177, eps=0.010\n",
      "[ENV 10] episode 56 return = 112.424, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 2] episode 52 return = 66.692, eps=0.010\n",
      "[ENV 13] episode 56 return = 23.099, eps=0.010\n",
      "[ENV 8] episode 53 return = 46.778, eps=0.010\n",
      "[ENV 10] episode 57 return = 22.001, eps=0.010\n",
      "[ENV 19] episode 51 return = 127.654, eps=0.010\n",
      "[ENV 10] episode 58 return = -0.005, eps=0.010\n",
      "[ENV 9] episode 60 return = 46.717, eps=0.010\n",
      "[ENV 17] episode 55 return = 129.000, eps=0.010\n",
      "[ENV 12] episode 60 return = 77.893, eps=0.010\n",
      "[ENV 3] episode 57 return = 120.273, eps=0.010\n",
      "[ENV 7] episode 54 return = 128.002, eps=0.010\n",
      "[ENV 1] episode 63 return = 100.237, eps=0.010\n",
      "[ENV 19] episode 52 return = 6.869, eps=0.010\n",
      "[ENV 0] episode 48 return = 86.857, eps=0.010\n",
      "[ENV 19] episode 53 return = 10.296, eps=0.010\n",
      "[ENV 5] episode 56 return = 79.219, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 18] episode 53 return = 68.018, eps=0.010\n",
      "[ENV 16] episode 52 return = 137.182, eps=0.010\n",
      "[ENV 13] episode 57 return = 41.097, eps=0.010\n",
      "[ENV 15] episode 55 return = 150.692, eps=0.010\n",
      "[ENV 11] episode 52 return = 127.303, eps=0.010\n",
      "[ENV 1] episode 64 return = 20.373, eps=0.010\n",
      "[ENV 2] episode 53 return = 61.980, eps=0.010\n",
      "[ENV 4] episode 58 return = 118.048, eps=0.010\n",
      "[ENV 19] episode 54 return = 32.308, eps=0.010\n",
      "[ENV 8] episode 54 return = 66.058, eps=0.010\n",
      "[ENV 11] episode 53 return = 13.953, eps=0.010\n",
      "[ENV 13] episode 58 return = 12.141, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 17] episode 56 return = 86.185, eps=0.010\n",
      "[ENV 2] episode 54 return = 42.573, eps=0.010\n",
      "[ENV 14] episode 54 return = 124.012, eps=0.010\n",
      "[ENV 11] episode 54 return = 27.444, eps=0.010\n",
      "[ENV 0] episode 49 return = 59.946, eps=0.010\n",
      "[ENV 16] episode 53 return = 56.806, eps=0.010\n",
      "[ENV 1] episode 65 return = 50.120, eps=0.010\n",
      "[ENV 15] episode 56 return = 48.279, eps=0.010\n",
      "[ENV 6] episode 55 return = 102.209, eps=0.010\n",
      "[ENV 7] episode 55 return = 103.676, eps=0.010\n",
      "[ENV 9] episode 61 return = 98.812, eps=0.010\n",
      "[ENV 19] episode 55 return = 61.316, eps=0.010\n",
      "[ENV 10] episode 59 return = 110.449, eps=0.010\n",
      "[ENV 13] episode 59 return = 45.298, eps=0.010\n",
      "[ENV 18] episode 54 return = 87.138, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 12] episode 61 return = 95.281, eps=0.010\n",
      "[ENV 5] episode 57 return = 102.426, eps=0.010\n",
      "[ENV 9] episode 62 return = 17.437, eps=0.010\n",
      "[ENV 3] episode 58 return = 141.533, eps=0.010\n",
      "[ENV 11] episode 55 return = 5.536, eps=0.010\n",
      "[ENV 4] episode 59 return = 103.593, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 6] episode 56 return = 64.361, eps=0.010\n",
      "[ENV 10] episode 60 return = 31.400, eps=0.010\n",
      "[ENV 8] episode 55 return = 114.749, eps=0.010\n",
      "[ENV 1] episode 66 return = 71.565, eps=0.010\n",
      "[ENV 15] episode 57 return = 69.878, eps=0.010\n",
      "[ENV 8] episode 56 return = 8.144, eps=0.010\n",
      "[ENV 16] episode 54 return = 89.698, eps=0.010\n",
      "[ENV 1] episode 67 return = 23.762, eps=0.010\n",
      "[ENV 19] episode 56 return = 73.073, eps=0.010\n",
      "[ENV 11] episode 56 return = 70.710, eps=0.010\n",
      "[ENV 9] episode 63 return = 87.777, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 17] episode 57 return = 91.022, eps=0.010\n",
      "[ENV 12] episode 62 return = 71.639, eps=0.010\n",
      "[ENV 2] episode 55 return = 146.090, eps=0.010\n",
      "[ENV 14] episode 55 return = 80.209, eps=0.010\n",
      "[ENV 16] episode 55 return = 12.227, eps=0.010\n",
      "[ENV 13] episode 60 return = 69.112, eps=0.010\n",
      "[ENV 0] episode 50 return = 146.628, eps=0.010\n",
      "[ENV 8] episode 57 return = 58.795, eps=0.010\n",
      "[ENV 1] episode 68 return = 58.065, eps=0.010\n",
      "[ENV 3] episode 59 return = 119.736, eps=0.010\n",
      "[ENV 10] episode 61 return = 87.607, eps=0.010\n",
      "[ENV 17] episode 58 return = 35.408, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 5] episode 58 return = 118.241, eps=0.010\n",
      "[ENV 16] episode 56 return = 25.750, eps=0.010\n",
      "[ENV 8] episode 58 return = 14.436, eps=0.010\n",
      "[ENV 4] episode 60 return = 124.211, eps=0.010\n",
      "[ENV 11] episode 57 return = 54.940, eps=0.010\n",
      "[ENV 12] episode 63 return = 46.914, eps=0.010\n",
      "[ENV 6] episode 57 return = 93.876, eps=0.010\n",
      "[ENV 7] episode 56 return = 150.939, eps=0.010\n",
      "[ENV 10] episode 62 return = 34.181, eps=0.010\n",
      "[ENV 9] episode 64 return = 79.536, eps=0.010\n",
      "[ENV 18] episode 55 return = 151.977, eps=0.010\n",
      "[ENV 12] episode 64 return = 25.743, eps=0.010\n",
      "[ENV 15] episode 58 return = 127.333, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 17] episode 59 return = 40.517, eps=0.010\n",
      "[ENV 14] episode 56 return = 92.548, eps=0.010\n",
      "[ENV 1] episode 69 return = 63.836, eps=0.010\n",
      "[ENV 2] episode 56 return = 84.434, eps=0.010\n",
      "[ENV 6] episode 58 return = 44.626, eps=0.010\n",
      "[ENV 13] episode 61 return = 95.402, eps=0.010\n",
      "[ENV 11] episode 58 return = 55.249, eps=0.010\n",
      "[ENV 8] episode 59 return = 53.951, eps=0.010\n",
      "[ENV 0] episode 51 return = 97.987, eps=0.010\n",
      "[ENV 7] episode 57 return = 28.908, eps=0.010\n",
      "[ENV 4] episode 61 return = 68.735, eps=0.010\n",
      "[ENV 19] episode 57 return = 130.360, eps=0.010\n",
      "[ENV 14] episode 57 return = 36.437, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 17] episode 60 return = 62.413, eps=0.010\n",
      "[ENV 18] episode 56 return = 87.252, eps=0.010\n",
      "[ENV 13] episode 62 return = 50.074, eps=0.010\n",
      "[ENV 5] episode 59 return = 134.707, eps=0.010\n",
      "[ENV 15] episode 59 return = 27.986, eps=0.010\n",
      "[ENV 10] episode 63 return = 116.654, eps=0.010\n",
      "[ENV 4] episode 62 return = 50.714, eps=0.010\n",
      "[ENV 6] episode 59 return = 85.126, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 9] episode 65 return = 98.519, eps=0.010\n",
      "[ENV 11] episode 59 return = 17.797, eps=0.010\n",
      "[ENV 3] episode 60 return = 110.320, eps=0.010\n",
      "[ENV 7] episode 58 return = 46.774, eps=0.010\n",
      "[ENV 0] episode 52 return = 8.747, eps=0.010\n",
      "[ENV 16] episode 57 return = 132.762, eps=0.010\n",
      "[ENV 8] episode 60 return = 38.015, eps=0.010\n",
      "[ENV 2] episode 57 return = 42.735, eps=0.010\n",
      "[ENV 10] episode 64 return = 55.096, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 4] episode 63 return = 60.711, eps=0.010\n",
      "[ENV 12] episode 65 return = 43.859, eps=0.010\n",
      "[ENV 18] episode 57 return = 74.268, eps=0.010\n",
      "[ENV 19] episode 58 return = 76.258, eps=0.010\n",
      "[ENV 2] episode 58 return = 46.112, eps=0.010\n",
      "[ENV 11] episode 60 return = 27.423, eps=0.010\n",
      "[ENV 17] episode 61 return = 107.107, eps=0.010\n",
      "[ENV 6] episode 60 return = 54.052, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 15] episode 60 return = 105.465, eps=0.010\n",
      "[ENV 7] episode 59 return = 84.188, eps=0.010\n",
      "[ENV 1] episode 70 return = 87.891, eps=0.010\n",
      "[ENV 12] episode 66 return = 34.797, eps=0.010\n",
      "[ENV 7] episode 60 return = 8.034, eps=0.010\n",
      "[ENV 9] episode 66 return = 103.454, eps=0.010\n",
      "[ENV 4] episode 64 return = 26.746, eps=0.010\n",
      "[ENV 13] episode 63 return = 26.722, eps=0.010\n",
      "[ENV 14] episode 58 return = 91.698, eps=0.010\n",
      "[ENV 2] episode 59 return = 46.992, eps=0.010\n",
      "[ENV 3] episode 61 return = 82.122, eps=0.010\n",
      "[ENV 5] episode 60 return = 46.180, eps=0.010\n",
      "[ENV 0] episode 53 return = 98.828, eps=0.010\n",
      "[ENV 16] episode 58 return = 121.308, eps=0.010\n",
      "[ENV 10] episode 65 return = 61.448, eps=0.010\n",
      "[ENV 19] episode 59 return = 53.820, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 8] episode 61 return = 128.729, eps=0.010\n",
      "[ENV 4] episode 65 return = 14.643, eps=0.010\n",
      "[ENV 15] episode 61 return = 74.187, eps=0.010\n",
      "[ENV 18] episode 58 return = 91.113, eps=0.010\n",
      "[ENV 3] episode 62 return = 47.618, eps=0.010\n",
      "[ENV 7] episode 61 return = 68.279, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 16] episode 59 return = 42.731, eps=0.010\n",
      "[ENV 0] episode 54 return = 54.746, eps=0.010\n",
      "[ENV 10] episode 66 return = 43.040, eps=0.010\n",
      "[ENV 11] episode 61 return = 143.633, eps=0.010\n",
      "[ENV 17] episode 62 return = 121.551, eps=0.010\n",
      "[ENV 3] episode 63 return = 47.685, eps=0.010\n",
      "[ENV 12] episode 67 return = 135.879, eps=0.010\n",
      "[ENV 4] episode 66 return = 71.025, eps=0.010\n",
      "[ENV 7] episode 62 return = 26.810, eps=0.010\n",
      "[ENV 16] episode 60 return = 1.613, eps=0.010\n",
      "[ENV 1] episode 71 return = 130.008, eps=0.010\n",
      "[ENV 9] episode 67 return = 18.794, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 19] episode 60 return = 97.053, eps=0.010\n",
      "[ENV 13] episode 64 return = 125.717, eps=0.010\n",
      "[ENV 18] episode 59 return = 94.143, eps=0.010\n",
      "[ENV 0] episode 55 return = 81.725, eps=0.010\n",
      "[ENV 3] episode 64 return = 56.112, eps=0.010\n",
      "[ENV 15] episode 62 return = 99.910, eps=0.010\n",
      "[ENV 3] episode 65 return = -0.093, eps=0.010\n",
      "[ENV 9] episode 68 return = 52.141, eps=0.010\n",
      "[ENV 16] episode 61 return = 37.509, eps=0.010\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[ENV 19] episode 61 return = 45.836, eps=0.010\n",
      "[ENV 17] episode 63 return = 75.367, eps=0.010\n",
      "[ENV 6] episode 61 return = 91.423, eps=0.010\n",
      "[ENV 1] episode 72 return = 39.958, eps=0.010\n",
      "[ENV 10] episode 67 return = 95.368, eps=0.010\n",
      "[ENV 2] episode 60 return = 156.180, eps=0.010\n",
      "[ENV 7] episode 63 return = 72.332, eps=0.010\n",
      "[ENV 18] episode 60 return = 30.475, eps=0.010\n",
      "[ENV 13] episode 65 return = 1.702, eps=0.010\n",
      "[ENV 0] episode 56 return = 46.678, eps=0.010\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import json\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import random\n",
    "from collections import deque\n",
    "import os\n",
    "import select\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ===========================\n",
    "#      ENVIRONMENT CONFIG\n",
    "# ===========================\n",
    "\n",
    "HOST = \"127.0.0.1\"\n",
    "\n",
    "# How many parallel Godot environments to train with\n",
    "N_ENVS = 20 \n",
    "\n",
    "# Port base for Godot instances\n",
    "BASE_PORT = 5000\n",
    "\n",
    "# Generate ports based on environments\n",
    "ENV_PORTS = [BASE_PORT + i for i in range(N_ENVS)]\n",
    "\n",
    "print(f\"Training with {N_ENVS} environments.\")\n",
    "print(f\"Expecting Godot instances on ports: {ENV_PORTS}\")\n",
    "\n",
    "# ===========================\n",
    "#  RL / model config\n",
    "# ===========================\n",
    "\n",
    "# State vector: tank_x, tank_y, goal_x, goal_y, dx, dy  (all normalized)\n",
    "STATE_SIZE = 8\n",
    "\n",
    "# Discrete actions: (turn, throttle) with values in {-1, 0, 1}\n",
    "# Godot will feed these into setTurn() and setDirection().\n",
    "ACTIONS: List[Dict[str, float]] = [\n",
    "    {\"turn\": -1.0, \"throttle\":  1.0},  # left + forward\n",
    "    {\"turn\":  0.0, \"throttle\":  1.0},  # straight + forward\n",
    "    {\"turn\":  1.0, \"throttle\":  1.0},  # right + forward\n",
    "    {\"turn\":  0.0, \"throttle\":  0.0},  # stop\n",
    "    {\"turn\":  0.0, \"throttle\": -1.0},  # straight + backward\n",
    "]\n",
    "N_ACTIONS = len(ACTIONS)\n",
    "\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 0.2\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_SIZE = 10_000\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "CHECKPOINT_PATH = \"tank_dqn_checkpoint.pt\"\n",
    "LOG_PATH = \"training_log.csv\"\n",
    "\n",
    "# ===========================\n",
    "#  Q-network\n",
    "# ===========================\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)  # [batch, output_dim] (Q-values for each action)\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "q_net = QNetwork(STATE_SIZE, N_ACTIONS).to(device)\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "ReplayEntry = Tuple[List[float], int, float, List[float], bool]\n",
    "replay_buffer: deque[ReplayEntry] = deque(maxlen=REPLAY_SIZE)\n",
    "\n",
    "# Globals for RL bookkeeping (shared across envs)\n",
    "epsilon = EPSILON_START\n",
    "total_steps = 0\n",
    "\n",
    "# Per-env bookkeeping\n",
    "num_envs = len(ENV_PORTS)\n",
    "buffers: List[str] = [\"\"] * num_envs\n",
    "prev_states: List[List[float] | None] = [None] * num_envs\n",
    "prev_actions: List[int | None] = [None] * num_envs\n",
    "episode_returns: List[float] = [0.0] * num_envs\n",
    "episode_idxs: List[int] = [0] * num_envs\n",
    "\n",
    "decoder = json.JSONDecoder()\n",
    "\n",
    "# ===========================\n",
    "#  Checkpoint + logging\n",
    "# ===========================\n",
    "\n",
    "def save_checkpoint() -> None:\n",
    "    state = {\n",
    "        \"q_net\": q_net.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"epsilon\": epsilon,\n",
    "        \"total_steps\": total_steps,\n",
    "        \"episode_idxs\": episode_idxs,\n",
    "    }\n",
    "    torch.save(state, CHECKPOINT_PATH)\n",
    "    print(f\"[CHECKPOINT] Saved to {CHECKPOINT_PATH}\")\n",
    "\n",
    "\n",
    "def load_checkpoint() -> None:\n",
    "    global epsilon, total_steps, episode_idxs\n",
    "    if not os.path.exists(CHECKPOINT_PATH):\n",
    "        print(\"[CHECKPOINT] No checkpoint found, starting fresh.\")\n",
    "        return\n",
    "    state = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "    q_net.load_state_dict(state[\"q_net\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer\"])\n",
    "    epsilon = state.get(\"epsilon\", epsilon)\n",
    "    total_steps = state.get(\"total_steps\", 0)\n",
    "    episode_idxs = state.get(\"episode_idxs\", episode_idxs)\n",
    "    print(f\"[CHECKPOINT] Loaded from {CHECKPOINT_PATH}, steps={total_steps}\")\n",
    "\n",
    "\n",
    "def log_episode_to_file(env_idx: int, ep_idx: int, ep_return: float, eps: float) -> None:\n",
    "    header_needed = not os.path.exists(LOG_PATH)\n",
    "    with open(LOG_PATH, \"a\") as f:\n",
    "        if header_needed:\n",
    "            f.write(\"env,episode,return,epsilon\\n\")\n",
    "        f.write(f\"{env_idx},{ep_idx},{ep_return},{eps}\\n\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Networking helpers\n",
    "# ===========================\n",
    "\n",
    "def send_json(sock: socket.socket, payload: Dict[str, Any]) -> None:\n",
    "    data = json.dumps(payload)\n",
    "    sock.sendall(data.encode(\"utf-8\"))\n",
    "\n",
    "\n",
    "def connect_all_envs() -> List[socket.socket]:\n",
    "    socks: List[socket.socket] = []\n",
    "    for p in ENV_PORTS:\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        s.connect((HOST, p))\n",
    "        s.setblocking(False)\n",
    "        print(f\"Connected to env on port {p}\")\n",
    "        socks.append(s)\n",
    "    return socks\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  State / RL helpers\n",
    "# ===========================\n",
    "\n",
    "def build_state_vector(message: Dict[str, Any]) -> List[float]:\n",
    "    arena = message.get(\"arena\", {})\n",
    "    tank = message.get(\"tank\", {})\n",
    "    goal = message.get(\"goal\", {})\n",
    "\n",
    "    arena_w = float(arena.get(\"width\", 1.0))\n",
    "    arena_h = float(arena.get(\"height\", 1.0))\n",
    "\n",
    "    tank_x = float(tank.get(\"x\", 0.0))\n",
    "    tank_y = float(tank.get(\"y\", 0.0))\n",
    "    goal_x = float(goal.get(\"x\", 0.0))\n",
    "    goal_y = float(goal.get(\"y\", 0.0))\n",
    "\n",
    "    # NEW: orientation\n",
    "    theta = float(tank.get(\"rot\", 0.0))  # radians from Godot\n",
    "    cos_theta = math.cos(theta)\n",
    "    sin_theta = math.sin(theta)\n",
    "\n",
    "    # Normalize positions to [0, 1]\n",
    "    tank_x_n = tank_x / arena_w\n",
    "    tank_y_n = tank_y / arena_h\n",
    "    goal_x_n = goal_x / arena_w\n",
    "    goal_y_n = goal_y / arena_h\n",
    "\n",
    "    # Relative position tank -> goal, also normalized\n",
    "    dx = (goal_x - tank_x) / arena_w\n",
    "    dy = (goal_y - tank_y) / arena_h\n",
    "\n",
    "    # State: position, relative goal, orientation (cos, sin)\n",
    "    return [tank_x_n, tank_y_n, goal_x_n, goal_y_n, dx, dy, cos_theta, sin_theta]\n",
    "\n",
    "\n",
    "def select_action(state: List[float]) -> int:\n",
    "    global epsilon\n",
    "\n",
    "    epsilon = max(EPSILON_END, epsilon - EPSILON_DECAY)\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(N_ACTIONS)\n",
    "\n",
    "    state_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_vals = q_net(state_t)\n",
    "    return int(torch.argmax(q_vals, dim=1).item())\n",
    "\n",
    "\n",
    "def store_transition(s: List[float], a: int, r: float, s_next: List[float], done: bool) -> None:\n",
    "    replay_buffer.append((s, a, r, s_next, done))\n",
    "\n",
    "\n",
    "def train_step() -> float | None:\n",
    "    global total_steps\n",
    "\n",
    "    if len(replay_buffer) < BATCH_SIZE:\n",
    "        return None\n",
    "\n",
    "    batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "\n",
    "    states = torch.tensor([b[0] for b in batch], dtype=torch.float32, device=device)\n",
    "    actions = torch.tensor([b[1] for b in batch], dtype=torch.int64, device=device)\n",
    "    rewards = torch.tensor([b[2] for b in batch], dtype=torch.float32, device=device)\n",
    "    next_states = torch.tensor([b[3] for b in batch], dtype=torch.float32, device=device)\n",
    "    dones = torch.tensor([b[4] for b in batch], dtype=torch.float32, device=device)\n",
    "\n",
    "    q_values = q_net(states)  # [B, N_ACTIONS]\n",
    "    q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)  # [B]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q = q_net(next_states)\n",
    "        max_next_q = next_q.max(dim=1)[0]\n",
    "        targets = rewards + GAMMA * max_next_q * (1.0 - dones)\n",
    "\n",
    "    loss = loss_fn(q_values, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_steps += 1\n",
    "    if total_steps % 1000 == 0:\n",
    "        save_checkpoint()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Per-env message handling\n",
    "# ===========================\n",
    "\n",
    "def on_message_multi_env(env_idx: int, sock: socket.socket, message: Dict[str, Any]) -> None:\n",
    "    state = build_state_vector(message)\n",
    "    reward = float(message.get(\"reward\", 0.0))\n",
    "    done = bool(message.get(\"done\", False))\n",
    "\n",
    "    episode_returns[env_idx] += reward\n",
    "\n",
    "    prev_s = prev_states[env_idx]\n",
    "    prev_a = prev_actions[env_idx]\n",
    "\n",
    "    if prev_s is not None and prev_a is not None:\n",
    "        store_transition(prev_s, prev_a, reward, state, done)\n",
    "        loss = train_step()\n",
    "    else:\n",
    "        loss = None\n",
    "\n",
    "    action_idx = select_action(state)\n",
    "    prev_states[env_idx] = state\n",
    "    prev_actions[env_idx] = action_idx\n",
    "\n",
    "    action = ACTIONS[action_idx]\n",
    "\n",
    "    state_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_vals = q_net(state_t)\n",
    "        chosen_q = float(q_vals[0, action_idx].item())\n",
    "\n",
    "    response = {\n",
    "        \"value\": chosen_q,\n",
    "        \"action\": {\n",
    "            \"turn\": action[\"turn\"],\n",
    "            \"throttle\": action[\"throttle\"],\n",
    "        },\n",
    "        \"debug\": {\n",
    "            \"env\": env_idx,\n",
    "            \"state\": state,\n",
    "            \"reward\": reward,\n",
    "            \"done\": done,\n",
    "            \"epsilon\": epsilon,\n",
    "            \"loss\": loss,\n",
    "            \"action_idx\": action_idx,\n",
    "            \"episode\": episode_idxs[env_idx],\n",
    "            \"episode_return\": episode_returns[env_idx],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    send_json(sock, response)\n",
    "\n",
    "    if done:\n",
    "        print(f\"[ENV {env_idx}] episode {episode_idxs[env_idx]} return = {episode_returns[env_idx]:.3f}, eps={epsilon:.3f}\")\n",
    "        log_episode_to_file(env_idx, episode_idxs[env_idx], episode_returns[env_idx], epsilon)\n",
    "\n",
    "        episode_idxs[env_idx] += 1\n",
    "        episode_returns[env_idx] = 0.0\n",
    "        prev_states[env_idx] = None\n",
    "        prev_actions[env_idx] = None\n",
    "\n",
    "\n",
    "def _process_buffer_for_env(env_idx: int, sock: socket.socket) -> None:\n",
    "    buf = buffers[env_idx]\n",
    "\n",
    "    while buf:\n",
    "        buf = buf.lstrip()\n",
    "        if not buf:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            obj, idx = decoder.raw_decode(buf)\n",
    "        except json.JSONDecodeError:\n",
    "            break\n",
    "\n",
    "        buf = buf[idx:]\n",
    "        try:\n",
    "            on_message_multi_env(env_idx, sock, obj)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in on_message for env {env_idx}: {e}\")\n",
    "\n",
    "    buffers[env_idx] = buf\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Multi-env receive loop\n",
    "# ===========================\n",
    "\n",
    "def receive_loop_multi_env() -> None:\n",
    "    socks = connect_all_envs()\n",
    "    print(\"All envs connected, starting training loop...\")\n",
    "\n",
    "    while True:\n",
    "        if not socks:\n",
    "            print(\"All envs disconnected, stopping.\")\n",
    "            break\n",
    "\n",
    "        readable, _, _ = select.select(socks, [], [], 0.1)\n",
    "\n",
    "        for s in readable:\n",
    "            env_idx = socks.index(s)\n",
    "            try:\n",
    "                chunk = s.recv(4096)\n",
    "            except BlockingIOError:\n",
    "                continue\n",
    "\n",
    "            if not chunk:\n",
    "                print(f\"Env {env_idx} disconnected.\")\n",
    "                socks.remove(s)\n",
    "                continue\n",
    "\n",
    "            buffers[env_idx] += chunk.decode(\"utf-8\")\n",
    "            _process_buffer_for_env(env_idx, s)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_checkpoint()\n",
    "    receive_loop_multi_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc9a52-a41b-40d9-9095-c8fae3a42f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
