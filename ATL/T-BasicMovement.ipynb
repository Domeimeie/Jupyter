{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0d140e-d46d-455c-a828-bd36e21fbaf9",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Modell "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534403f2-c4a3-4b8e-8810-837c585e13ea",
   "metadata": {},
   "source": [
    "# Godot Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a907490-767a-4d9a-bc88-ef93e5b24bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 16 environments.\n",
      "Expecting Godot instances on ports: [5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015]\n",
      "Using device: cpu\n",
      "[CHECKPOINT] No checkpoint found, starting fresh.\n",
      "Connected to env on port 5000\n",
      "Connected to env on port 5001\n",
      "Connected to env on port 5002\n",
      "Connected to env on port 5003\n",
      "Connected to env on port 5004\n",
      "Connected to env on port 5005\n",
      "Connected to env on port 5006\n",
      "Connected to env on port 5007\n",
      "Connected to env on port 5008\n",
      "Connected to env on port 5009\n",
      "Connected to env on port 5010\n",
      "Connected to env on port 5011\n",
      "Connected to env on port 5012\n",
      "Connected to env on port 5013\n",
      "Connected to env on port 5014\n",
      "Connected to env on port 5015\n",
      "All envs connected, starting training loop...\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n",
      "[CHECKPOINT] Saved to tank_dqn_checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import json\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import random\n",
    "from collections import deque\n",
    "import os\n",
    "import select\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ===========================\n",
    "#      ENVIRONMENT CONFIG\n",
    "# ===========================\n",
    "\n",
    "HOST = \"127.0.0.1\"\n",
    "\n",
    "# How many parallel Godot environments to train with\n",
    "N_ENVS = 16 \n",
    "\n",
    "# Port base for Godot instances\n",
    "BASE_PORT = 5000\n",
    "\n",
    "# Generate ports based on environments\n",
    "ENV_PORTS = [BASE_PORT + i for i in range(N_ENVS)]\n",
    "\n",
    "print(f\"Training with {N_ENVS} environments.\")\n",
    "print(f\"Expecting Godot instances on ports: {ENV_PORTS}\")\n",
    "\n",
    "# ===========================\n",
    "#  RL / model config\n",
    "# ===========================\n",
    "\n",
    "# State vector: tank_x, tank_y, goal_x, goal_y, dx, dy  (all normalized)\n",
    "STATE_SIZE = 8\n",
    "\n",
    "# Discrete actions: (turn, throttle) with values in {-1, 0, 1}\n",
    "# Godot will feed these into setTurn() and setDirection().\n",
    "ACTIONS: List[Dict[str, float]] = [\n",
    "    {\"turn\": -1.0, \"throttle\":  1.0},  # left + forward\n",
    "    {\"turn\":  0.0, \"throttle\":  1.0},  # straight + forward\n",
    "    {\"turn\":  1.0, \"throttle\":  1.0},  # right + forward\n",
    "    {\"turn\":  0.0, \"throttle\":  0.0},  # stop\n",
    "    {\"turn\":  0.0, \"throttle\": -1.0},  # straight + backward\n",
    "]\n",
    "N_ACTIONS = len(ACTIONS)\n",
    "\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 0.2\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_SIZE = 10_000\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "CHECKPOINT_PATH = \"tank_dqn_checkpoint.pt\"\n",
    "LOG_PATH = \"training_log.csv\"\n",
    "\n",
    "# ===========================\n",
    "#  Q-network\n",
    "# ===========================\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)  # [batch, output_dim] (Q-values for each action)\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "q_net = QNetwork(STATE_SIZE, N_ACTIONS).to(device)\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "ReplayEntry = Tuple[List[float], int, float, List[float], bool]\n",
    "replay_buffer: deque[ReplayEntry] = deque(maxlen=REPLAY_SIZE)\n",
    "\n",
    "# Globals for RL bookkeeping (shared across envs)\n",
    "epsilon = EPSILON_START\n",
    "total_steps = 0\n",
    "\n",
    "# Per-env bookkeeping\n",
    "num_envs = len(ENV_PORTS)\n",
    "buffers: List[str] = [\"\"] * num_envs\n",
    "prev_states: List[List[float] | None] = [None] * num_envs\n",
    "prev_actions: List[int | None] = [None] * num_envs\n",
    "episode_returns: List[float] = [0.0] * num_envs\n",
    "episode_idxs: List[int] = [0] * num_envs\n",
    "\n",
    "decoder = json.JSONDecoder()\n",
    "\n",
    "# ===========================\n",
    "#  Checkpoint + logging\n",
    "# ===========================\n",
    "\n",
    "def save_checkpoint() -> None:\n",
    "    state = {\n",
    "        \"q_net\": q_net.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"epsilon\": epsilon,\n",
    "        \"total_steps\": total_steps,\n",
    "        \"episode_idxs\": episode_idxs,\n",
    "    }\n",
    "    torch.save(state, CHECKPOINT_PATH)\n",
    "    print(f\"[CHECKPOINT] Saved to {CHECKPOINT_PATH}\")\n",
    "\n",
    "\n",
    "def load_checkpoint() -> None:\n",
    "    global epsilon, total_steps, episode_idxs\n",
    "    if not os.path.exists(CHECKPOINT_PATH):\n",
    "        print(\"[CHECKPOINT] No checkpoint found, starting fresh.\")\n",
    "        return\n",
    "    state = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "    q_net.load_state_dict(state[\"q_net\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer\"])\n",
    "    epsilon = state.get(\"epsilon\", epsilon)\n",
    "    total_steps = state.get(\"total_steps\", 0)\n",
    "    episode_idxs = state.get(\"episode_idxs\", episode_idxs)\n",
    "    print(f\"[CHECKPOINT] Loaded from {CHECKPOINT_PATH}, steps={total_steps}\")\n",
    "\n",
    "\n",
    "def log_episode_to_file(env_idx: int, ep_idx: int, ep_return: float, eps: float) -> None:\n",
    "    header_needed = not os.path.exists(LOG_PATH)\n",
    "    with open(LOG_PATH, \"a\") as f:\n",
    "        if header_needed:\n",
    "            f.write(\"env,episode,return,epsilon\\n\")\n",
    "        f.write(f\"{env_idx},{ep_idx},{ep_return},{eps}\\n\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Networking helpers\n",
    "# ===========================\n",
    "\n",
    "def send_json(sock: socket.socket, payload: Dict[str, Any]) -> None:\n",
    "    data = json.dumps(payload)\n",
    "    sock.sendall(data.encode(\"utf-8\"))\n",
    "\n",
    "\n",
    "def connect_all_envs() -> List[socket.socket]:\n",
    "    socks: List[socket.socket] = []\n",
    "    for p in ENV_PORTS:\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        s.connect((HOST, p))\n",
    "        s.setblocking(False)\n",
    "        print(f\"Connected to env on port {p}\")\n",
    "        socks.append(s)\n",
    "    return socks\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  State / RL helpers\n",
    "# ===========================\n",
    "\n",
    "def build_state_vector(message: Dict[str, Any]) -> List[float]:\n",
    "    arena = message.get(\"arena\", {})\n",
    "    tank = message.get(\"tank\", {})\n",
    "    goal = message.get(\"goal\", {})\n",
    "\n",
    "    arena_w = float(arena.get(\"width\", 1.0))\n",
    "    arena_h = float(arena.get(\"height\", 1.0))\n",
    "\n",
    "    tank_x = float(tank.get(\"x\", 0.0))\n",
    "    tank_y = float(tank.get(\"y\", 0.0))\n",
    "    goal_x = float(goal.get(\"x\", 0.0))\n",
    "    goal_y = float(goal.get(\"y\", 0.0))\n",
    "\n",
    "    # NEW: orientation\n",
    "    theta = float(tank.get(\"rot\", 0.0))  # radians from Godot\n",
    "    cos_theta = math.cos(theta)\n",
    "    sin_theta = math.sin(theta)\n",
    "\n",
    "    # Normalize positions to [0, 1]\n",
    "    tank_x_n = tank_x / arena_w\n",
    "    tank_y_n = tank_y / arena_h\n",
    "    goal_x_n = goal_x / arena_w\n",
    "    goal_y_n = goal_y / arena_h\n",
    "\n",
    "    # Relative position tank -> goal, also normalized\n",
    "    dx = (goal_x - tank_x) / arena_w\n",
    "    dy = (goal_y - tank_y) / arena_h\n",
    "\n",
    "    # State: position, relative goal, orientation (cos, sin)\n",
    "    return [tank_x_n, tank_y_n, goal_x_n, goal_y_n, dx, dy, cos_theta, sin_theta]\n",
    "\n",
    "\n",
    "def select_action(state: List[float]) -> int:\n",
    "    global epsilon\n",
    "\n",
    "    epsilon = max(EPSILON_END, epsilon - EPSILON_DECAY)\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(N_ACTIONS)\n",
    "\n",
    "    state_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_vals = q_net(state_t)\n",
    "    return int(torch.argmax(q_vals, dim=1).item())\n",
    "\n",
    "\n",
    "def store_transition(s: List[float], a: int, r: float, s_next: List[float], done: bool) -> None:\n",
    "    replay_buffer.append((s, a, r, s_next, done))\n",
    "\n",
    "\n",
    "def train_step() -> float | None:\n",
    "    global total_steps\n",
    "\n",
    "    if len(replay_buffer) < BATCH_SIZE:\n",
    "        return None\n",
    "\n",
    "    batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "\n",
    "    states = torch.tensor([b[0] for b in batch], dtype=torch.float32, device=device)\n",
    "    actions = torch.tensor([b[1] for b in batch], dtype=torch.int64, device=device)\n",
    "    rewards = torch.tensor([b[2] for b in batch], dtype=torch.float32, device=device)\n",
    "    next_states = torch.tensor([b[3] for b in batch], dtype=torch.float32, device=device)\n",
    "    dones = torch.tensor([b[4] for b in batch], dtype=torch.float32, device=device)\n",
    "\n",
    "    q_values = q_net(states)  # [B, N_ACTIONS]\n",
    "    q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)  # [B]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q = q_net(next_states)\n",
    "        max_next_q = next_q.max(dim=1)[0]\n",
    "        targets = rewards + GAMMA * max_next_q * (1.0 - dones)\n",
    "\n",
    "    loss = loss_fn(q_values, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_steps += 1\n",
    "    if total_steps % 1000 == 0:\n",
    "        save_checkpoint()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Per-env message handling\n",
    "# ===========================\n",
    "\n",
    "def on_message_multi_env(env_idx: int, sock: socket.socket, message: Dict[str, Any]) -> None:\n",
    "    state = build_state_vector(message)\n",
    "    reward = float(message.get(\"reward\", 0.0))\n",
    "    done = bool(message.get(\"done\", False))\n",
    "\n",
    "    episode_returns[env_idx] += reward\n",
    "\n",
    "    prev_s = prev_states[env_idx]\n",
    "    prev_a = prev_actions[env_idx]\n",
    "\n",
    "    if prev_s is not None and prev_a is not None:\n",
    "        store_transition(prev_s, prev_a, reward, state, done)\n",
    "        loss = train_step()\n",
    "    else:\n",
    "        loss = None\n",
    "\n",
    "    action_idx = select_action(state)\n",
    "    prev_states[env_idx] = state\n",
    "    prev_actions[env_idx] = action_idx\n",
    "\n",
    "    action = ACTIONS[action_idx]\n",
    "\n",
    "    state_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_vals = q_net(state_t)\n",
    "        chosen_q = float(q_vals[0, action_idx].item())\n",
    "\n",
    "    response = {\n",
    "        \"value\": chosen_q,\n",
    "        \"action\": {\n",
    "            \"turn\": action[\"turn\"],\n",
    "            \"throttle\": action[\"throttle\"],\n",
    "        },\n",
    "        \"debug\": {\n",
    "            \"env\": env_idx,\n",
    "            \"state\": state,\n",
    "            \"reward\": reward,\n",
    "            \"done\": done,\n",
    "            \"epsilon\": epsilon,\n",
    "            \"loss\": loss,\n",
    "            \"action_idx\": action_idx,\n",
    "            \"episode\": episode_idxs[env_idx],\n",
    "            \"episode_return\": episode_returns[env_idx],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    send_json(sock, response)\n",
    "\n",
    "    if done:\n",
    "        print(f\"[ENV {env_idx}] episode {episode_idxs[env_idx]} return = {episode_returns[env_idx]:.3f}, eps={epsilon:.3f}\")\n",
    "        log_episode_to_file(env_idx, episode_idxs[env_idx], episode_returns[env_idx], epsilon)\n",
    "\n",
    "        episode_idxs[env_idx] += 1\n",
    "        episode_returns[env_idx] = 0.0\n",
    "        prev_states[env_idx] = None\n",
    "        prev_actions[env_idx] = None\n",
    "\n",
    "\n",
    "def _process_buffer_for_env(env_idx: int, sock: socket.socket) -> None:\n",
    "    buf = buffers[env_idx]\n",
    "\n",
    "    while buf:\n",
    "        buf = buf.lstrip()\n",
    "        if not buf:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            obj, idx = decoder.raw_decode(buf)\n",
    "        except json.JSONDecodeError:\n",
    "            break\n",
    "\n",
    "        buf = buf[idx:]\n",
    "        try:\n",
    "            on_message_multi_env(env_idx, sock, obj)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in on_message for env {env_idx}: {e}\")\n",
    "\n",
    "    buffers[env_idx] = buf\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Multi-env receive loop\n",
    "# ===========================\n",
    "\n",
    "def receive_loop_multi_env() -> None:\n",
    "    socks = connect_all_envs()\n",
    "    print(\"All envs connected, starting training loop...\")\n",
    "\n",
    "    while True:\n",
    "        if not socks:\n",
    "            print(\"All envs disconnected, stopping.\")\n",
    "            break\n",
    "\n",
    "        readable, _, _ = select.select(socks, [], [], 0.1)\n",
    "\n",
    "        for s in readable:\n",
    "            env_idx = socks.index(s)\n",
    "            try:\n",
    "                chunk = s.recv(4096)\n",
    "            except BlockingIOError:\n",
    "                continue\n",
    "\n",
    "            if not chunk:\n",
    "                print(f\"Env {env_idx} disconnected.\")\n",
    "                socks.remove(s)\n",
    "                continue\n",
    "\n",
    "            buffers[env_idx] += chunk.decode(\"utf-8\")\n",
    "            _process_buffer_for_env(env_idx, s)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_checkpoint()\n",
    "    receive_loop_multi_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc9a52-a41b-40d9-9095-c8fae3a42f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
