{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0d140e-d46d-455c-a828-bd36e21fbaf9",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Modell "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534403f2-c4a3-4b8e-8810-837c585e13ea",
   "metadata": {},
   "source": [
    "# Godot Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a907490-767a-4d9a-bc88-ef93e5b24bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import json\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "HOST = \"127.0.0.1\"\n",
    "PORT = 5000\n",
    "\n",
    "# ===========================\n",
    "#  RL / model config\n",
    "# ===========================\n",
    "\n",
    "# State vector: tank_x, tank_y, goal_x, goal_y, dx, dy  (all normalized)\n",
    "STATE_SIZE = 6\n",
    "\n",
    "# Discrete actions: (turn, throttle) with values in {-1, 0, 1}\n",
    "# Godot will feed these into setTurn() and setDirection().\n",
    "ACTIONS: List[Dict[str, float]] = [\n",
    "    {\"turn\": -1.0, \"throttle\":  1.0},  # left + forward\n",
    "    {\"turn\":  0.0, \"throttle\":  1.0},  # straight + forward\n",
    "    {\"turn\":  1.0, \"throttle\":  1.0},  # right + forward\n",
    "    {\"turn\":  0.0, \"throttle\":  0.0},  # stop\n",
    "    {\"turn\":  0.0, \"throttle\": -1.0},  # straight + backward\n",
    "]\n",
    "N_ACTIONS = len(ACTIONS)\n",
    "\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 0.2\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_SIZE = 10_000\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Q-network\n",
    "# ===========================\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)  # [batch, output_dim] (Q-values for each action)\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "q_net = QNetwork(STATE_SIZE, N_ACTIONS).to(device)\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "ReplayEntry = Tuple[List[float], int, float, List[float], bool]\n",
    "replay_buffer: deque[ReplayEntry] = deque(maxlen=REPLAY_SIZE)\n",
    "\n",
    "epsilon = EPSILON_START\n",
    "prev_state: List[float] | None = None\n",
    "prev_action_idx: int | None = None\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Networking helpers\n",
    "# ===========================\n",
    "\n",
    "def send_json(sock: socket.socket, payload: Dict[str, Any]) -> None:\n",
    "    data = json.dumps(payload)\n",
    "    sock.sendall(data.encode(\"utf-8\"))\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  State / RL helpers\n",
    "# ===========================\n",
    "\n",
    "def build_state_vector(message: Dict[str, Any]) -> List[float]:\n",
    "    arena = message.get(\"arena\", {})\n",
    "    tank = message.get(\"tank\", {})\n",
    "    goal = message.get(\"goal\", {})\n",
    "\n",
    "    arena_w = float(arena.get(\"width\", 1.0))\n",
    "    arena_h = float(arena.get(\"height\", 1.0))\n",
    "\n",
    "    tank_x = float(tank.get(\"x\", 0.0))\n",
    "    tank_y = float(tank.get(\"y\", 0.0))\n",
    "    goal_x = float(goal.get(\"x\", 0.0))\n",
    "    goal_y = float(goal.get(\"y\", 0.0))\n",
    "\n",
    "    tank_x_n = tank_x / arena_w\n",
    "    tank_y_n = tank_y / arena_h\n",
    "    goal_x_n = goal_x / arena_w\n",
    "    goal_y_n = goal_y / arena_h\n",
    "\n",
    "    dx = (goal_x - tank_x) / arena_w\n",
    "    dy = (goal_y - tank_y) / arena_h\n",
    "\n",
    "    return [tank_x_n, tank_y_n, goal_x_n, goal_y_n, dx, dy]\n",
    "\n",
    "\n",
    "def select_action(state: List[float]) -> int:\n",
    "    global epsilon\n",
    "\n",
    "    epsilon = max(EPSILON_END, epsilon - EPSILON_DECAY)\n",
    "\n",
    "    # Epsilon-greedy\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(N_ACTIONS)\n",
    "\n",
    "    state_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_vals = q_net(state_t)\n",
    "    return int(torch.argmax(q_vals, dim=1).item())\n",
    "\n",
    "\n",
    "def store_transition(s: List[float], a: int, r: float, s_next: List[float], done: bool) -> None:\n",
    "    replay_buffer.append((s, a, r, s_next, done))\n",
    "\n",
    "\n",
    "def train_step() -> float | None:\n",
    "    if len(replay_buffer) < BATCH_SIZE:\n",
    "        return None\n",
    "\n",
    "    batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "\n",
    "    states = torch.tensor([b[0] for b in batch], dtype=torch.float32, device=device)\n",
    "    actions = torch.tensor([b[1] for b in batch], dtype=torch.int64, device=device)\n",
    "    rewards = torch.tensor([b[2] for b in batch], dtype=torch.float32, device=device)\n",
    "    next_states = torch.tensor([b[3] for b in batch], dtype=torch.float32, device=device)\n",
    "    dones = torch.tensor([b[4] for b in batch], dtype=torch.float32, device=device)\n",
    "\n",
    "    q_values = q_net(states)                # [B, N_ACTIONS]\n",
    "    q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)  # [B]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q = q_net(next_states)\n",
    "        max_next_q = next_q.max(dim=1)[0]\n",
    "        targets = rewards + GAMMA * max_next_q * (1.0 - dones)\n",
    "\n",
    "    loss = loss_fn(q_values, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Message handler\n",
    "# ===========================\n",
    "\n",
    "def on_message(sock: socket.socket, message: Dict[str, Any]) -> None:\n",
    "    global prev_state, prev_action_idx\n",
    "\n",
    "    state = build_state_vector(message)\n",
    "    reward = float(message.get(\"reward\", 0.0))\n",
    "    done = bool(message.get(\"done\", False))\n",
    "\n",
    "    # RL: use (prev_state, prev_action) -> current state as next_state\n",
    "    if prev_state is not None and prev_action_idx is not None:\n",
    "        store_transition(prev_state, prev_action_idx, reward, state, done)\n",
    "        loss = train_step()\n",
    "    else:\n",
    "        loss = None\n",
    "\n",
    "    # Choose next action for current state\n",
    "    action_idx = select_action(state)\n",
    "    prev_state = state\n",
    "    prev_action_idx = action_idx\n",
    "\n",
    "    action = ACTIONS[action_idx]\n",
    "\n",
    "    # For debug: Q-value of chosen action\n",
    "    state_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_vals = q_net(state_t)\n",
    "        chosen_q = float(q_vals[0, action_idx].item())\n",
    "\n",
    "    response = {\n",
    "        \"value\": chosen_q,\n",
    "        \"action\": {\n",
    "            \"turn\": action[\"turn\"],        # -1, 0, 1\n",
    "            \"throttle\": action[\"throttle\"] # -1, 0, 1\n",
    "        },\n",
    "        \"debug\": {\n",
    "            \"state\": state,\n",
    "            \"reward\": reward,\n",
    "            \"done\": done,\n",
    "            \"epsilon\": epsilon,\n",
    "            \"loss\": loss,\n",
    "            \"action_idx\": action_idx,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(\"\\nReceived from Godot:\")\n",
    "    print(json.dumps(message, indent=4))\n",
    "    print(\"\\nSending back to Godot:\")\n",
    "    print(json.dumps(response, indent=4))\n",
    "\n",
    "    send_json(sock, response)\n",
    "\n",
    "    if done:\n",
    "        # Episode ended â€“ next step starts fresh\n",
    "        prev_state = None\n",
    "        prev_action_idx = None\n",
    "\n",
    "\n",
    "# ===========================\n",
    "#  Main receive loop\n",
    "# ===========================\n",
    "\n",
    "def receive_loop() -> None:\n",
    "    print(f\"Connecting to {HOST}:{PORT} ...\")\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "        sock.connect((HOST, PORT))\n",
    "        print(\"Connected! Waiting for JSON messages...\\n\")\n",
    "\n",
    "        buffer = \"\"\n",
    "        decoder = json.JSONDecoder()\n",
    "\n",
    "        while True:\n",
    "            chunk = sock.recv(4096)\n",
    "            if not chunk:\n",
    "                print(\"Connection closed by server.\")\n",
    "                break\n",
    "\n",
    "            buffer += chunk.decode(\"utf-8\")\n",
    "\n",
    "            while buffer:\n",
    "                buffer = buffer.lstrip()\n",
    "                if not buffer:\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    obj, idx = decoder.raw_decode(buffer)\n",
    "                except json.JSONDecodeError:\n",
    "                    break\n",
    "\n",
    "                buffer = buffer[idx:]\n",
    "\n",
    "                try:\n",
    "                    on_message(sock, obj)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in on_message: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    receive_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc9a52-a41b-40d9-9095-c8fae3a42f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
