{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f13628d-cd8d-47d5-b1a4-38e1decd53af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIP/ROCm version: 6.4.43482-0f2d60242\n",
      "Torch CUDA available (ROCm uses same API): True\n",
      "Device count: 1\n",
      "Device name: AMD Radeon RX 7900 XTX\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"HIP/ROCm version:\", torch.version.hip)\n",
    "print(\"Torch CUDA available (ROCm uses same API):\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU only\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e995f9c-e223-44b3-9eb6-a676cf21af74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiply OK: torch.Size([5000, 5000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5000, 5000, device=\"cuda\")\n",
    "y = torch.mm(x, x)\n",
    "print(\"Matrix multiply OK:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a124ebe4-23a6-4a5c-b355-98eae787ffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking 4000x4000 matmul on cpu with torch.float32...\n",
      "Time: 0.145 seconds\n",
      "\n",
      "Benchmarking 4000x4000 matmul on cuda with torch.float32...\n",
      "Time: 0.005 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch, time\n",
    "\n",
    "def benchmark_matmul(size=8000, dtype=torch.float32, device=\"cpu\"):\n",
    "    print(f\"\\nBenchmarking {size}x{size} matmul on {device} with {dtype}...\")\n",
    "    x = torch.rand(size, size, device=device, dtype=dtype)\n",
    "    y = torch.rand(size, size, device=device, dtype=dtype)\n",
    "\n",
    "    # Warm-up (important for GPU JIT / caches)\n",
    "    _ = torch.mm(x, y)\n",
    "\n",
    "    if device.startswith(\"cuda\"):\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    start = time.time()\n",
    "    _ = torch.mm(x, y)\n",
    "    if device.startswith(\"cuda\"):\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Time: {end - start:.3f} seconds\")\n",
    "\n",
    "# Run on CPU\n",
    "benchmark_matmul(size=4000, device=\"cpu\")\n",
    "\n",
    "# Run on GPU (ROCm exposes GPU as 'cuda')\n",
    "if torch.cuda.is_available():\n",
    "    benchmark_matmul(size=4000, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13f378-ca19-4d90-961b-f008b1cd85bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Matrix sizes and data types\n",
    "sizes = [2000, 4000, 8000]\n",
    "dtypes = [torch.float32, torch.float16, torch.bfloat16]\n",
    "\n",
    "def benchmark_matmul(size, dtype, device):\n",
    "    x = torch.rand(size, size, device=device, dtype=dtype)\n",
    "    y = torch.rand(size, size, device=device, dtype=dtype)\n",
    "    _ = torch.mm(x, y)  # warm-up\n",
    "    if device.startswith(\"cuda\"):\n",
    "        torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    _ = torch.mm(x, y)\n",
    "    if device.startswith(\"cuda\"):\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    return end - start\n",
    "\n",
    "# Store results\n",
    "results = {dtype: {\"cpu\": [], \"gpu\": []} for dtype in dtypes}\n",
    "\n",
    "for dtype in dtypes:\n",
    "    for size in sizes:\n",
    "        cpu_time = benchmark_matmul(size, dtype, \"cpu\")\n",
    "        gpu_time = benchmark_matmul(size, dtype, \"cuda\") if torch.cuda.is_available() else None\n",
    "        results[dtype][\"cpu\"].append(cpu_time)\n",
    "        results[dtype][\"gpu\"].append(gpu_time)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "markers = {\"cpu\": \"o\", \"gpu\": \"s\"}\n",
    "colors = {\"cpu\": \"blue\", \"gpu\": \"red\"}\n",
    "\n",
    "for dtype in dtypes:\n",
    "    plt.plot(sizes, results[dtype][\"cpu\"], marker=markers[\"cpu\"], color=colors[\"cpu\"], linestyle=\"--\", label=f\"{dtype} CPU\")\n",
    "    plt.plot(sizes, results[dtype][\"gpu\"], marker=markers[\"gpu\"], color=colors[\"gpu\"], linestyle=\"-\", label=f\"{dtype} GPU\")\n",
    "\n",
    "plt.xlabel(\"Matrix size (N x N)\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.title(\"PyTorch MatMul Benchmark: CPU vs GPU (RX 7900 XTX)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721bddad-12a1-4167-89c8-6570a285f334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
