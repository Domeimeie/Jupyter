{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "715729f6-764c-42cb-9e4e-46f7a16b8d0f",
   "metadata": {},
   "source": [
    "# Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50398c67-dc36-4df8-8442-d49ee0039868",
   "metadata": {},
   "source": [
    "## Ausgangslage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05150e37-bdeb-4bf9-af10-f5c052b3cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c6240d9-647a-477c-ba83-2a75674a21a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inputs = np.logspace(-2, 1, 100)   # 50 values from 0.01 to 1 (log-spaced)\n",
    "y_train_inputs = np.log10(x_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3e7192d-9210-43d8-8aee-0fc8c45d7908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize(x_inputs, y_train_inputs):\n",
    "    # Scatter plot (points)\n",
    "    plt.scatter(x_inputs, y_train_inputs, color=\"blue\", label=\"Data points\")\n",
    "\n",
    "    # Line plot (connects points)\n",
    "    plt.plot(x_inputs, y_train_inputs, color=\"orange\", linestyle=\"--\", label=\"Line\")\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel(\"X values\")\n",
    "    plt.ylabel(\"Y values\")\n",
    "    plt.title(\"Visualization of x_inputs vs y_train_inputs\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "visualize(x_inputs, y_train_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a7d1d4-da17-4487-86d1-a88b1287af4e",
   "metadata": {},
   "source": [
    "## Bereitstellung des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "518c3bec-ae1a-44fc-b588-38177888fbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random weight1: -0.18378787856608525\n",
      "Random bias2: -0.270884287843593\n",
      "Random weigh2t: -0.24435235329447502\n",
      "Random bias2: -0.07401364775700536\n"
     ]
    }
   ],
   "source": [
    "weight1 = random.uniform(-0.5, 0.5)\n",
    "print(f\"Random weight1: {weight1}\")\n",
    "bias1   = random.uniform(-0.5, 0.5)\n",
    "print(f\"Random bias2: {bias1}\")\n",
    "weight2 = random.uniform(-0.5, 0.5)\n",
    "print(f\"Random weigh2t: {weight2}\")\n",
    "bias2   = random.uniform(-0.5, 0.5)\n",
    "print(f\"Random bias2: {bias2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c15ed05d-43be-443d-9648-df640be94968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wir werden 10000 Mal trainieren\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "print(f\"Wir werden {epochs} Mal trainieren\")\n",
    "# print(list(range(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a56f425d-9b5b-4bd8-adf6-3fafd0892e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wir trainieren mit einer learning rate von 0.01\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "print(f\"wir trainieren mit einer learning rate von {learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73ebfdd7-2195-4fed-8c58-264621a2378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "package = zip(x_inputs, y_train_inputs)\n",
    "# print(list(package))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad9965-7ab2-450e-a8c9-4a55d49f78a7",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52b70b18-0ba7-465b-a6eb-9d2beca3baff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ich trainiere in Epoche 1\n",
      "Wir haben 0.01 und -2.0 als Trainings Daten\n",
      "Ich sage y vorraus: -0.83193710\n",
      "\u001b[31mIch lag um 1.16806290 daneben\u001b[0m\n",
      "neues weight1: -0.36293634 und neues bias1: -0.47455680\n",
      "neues weight2: -3.21567095 und neues bias2: -2.43523405\n",
      "Wir haben 0.010722672220103232 und -1.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.00425995\n",
      "\u001b[31mIch lag um 0.96543702 daneben\u001b[0m\n",
      "neues weight1: -0.36240241 und neues bias1: -0.42476174\n",
      "neues weight2: -3.20707856 und neues bias2: -2.45454279\n",
      "Wir haben 0.011497569953977356 und -1.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.15751864\n",
      "\u001b[31mIch lag um 0.78187530 daneben\u001b[0m\n",
      "neues weight1: -0.36192011 und neues bias1: -0.38281368\n",
      "neues weight2: -3.20075435 und neues bias2: -2.47018030\n",
      "Wir haben 0.012328467394420659 und -1.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.28907260\n",
      "\u001b[31mIch lag um 0.62001831 daneben\u001b[0m\n",
      "neues weight1: -0.36149741 und neues bias1: -0.34852772\n",
      "neues weight2: -3.19617850 und neues bias2: -2.48258067\n",
      "Wir haben 0.013219411484660288 und -1.878787878787879 als Trainings Daten\n",
      "Ich sage y vorraus: -1.39810243\n",
      "\u001b[31mIch lag um 0.48068545 daneben\u001b[0m\n",
      "neues weight1: -0.36113798 und neues bias1: -0.32133813\n",
      "neues weight2: -3.19291653 und neues bias2: -2.49219438\n",
      "Wir haben 0.014174741629268055 und -1.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -1.48535987\n",
      "\u001b[31mIch lag um 0.36312498 daneben\u001b[0m\n",
      "neues weight1: -0.36084197 und neues bias1: -0.30045534\n",
      "neues weight2: -3.19062642 und neues bias2: -2.49945688\n",
      "Wir haben 0.01519911082952934 und -1.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -1.55267389\n",
      "\u001b[31mIch lag um 0.26550793 daneben\u001b[0m\n",
      "neues weight1: -0.36060713 und neues bias1: -0.28500448\n",
      "neues weight2: -3.18905069 und neues bias2: -2.50476703\n",
      "Wir haben 0.016297508346206444 und -1.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -1.60243814\n",
      "\u001b[31mIch lag um 0.18544065 daneben\u001b[0m\n",
      "neues weight1: -0.36042981 und neues bias1: -0.27412379\n",
      "neues weight2: -3.18800129 und neues bias2: -2.50847585\n",
      "Wir haben 0.01747528400007684 und -1.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -1.63720792\n",
      "\u001b[31mIch lag um 0.12036784 daneben\u001b[0m\n",
      "neues weight1: -0.36030571 und neues bias1: -0.26702236\n",
      "neues weight2: -3.18734337 und neues bias2: -2.51088320\n",
      "Wir haben 0.01873817422860384 und -1.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -1.65943898\n",
      "\u001b[31mIch lag um 0.06783375 daneben\u001b[0m\n",
      "neues weight1: -0.36023046 und neues bias1: -0.26300674\n",
      "neues weight2: -3.18698096 und neues bias2: -2.51223988\n",
      "Wir haben 0.02009233002565047 und -1.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67134720\n",
      "\u001b[31mIch lag um 0.02562249 daneben\u001b[0m\n",
      "neues weight1: -0.36019993 und neues bias1: -0.26148727\n",
      "neues weight2: -3.18684575 und neues bias2: -2.51275233\n",
      "Wir haben 0.021544346900318846 und -1.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67485239\n",
      "\u001b[31mIch lag um -0.00818573 daneben\u001b[0m\n",
      "neues weight1: -0.36021039 und neues bias1: -0.26197294\n",
      "neues weight2: -3.18688879 und neues bias2: -2.51258861\n",
      "Wir haben 0.023101297000831605 und -1.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67157303\n",
      "\u001b[31mIch lag um -0.03520940 daneben\u001b[0m\n",
      "neues weight1: -0.36025863 und neues bias1: -0.26406082\n",
      "neues weight2: -3.18707463 und neues bias2: -2.51188443\n",
      "Wir haben 0.024770763559917114 und -1.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.66284803\n",
      "\u001b[31mIch lag um -0.05678742 daneben\u001b[0m\n",
      "neues weight1: -0.36034193 und neues bias1: -0.26742364\n",
      "neues weight2: -3.18737719 und neues bias2: -2.51074868\n",
      "Wir haben 0.026560877829466867 und -1.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -1.64977028\n",
      "\u001b[31mIch lag um -0.07401270 daneben\u001b[0m\n",
      "neues weight1: -0.36045810 und neues bias1: -0.27179751\n",
      "neues weight2: -3.18777704 und neues bias2: -2.50926842\n",
      "Wir haben 0.02848035868435802 und -1.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -1.63322328\n",
      "\u001b[31mIch lag um -0.08776873 daneben\u001b[0m\n",
      "neues weight1: -0.36060543 und neues bias1: -0.27697065\n",
      "neues weight2: -3.18825944 und neues bias2: -2.50751305\n",
      "Wir haben 0.030538555088334154 und -1.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -1.61391621\n",
      "\u001b[31mIch lag um -0.09876470 daneben\u001b[0m\n",
      "neues weight1: -0.36078265 und neues bias1: -0.28277368\n",
      "neues weight2: -3.18881307 und neues bias2: -2.50553776\n",
      "Wir haben 0.03274549162877728 und -1.4848484848484849 als Trainings Daten\n",
      "Ich sage y vorraus: -1.59241531\n",
      "\u001b[31mIch lag um -0.10756682 daneben\u001b[0m\n",
      "neues weight1: -0.36098887 und neues bias1: -0.28907137\n",
      "neues weight2: -3.18942911 und neues bias2: -2.50338642\n",
      "Wir haben 0.03511191734215131 und -1.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -1.56917082\n",
      "\u001b[31mIch lag um -0.11462537 daneben\u001b[0m\n",
      "neues weight1: -0.36122358 und neues bias1: -0.29575583\n",
      "neues weight2: -3.19010061 und neues bias2: -2.50109391\n",
      "Wir haben 0.037649358067924674 und -1.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -1.54453966\n",
      "\u001b[31mIch lag um -0.12029723 daneben\u001b[0m\n",
      "neues weight1: -0.36148656 und neues bias1: -0.30274096\n",
      "neues weight2: -3.19082203 und neues bias2: -2.49868797\n",
      "Wir haben 0.040370172585965536 und -1.393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.51880396\n",
      "\u001b[31mIch lag um -0.12486457 daneben\u001b[0m\n",
      "neues weight1: -0.36177791 und neues bias1: -0.30995789\n",
      "neues weight2: -3.19158894 und neues bias2: -2.49619068\n",
      "Wir haben 0.04328761281083057 und -1.3636363636363638 als Trainings Daten\n",
      "Ich sage y vorraus: -1.49218627\n",
      "\u001b[31mIch lag um -0.12854991 daneben\u001b[0m\n",
      "neues weight1: -0.36209796 und neues bias1: -0.31735144\n",
      "neues weight2: -3.19239772 und neues bias2: -2.49361968\n",
      "Wir haben 0.046415888336127795 und -1.3333333333333333 als Trainings Daten\n",
      "Ich sage y vorraus: -1.46486170\n",
      "\u001b[31mIch lag um -0.13152837 daneben\u001b[0m\n",
      "neues weight1: -0.36244727 und neues bias1: -0.32487717\n",
      "neues weight2: -3.19324542 und neues bias2: -2.49098911\n",
      "Wir haben 0.049770235643321115 und -1.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.43696771\n",
      "\u001b[31mIch lag um -0.13393740 daneben\u001b[0m\n",
      "neues weight1: -0.36282662 und neues bias1: -0.33249911\n",
      "neues weight2: -3.19412962 und neues bias2: -2.48831036\n",
      "Wir haben 0.0533669923120631 und -1.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -1.40861184\n",
      "\u001b[31mIch lag um -0.13588457 daneben\u001b[0m\n",
      "neues weight1: -0.36323695 und neues bias1: -0.34018791\n",
      "neues weight2: -3.19504827 und neues bias2: -2.48559267\n",
      "Wir haben 0.05722367659350217 und -1.2424242424242424 als Trainings Daten\n",
      "Ich sage y vorraus: -1.37987794\n",
      "\u001b[31mIch lag um -0.13745369 daneben\u001b[0m\n",
      "neues weight1: -0.36367937 und neues bias1: -0.34791938\n",
      "neues weight2: -3.19599965 und neues bias2: -2.48284360\n",
      "Wir haben 0.06135907273413173 und -1.2121212121212122 als Trainings Daten\n",
      "Ich sage y vorraus: -1.35083098\n",
      "\u001b[31mIch lag um -0.13870976 daneben\u001b[0m\n",
      "neues weight1: -0.36415515 und neues bias1: -0.35567338\n",
      "neues weight2: -3.19698226 und neues bias2: -2.48006940\n",
      "Wir haben 0.06579332246575682 und -1.1818181818181817 als Trainings Daten\n",
      "Ich sage y vorraus: -1.32152093\n",
      "\u001b[31mIch lag um -0.13970275 daneben\u001b[0m\n",
      "neues weight1: -0.36466567 und neues bias1: -0.36343285\n",
      "neues weight2: -3.19799479 und neues bias2: -2.47727535\n",
      "Wir haben 0.07054802310718646 und -1.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: -1.29198579\n",
      "\u001b[31mIch lag um -0.14047064 daneben\u001b[0m\n",
      "neues weight1: -0.36521244 und neues bias1: -0.37118314\n",
      "neues weight2: -3.19903606 und neues bias2: -2.47446593\n",
      "Wir haben 0.07564633275546291 und -1.121212121212121 als Trainings Daten\n",
      "Ich sage y vorraus: -1.26225394\n",
      "\u001b[31mIch lag um -0.14104182 daneben\u001b[0m\n",
      "neues weight1: -0.36579705 und neues bias1: -0.37891136\n",
      "neues weight2: -3.20010496 und neues bias2: -2.47164510\n",
      "Wir haben 0.08111308307896872 und -1.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -1.23234604\n",
      "\u001b[31mIch lag um -0.14143695 daneben\u001b[0m\n",
      "neues weight1: -0.36642118 und neues bias1: -0.38660599\n",
      "neues weight2: -3.20120044 und neues bias2: -2.46881636\n",
      "Wir haben 0.08697490026177834 und -1.0606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.20227646\n",
      "\u001b[31mIch lag um -0.14167040 daneben\u001b[0m\n",
      "neues weight1: -0.36708658 und neues bias1: -0.39425648\n",
      "neues weight2: -3.20232146 und neues bias2: -2.46598295\n",
      "Wir haben 0.093260334688322 und -1.0303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.17205446\n",
      "\u001b[31mIch lag um -0.14175143 daneben\u001b[0m\n",
      "neues weight1: -0.36779503 und neues bias1: -0.40185293\n",
      "neues weight2: -3.20346698 und neues bias2: -2.46314792\n",
      "Wir haben 0.1 und -1.0 als Trainings Daten\n",
      "Ich sage y vorraus: -1.14168503\n",
      "\u001b[31mIch lag um -0.14168503 daneben\u001b[0m\n",
      "neues weight1: -0.36854833 und neues bias1: -0.40938590\n",
      "neues weight2: -3.20463591 und neues bias2: -2.46031422\n",
      "Wir haben 0.10722672220103231 und -0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.11116961\n",
      "\u001b[31mIch lag um -0.14147264 daneben\u001b[0m\n",
      "neues weight1: -0.36934827 und neues bias1: -0.41684617\n",
      "neues weight2: -3.20582711 und neues bias2: -2.45748477\n",
      "Wir haben 0.11497569953977356 und -0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.08050666\n",
      "\u001b[31mIch lag um -0.14111272 daneben\u001b[0m\n",
      "neues weight1: -0.37019661 und neues bias1: -0.42422463\n",
      "neues weight2: -3.20703933 und neues bias2: -2.45466251\n",
      "Wir haben 0.12328467394420659 und -0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.04969200\n",
      "\u001b[31mIch lag um -0.14060109 daneben\u001b[0m\n",
      "neues weight1: -0.37109504 und neues bias1: -0.43151208\n",
      "neues weight2: -3.20827125 und neues bias2: -2.45185049\n",
      "Wir haben 0.13219411484660293 und -0.8787878787878787 als Trainings Daten\n",
      "Ich sage y vorraus: -1.01871920\n",
      "\u001b[31mIch lag um -0.13993132 daneben\u001b[0m\n",
      "neues weight1: -0.37204514 und neues bias1: -0.43869922\n",
      "neues weight2: -3.20952139 und neues bias2: -2.44905187\n",
      "Wir haben 0.14174741629268056 und -0.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -0.98757977\n",
      "\u001b[31mIch lag um -0.13909493 daneben\u001b[0m\n",
      "neues weight1: -0.37304832 und neues bias1: -0.44577646\n",
      "neues weight2: -3.21078814 und neues bias2: -2.44626997\n",
      "Wir haben 0.1519911082952934 und -0.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -0.95626343\n",
      "\u001b[31mIch lag um -0.13808161 daneben\u001b[0m\n",
      "neues weight1: -0.37410579 und neues bias1: -0.45273392\n",
      "neues weight2: -3.21206971 und neues bias2: -2.44350834\n",
      "Wir haben 0.16297508346206444 und -0.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -0.92475817\n",
      "\u001b[31mIch lag um -0.13687938 daneben\u001b[0m\n",
      "neues weight1: -0.37521850 und neues bias1: -0.45956137\n",
      "neues weight2: -3.21336412 und neues bias2: -2.44077075\n",
      "Wir haben 0.17475284000076838 und -0.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -0.89305044\n",
      "\u001b[31mIch lag um -0.13547468 daneben\u001b[0m\n",
      "neues weight1: -0.37638703 und neues bias1: -0.46624814\n",
      "neues weight2: -3.21466915 und neues bias2: -2.43806125\n",
      "Wir haben 0.1873817422860384 und -0.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -0.86112526\n",
      "\u001b[31mIch lag um -0.13385254 daneben\u001b[0m\n",
      "neues weight1: -0.37761157 und neues bias1: -0.47278312\n",
      "neues weight2: -3.21598236 und neues bias2: -2.43538420\n",
      "Wir haben 0.20092330025650468 und -0.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -0.82896630\n",
      "\u001b[31mIch lag um -0.13199660 daneben\u001b[0m\n",
      "neues weight1: -0.37889177 und neues bias1: -0.47915476\n",
      "neues weight2: -3.21730103 und neues bias2: -2.43274427\n",
      "Wir haben 0.21544346900318845 und -0.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -0.79655598\n",
      "\u001b[31mIch lag um -0.12988932 daneben\u001b[0m\n",
      "neues weight1: -0.38022672 und neues bias1: -0.48535100\n",
      "neues weight2: -3.21862216 und neues bias2: -2.43014648\n",
      "Wir haben 0.23101297000831605 und -0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -0.76387566\n",
      "\u001b[31mIch lag um -0.12751202 daneben\u001b[0m\n",
      "neues weight1: -0.38161473 und neues bias1: -0.49135936\n",
      "neues weight2: -3.21994241 und neues bias2: -2.42759624\n",
      "Wir haben 0.24770763559917114 und -0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -0.73090571\n",
      "\u001b[31mIch lag um -0.12484511 daneben\u001b[0m\n",
      "neues weight1: -0.38305330 und neues bias1: -0.49716691\n",
      "neues weight2: -3.22125811 und neues bias2: -2.42509934\n",
      "Wir haben 0.26560877829466867 und -0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -0.69762576\n",
      "\u001b[31mIch lag um -0.12186819 daneben\u001b[0m\n",
      "neues weight1: -0.38453896 und neues bias1: -0.50276032\n",
      "neues weight2: -3.22256520 und neues bias2: -2.42266198\n",
      "Wir haben 0.2848035868435802 und -0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -0.66401491\n",
      "\u001b[31mIch lag um -0.11856036 daneben\u001b[0m\n",
      "neues weight1: -0.38606711 und neues bias1: -0.50812594\n",
      "neues weight2: -3.22385924 und neues bias2: -2.42029077\n",
      "Wir haben 0.30538555088334157 und -0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -0.63005199\n",
      "\u001b[31mIch lag um -0.11490048 daneben\u001b[0m\n",
      "neues weight1: -0.38763188 und neues bias1: -0.51324987\n",
      "neues weight2: -3.22513534 und neues bias2: -2.41799276\n",
      "Wir haben 0.32745491628777285 und -0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: -0.59571600\n",
      "\u001b[31mIch lag um -0.11086751 daneben\u001b[0m\n",
      "neues weight1: -0.38922600 und neues bias1: -0.51811807\n",
      "neues weight2: -3.22638820 und neues bias2: -2.41577541\n",
      "Wir haben 0.3511191734215131 und -0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -0.56098646\n",
      "\u001b[31mIch lag um -0.10644100 daneben\u001b[0m\n",
      "neues weight1: -0.39084061 und neues bias1: -0.52271655\n",
      "neues weight2: -3.22761202 und neues bias2: -2.41364659\n",
      "Wir haben 0.37649358067924676 und -0.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.52584401\n",
      "\u001b[31mIch lag um -0.10160159 daneben\u001b[0m\n",
      "neues weight1: -0.39246515 und neues bias1: -0.52703147\n",
      "neues weight2: -3.22880053 und neues bias2: -2.41161456\n",
      "Wir haben 0.4037017258596556 und -0.3939393939393938 als Trainings Daten\n",
      "Ich sage y vorraus: -0.49027102\n",
      "\u001b[31mIch lag um -0.09633163 daneben\u001b[0m\n",
      "neues weight1: -0.39408720 und neues bias1: -0.53104942\n",
      "neues weight2: -3.22994700 und neues bias2: -2.40968793\n",
      "Wir haben 0.43287612810830595 und -0.36363636363636354 als Trainings Daten\n",
      "Ich sage y vorraus: -0.45425234\n",
      "\u001b[31mIch lag um -0.09061597 daneben\u001b[0m\n",
      "neues weight1: -0.39569240 und neues bias1: -0.53475763\n",
      "neues weight2: -3.23104420 und neues bias2: -2.40787561\n",
      "Wir haben 0.464158883361278 und -0.33333333333333326 als Trainings Daten\n",
      "Ich sage y vorraus: -0.41777610\n",
      "\u001b[31mIch lag um -0.08444277 daneben\u001b[0m\n",
      "neues weight1: -0.39726433 und neues bias1: -0.53814426\n",
      "neues weight2: -3.23208442 und neues bias2: -2.40618675\n",
      "Wir haben 0.49770235643321115 und -0.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -0.38083476\n",
      "\u001b[31mIch lag um -0.07780446 daneben\u001b[0m\n",
      "neues weight1: -0.39878455 und neues bias1: -0.54119873\n",
      "neues weight2: -3.23305952 und neues bias2: -2.40463066\n",
      "Wir haben 0.533669923120631 und -0.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -0.34342605\n",
      "\u001b[31mIch lag um -0.07069878 daneben\u001b[0m\n",
      "neues weight1: -0.40023259 und neues bias1: -0.54391209\n",
      "neues weight2: -3.23396099 und neues bias2: -2.40321669\n",
      "Wir haben 0.5722367659350217 und -0.24242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.30555414\n",
      "\u001b[31mIch lag um -0.06312990 daneben\u001b[0m\n",
      "neues weight1: -0.40158609 und neues bias1: -0.54627737\n",
      "neues weight2: -3.23477995 und neues bias2: -2.40195409\n",
      "Wir haben 0.6135907273413173 und -0.21212121212121215 als Trainings Daten\n",
      "Ich sage y vorraus: -0.26723076\n",
      "\u001b[31mIch lag um -0.05510955 daneben\u001b[0m\n",
      "neues weight1: -0.40282101 und neues bias1: -0.54828999\n",
      "neues weight2: -3.23550732 und neues bias2: -2.40085190\n",
      "Wir haben 0.6579332246575682 und -0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: -0.22847634\n",
      "\u001b[31mIch lag um -0.04665816 daneben\u001b[0m\n",
      "neues weight1: -0.40391198 und neues bias1: -0.54994816\n",
      "neues weight2: -3.23613386 und neues bias2: -2.39991874\n",
      "Wir haben 0.7054802310718645 und -0.15151515151515138 als Trainings Daten\n",
      "Ich sage y vorraus: -0.18932103\n",
      "\u001b[31mIch lag um -0.03780588 daneben\u001b[0m\n",
      "neues weight1: -0.40483272 und neues bias1: -0.55125328\n",
      "neues weight2: -3.23665037 und neues bias2: -2.39916262\n",
      "Wir haben 0.7564633275546291 und -0.12121212121212109 als Trainings Daten\n",
      "Ich sage y vorraus: -0.14980564\n",
      "\u001b[31mIch lag um -0.02859352 daneben\u001b[0m\n",
      "neues weight1: -0.40555664 und neues bias1: -0.55221026\n",
      "neues weight2: -3.23704780 und neues bias2: -2.39859075\n",
      "Wir haben 0.8111308307896873 und -0.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -0.10998225\n",
      "\u001b[31mIch lag um -0.01907316 daneben\u001b[0m\n",
      "neues weight1: -0.40605758 und neues bias1: -0.55282785\n",
      "neues weight2: -3.23731749 und neues bias2: -2.39820928\n",
      "Wir haben 0.8697490026177834 und -0.06060606060606055 als Trainings Daten\n",
      "Ich sage y vorraus: -0.06991462\n",
      "\u001b[31mIch lag um -0.00930856 daneben\u001b[0m\n",
      "neues weight1: -0.40631063 und neues bias1: -0.55311880\n",
      "neues weight2: -3.23745139 und neues bias2: -2.39802311\n",
      "Wir haben 0.9326033468832199 und -0.030303030303030297 als Trainings Daten\n",
      "Ich sage y vorraus: -0.02967809\n",
      "\u001b[31mIch lag um 0.00062494 daneben\u001b[0m\n",
      "neues weight1: -0.40629309 und neues bias1: -0.55309999\n",
      "neues weight2: -3.23744224 und neues bias2: -2.39803561\n",
      "Wir haben 1.0 und 0.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.01064093\n",
      "\u001b[31mIch lag um 0.01064093 daneben\u001b[0m\n",
      "neues weight1: -0.40598549 und neues bias1: -0.55279239\n",
      "neues weight2: -3.23728391 und neues bias2: -2.39824843\n",
      "Wir haben 1.072267222010323 und 0.030303030303030252 als Trainings Daten\n",
      "Ich sage y vorraus: 0.05094590\n",
      "\u001b[31mIch lag um 0.02064287 daneben\u001b[0m\n",
      "neues weight1: -0.40537266 und neues bias1: -0.55222086\n",
      "neues weight2: -3.23697155 und neues bias2: -2.39866129\n",
      "Wir haben 1.1497569953977356 und 0.060606060606060524 als Trainings Daten\n",
      "Ich sage y vorraus: 0.09113171\n",
      "\u001b[31mIch lag um 0.03052565 daneben\u001b[0m\n",
      "neues weight1: -0.40444477 und neues bias1: -0.55141383\n",
      "neues weight2: -3.23650196 und neues bias2: -2.39927180\n",
      "Wir haben 1.232846739442066 und 0.09090909090909083 als Trainings Daten\n",
      "Ich sage y vorraus: 0.13108658\n",
      "\u001b[31mIch lag um 0.04017749 daneben\u001b[0m\n",
      "neues weight1: -0.40319831 und neues bias1: -0.55040278\n",
      "neues weight2: -3.23587373 und neues bias2: -2.40007535\n",
      "Wir haben 1.3219411484660286 und 0.12121212121212108 als Trainings Daten\n",
      "Ich sage y vorraus: 0.17069432\n",
      "\u001b[31mIch lag um 0.04948220 daneben\u001b[0m\n",
      "neues weight1: -0.40163691 und neues bias1: -0.54922164\n",
      "neues weight2: -3.23508750 und neues bias2: -2.40106499\n",
      "Wir haben 1.4174741629268048 und 0.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: 0.20983679\n",
      "\u001b[31mIch lag um 0.05832164 daneben\u001b[0m\n",
      "neues weight1: -0.39977199 und neues bias1: -0.54790598\n",
      "neues weight2: -3.23414612 und neues bias2: -2.40223143\n",
      "Wir haben 1.5199110829529332 und 0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: 0.24839634\n",
      "\u001b[31mIch lag um 0.06657816 daneben\u001b[0m\n",
      "neues weight1: -0.39762314 und neues bias1: -0.54649218\n",
      "neues weight2: -3.23305481 und neues bias2: -2.40356299\n",
      "Wir haben 1.629750834620645 und 0.21212121212121235 als Trainings Daten\n",
      "Ich sage y vorraus: 0.28625805\n",
      "\u001b[31mIch lag um 0.07413684 daneben\u001b[0m\n",
      "neues weight1: -0.39521827 und neues bias1: -0.54501658\n",
      "neues weight2: -3.23182121 und neues bias2: -2.40504573\n",
      "Wir haben 1.7475284000076847 und 0.24242424242424265 als Trainings Daten\n",
      "Ich sage y vorraus: 0.32331147\n",
      "\u001b[31mIch lag um 0.08088723 daneben\u001b[0m\n",
      "neues weight1: -0.39259336 und neues bias1: -0.54351450\n",
      "neues weight2: -3.23045548 und neues bias2: -2.40666347\n",
      "Wir haben 1.873817422860385 und 0.27272727272727293 als Trainings Daten\n",
      "Ich sage y vorraus: 0.35945181\n",
      "\u001b[31mIch lag um 0.08672454 daneben\u001b[0m\n",
      "neues weight1: -0.38979196 und neues bias1: -0.54201948\n",
      "neues weight2: -3.22897030 und neues bias2: -2.40839796\n",
      "Wir haben 2.0092330025650478 und 0.30303030303030315 als Trainings Daten\n",
      "Ich sage y vorraus: 0.39458030\n",
      "\u001b[31mIch lag um 0.09155000 daneben\u001b[0m\n",
      "neues weight1: -0.38686435 und neues bias1: -0.54056240\n",
      "neues weight2: -3.22738086 und neues bias2: -2.41022896\n",
      "Wir haben 2.1544346900318843 und 0.3333333333333334 als Trainings Daten\n",
      "Ich sage y vorraus: 0.42860374\n",
      "\u001b[31mIch lag um 0.09527041 daneben\u001b[0m\n",
      "neues weight1: -0.38386634 und neues bias1: -0.53917085\n",
      "neues weight2: -3.22570485 und neues bias2: -2.41213437\n",
      "Wir haben 2.31012970008316 und 0.3636363636363637 als Trainings Daten\n",
      "Ich sage y vorraus: 0.46143323\n",
      "\u001b[31mIch lag um 0.09779686 daneben\u001b[0m\n",
      "neues weight1: -0.38085779 und neues bias1: -0.53786852\n",
      "neues weight2: -3.22396243 und neues bias2: -2.41409031\n",
      "Wir haben 2.4770763559917115 und 0.39393939393939403 als Trainings Daten\n",
      "Ich sage y vorraus: 0.49298230\n",
      "\u001b[31mIch lag um 0.09904291 daneben\u001b[0m\n",
      "neues weight1: -0.37790083 und neues bias1: -0.53667479\n",
      "neues weight2: -3.22217628 und neues bias2: -2.41607117\n",
      "Wir haben 2.656087782946687 und 0.42424242424242437 als Trainings Daten\n",
      "Ich sage y vorraus: 0.52316476\n",
      "\u001b[31mIch lag um 0.09892233 daneben\u001b[0m\n",
      "neues weight1: -0.37505773 und neues bias1: -0.53560438\n",
      "neues weight2: -3.22037156 und neues bias2: -2.41804961\n",
      "Wir haben 2.848035868435802 und 0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: 0.55189267\n",
      "\u001b[31mIch lag um 0.09734721 daneben\u001b[0m\n",
      "neues weight1: -0.37238847 und neues bias1: -0.53466715\n",
      "neues weight2: -3.21857602 und neues bias2: -2.41999656\n",
      "Wir haben 3.0538555088334154 und 0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.57907524\n",
      "\u001b[31mIch lag um 0.09422675 daneben\u001b[0m\n",
      "neues weight1: -0.36994809 und neues bias1: -0.53386804\n",
      "neues weight2: -3.21682001 und neues bias2: -2.42188109\n",
      "Wir haben 3.2745491628777286 und 0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: 0.60461928\n",
      "\u001b[31mIch lag um 0.08946777 daneben\u001b[0m\n",
      "neues weight1: -0.36778377 und neues bias1: -0.53320709\n",
      "neues weight2: -3.21513652 und neues bias2: -2.42367045\n",
      "Wir haben 3.511191734215131 und 0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: 0.62843214\n",
      "\u001b[31mIch lag um 0.08297759 daneben\u001b[0m\n",
      "neues weight1: -0.36593195 und neues bias1: -0.53267968\n",
      "neues weight2: -3.21356112 und neues bias2: -2.42533000\n",
      "Wir haben 3.7649358067924674 und 0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: 0.65042771\n",
      "\u001b[31mIch lag um 0.07467013 daneben\u001b[0m\n",
      "neues weight1: -0.36441556 und neues bias1: -0.53227691\n",
      "neues weight2: -3.21213175 und neues bias2: -2.42682340\n",
      "Wir haben 4.037017258596554 und 0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: 0.67053577\n",
      "\u001b[31mIch lag um 0.06447516 daneben\u001b[0m\n",
      "neues weight1: -0.36324195 und neues bias1: -0.53198620\n",
      "neues weight2: -3.21088833 und neues bias2: -2.42811290\n",
      "Wir haben 4.328761281083057 und 0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: 0.68871412\n",
      "\u001b[31mIch lag um 0.05235048 daneben\u001b[0m\n",
      "neues weight1: -0.36240182 und neues bias1: -0.53179212\n",
      "neues weight2: -3.20987199 und neues bias2: -2.42915991\n",
      "Wir haben 4.641588833612782 und 0.666666666666667 als Trainings Daten\n",
      "Ich sage y vorraus: 0.70496203\n",
      "\u001b[31mIch lag um 0.03829536 daneben\u001b[0m\n",
      "neues weight1: -0.36186959 und neues bias1: -0.53167746\n",
      "neues weight2: -3.20912416 und neues bias2: -2.42992582\n",
      "Wir haben 4.9770235643321135 und 0.6969696969696972 als Trainings Daten\n",
      "Ich sage y vorraus: 0.71933241\n",
      "\u001b[31mIch lag um 0.02236271 daneben\u001b[0m\n",
      "neues weight1: -0.36160555 und neues bias1: -0.53162441\n",
      "neues weight2: -3.20868525 und neues bias2: -2.43037308\n",
      "Wir haben 5.336699231206313 und 0.7272727272727275 als Trainings Daten\n",
      "Ich sage y vorraus: 0.73193986\n",
      "\u001b[31mIch lag um 0.00466713 daneben\u001b[0m\n",
      "neues weight1: -0.36155968 und neues bias1: -0.53161581\n",
      "neues weight2: -3.20859325 und neues bias2: -2.43046642\n",
      "Wir haben 5.72236765935022 und 0.7575757575757578 als Trainings Daten\n",
      "Ich sage y vorraus: 0.74296175\n",
      "\u001b[31mIch lag um -0.01461401 daneben\u001b[0m\n",
      "neues weight1: -0.36167667 und neues bias1: -0.53163625\n",
      "neues weight2: -3.20888233 und neues bias2: -2.43017414\n",
      "Wir haben 6.135907273413176 und 0.7878787878787881 als Trainings Daten\n",
      "Ich sage y vorraus: 0.75263123\n",
      "\u001b[31mIch lag um -0.03524756 daneben\u001b[0m\n",
      "neues weight1: -0.36190135 und neues bias1: -0.53167287\n",
      "neues weight2: -3.20958155 und neues bias2: -2.42946919\n",
      "Wir haben 6.5793322465756825 und 0.8181818181818183 als Trainings Daten\n",
      "Ich sage y vorraus: 0.76122276\n",
      "\u001b[31mIch lag um -0.05695906 daneben\u001b[0m\n",
      "neues weight1: -0.36218367 und neues bias1: -0.53171578\n",
      "neues weight2: -3.21071403 und neues bias2: -2.42833001\n",
      "Wir haben 7.054802310718645 und 0.8484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.76903271\n",
      "\u001b[31mIch lag um -0.07945214 daneben\u001b[0m\n",
      "neues weight1: -0.36248239 und neues bias1: -0.53175812\n",
      "neues weight2: -3.21229646 und neues bias2: -2.42674096\n",
      "Wir haben 7.56463327554629 und 0.8787878787878789 als Trainings Daten\n",
      "Ich sage y vorraus: 0.77635878\n",
      "\u001b[31mIch lag um -0.10242910 daneben\u001b[0m\n",
      "neues weight1: -0.36276702 und neues bias1: -0.53179575\n",
      "neues weight2: -3.21433918 und neues bias2: -2.42469238\n",
      "Wir haben 8.111308307896872 und 0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: 0.78348146\n",
      "\u001b[31mIch lag um -0.12560945 daneben\u001b[0m\n",
      "neues weight1: -0.36301805 und neues bias1: -0.53182670\n",
      "neues weight2: -3.21684655 und neues bias2: -2.42218019\n",
      "Wir haben 8.697490026177835 und 0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79065016\n",
      "\u001b[31mIch lag um -0.14874378 daneben\u001b[0m\n",
      "neues weight1: -0.36322575 und neues bias1: -0.53185058\n",
      "neues weight2: -3.21981771 und neues bias2: -2.41920532\n",
      "Wir haben 9.326033468832199 und 0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79807479\n",
      "\u001b[31mIch lag um -0.17162218 daneben\u001b[0m\n",
      "neues weight1: -0.36338815 und neues bias1: -0.53186799\n",
      "neues weight2: -3.22324745 und neues bias2: -2.41577287\n",
      "Wir haben 10.0 und 1.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.80592256\n",
      "\u001b[31mIch lag um -0.19407744 daneben\u001b[0m\n",
      "neues weight1: -0.36350860 und neues bias1: -0.53188004\n",
      "neues weight2: -3.22712713 und neues bias2: -2.41189132\n",
      "\n",
      "\n",
      "Ich trainiere in Epoche 1001\n",
      "Wir haben 0.01 und -2.0 als Trainings Daten\n",
      "Ich sage y vorraus: -0.83131232\n",
      "\u001b[31mIch lag um 1.16868768 daneben\u001b[0m\n",
      "neues weight1: -0.36209175 und neues bias1: -0.47812633\n",
      "neues weight2: -3.23286236 und neues bias2: -2.45215919\n",
      "Wir haben 0.010722672220103232 und -1.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.00431840\n",
      "\u001b[31mIch lag um 0.96537857 daneben\u001b[0m\n",
      "neues weight1: -0.36155670 und neues bias1: -0.42822697\n",
      "neues weight2: -3.22421544 und neues bias2: -2.47146676\n",
      "Wir haben 0.011497569953977356 und -1.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.15820602\n",
      "\u001b[31mIch lag um 0.78118792 daneben\u001b[0m\n",
      "neues weight1: -0.36107360 und neues bias1: -0.38620986\n",
      "neues weight2: -3.21785170 und neues bias2: -2.48709052\n",
      "Wir haben 0.012328467394420659 und -1.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.29027421\n",
      "\u001b[31mIch lag um 0.61881670 daneben\u001b[0m\n",
      "neues weight1: -0.36065054 und neues bias1: -0.35189375\n",
      "neues weight2: -3.21324857 und neues bias2: -2.49946685\n",
      "Wir haben 0.013219411484660288 und -1.878787878787879 als Trainings Daten\n",
      "Ich sage y vorraus: -1.39966868\n",
      "\u001b[31mIch lag um 0.47911920 daneben\u001b[0m\n",
      "neues weight1: -0.36029119 und neues bias1: -0.32471025\n",
      "neues weight2: -3.20996881 und neues bias2: -2.50904924\n",
      "Wir haben 0.014174741629268055 und -1.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -1.48713452\n",
      "\u001b[31mIch lag um 0.36135033 daneben\u001b[0m\n",
      "neues weight1: -0.35999568 und neues bias1: -0.30386297\n",
      "neues weight2: -3.20766804 und neues bias2: -2.51627625\n",
      "Wir haben 0.01519911082952934 und -1.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -1.55451593\n",
      "\u001b[31mIch lag um 0.26366588 daneben\u001b[0m\n",
      "neues weight1: -0.35976170 und neues bias1: -0.28846856\n",
      "neues weight2: -3.20608694 und neues bias2: -2.52154956\n",
      "Wir haben 0.016297508346206444 und -1.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -1.60423393\n",
      "\u001b[31mIch lag um 0.18364486 daneben\u001b[0m\n",
      "neues weight1: -0.35958550 und neues bias1: -0.27765692\n",
      "neues weight2: -3.20503606 und neues bias2: -2.52522246\n",
      "Wir haben 0.01747528400007684 und -1.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -1.63887478\n",
      "\u001b[31mIch lag um 0.11870097 daneben\u001b[0m\n",
      "neues weight1: -0.35946270 und neues bias1: -0.27063002\n",
      "neues weight2: -3.20437953 und neues bias2: -2.52759648\n",
      "Wir haben 0.01873817422860384 und -1.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -1.66092326\n",
      "\u001b[31mIch lag um 0.06634947 daneben\u001b[0m\n",
      "neues weight1: -0.35938885 und neues bias1: -0.26668889\n",
      "neues weight2: -3.20402062 und neues bias2: -2.52892347\n",
      "Wir haben 0.02009233002565047 und -1.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67261949\n",
      "\u001b[31mIch lag um 0.02435021 daneben\u001b[0m\n",
      "neues weight1: -0.35935974 und neues bias1: -0.26523998\n",
      "neues weight2: -3.20389047 und neues bias2: -2.52941047\n",
      "Wir haben 0.021544346900318846 und -1.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67590193\n",
      "\u001b[31mIch lag um -0.00923526 daneben\u001b[0m\n",
      "neues weight1: -0.35937158 und neues bias1: -0.26578975\n",
      "neues weight2: -3.20393967 und neues bias2: -2.52922577\n",
      "Wir haben 0.023101297000831605 und -1.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67240243\n",
      "\u001b[31mIch lag um -0.03603879 daneben\u001b[0m\n",
      "neues weight1: -0.35942112 und neues bias1: -0.26793392\n",
      "neues weight2: -3.20413243 und neues bias2: -2.52850499\n",
      "Wir haben 0.024770763559917114 und -1.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.66346875\n",
      "\u001b[31mIch lag um -0.05740815 daneben\u001b[0m\n",
      "neues weight1: -0.35950560 und neues bias1: -0.27134465\n",
      "neues weight2: -3.20444240 und neues bias2: -2.52735683\n",
      "Wir haben 0.026560877829466867 und -1.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -1.65019914\n",
      "\u001b[31mIch lag um -0.07444157 daneben\u001b[0m\n",
      "neues weight1: -0.35962283 und neues bias1: -0.27575804\n",
      "neues weight2: -3.20484994 und neues bias2: -2.52586800\n",
      "Wir haben 0.02848035868435802 und -1.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -1.63347982\n",
      "\u001b[31mIch lag um -0.08802528 daneben\u001b[0m\n",
      "neues weight1: -0.35977106 und neues bias1: -0.28096274\n",
      "neues weight2: -3.20534015 und neues bias2: -2.52410749\n",
      "Wir haben 0.030538555088334154 und -1.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -1.61402083\n",
      "\u001b[31mIch lag um -0.09886932 daneben\u001b[0m\n",
      "neues weight1: -0.35994901 und neues bias1: -0.28678998\n",
      "neues weight2: -3.20590159 und neues bias2: -2.52213011\n",
      "Wir haben 0.03274549162877728 und -1.4848484848484849 als Trainings Daten\n",
      "Ich sage y vorraus: -1.59238804\n",
      "\u001b[31mIch lag um -0.10753955 daneben\u001b[0m\n",
      "neues weight1: -0.36015581 und neues bias1: -0.29310528\n",
      "neues weight2: -3.20652534 und neues bias2: -2.51997931\n",
      "Wir haben 0.03511191734215131 und -1.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -1.56903055\n",
      "\u001b[31mIch lag um -0.11448509 daneben\u001b[0m\n",
      "neues weight1: -0.36039093 und neues bias1: -0.29980152\n",
      "neues weight2: -3.20720439 und neues bias2: -2.51768961\n",
      "Wir haben 0.037649358067924674 und -1.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -1.54430369\n",
      "\u001b[31mIch lag um -0.12006126 daneben\u001b[0m\n",
      "neues weight1: -0.36065417 und neues bias1: -0.30679337\n",
      "neues weight2: -3.20793316 und neues bias2: -2.51528839\n",
      "Wir haben 0.040370172585965536 und -1.393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.51848785\n",
      "\u001b[31mIch lag um -0.12454846 daneben\u001b[0m\n",
      "neues weight1: -0.36094561 und neues bias1: -0.31401269\n",
      "neues weight2: -3.20870718 und neues bias2: -2.51279742\n",
      "Wir haben 0.04328761281083057 und -1.3636363636363638 als Trainings Daten\n",
      "Ich sage y vorraus: -1.49180376\n",
      "\u001b[31mIch lag um -0.12816739 daneben\u001b[0m\n",
      "neues weight1: -0.36126561 und neues bias1: -0.32140495\n",
      "neues weight2: -3.20952282 und neues bias2: -2.51023407\n",
      "Wir haben 0.046415888336127795 und -1.3333333333333333 als Trainings Daten\n",
      "Ich sage y vorraus: -1.46442478\n",
      "\u001b[31mIch lag um -0.13109145 daneben\u001b[0m\n",
      "neues weight1: -0.36161472 und neues bias1: -0.32892633\n",
      "neues weight2: -3.21037713 und neues bias2: -2.50761224\n",
      "Wir haben 0.049770235643321115 und -1.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.43648676\n",
      "\u001b[31mIch lag um -0.13345646 daneben\u001b[0m\n",
      "neues weight1: -0.36199372 und neues bias1: -0.33654136\n",
      "neues weight2: -3.21126768 und neues bias2: -2.50494311\n",
      "Wir haben 0.0533669923120631 und -1.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -1.40809578\n",
      "\u001b[31mIch lag um -0.13536851 daneben\u001b[0m\n",
      "neues weight1: -0.36240357 und neues bias1: -0.34422115\n",
      "neues weight2: -3.21219241 und neues bias2: -2.50223574\n",
      "Wir haben 0.05722367659350217 und -1.2424242424242424 als Trainings Daten\n",
      "Ich sage y vorraus: -1.37933439\n",
      "\u001b[31mIch lag um -0.13691015 daneben\u001b[0m\n",
      "neues weight1: -0.36284538 und neues bias1: -0.35194194\n",
      "neues weight2: -3.21314962 und neues bias2: -2.49949754\n",
      "Wir haben 0.06135907273413173 und -1.2121212121212122 als Trainings Daten\n",
      "Ich sage y vorraus: -1.35026641\n",
      "\u001b[31mIch lag um -0.13814520 daneben\u001b[0m\n",
      "neues weight1: -0.36332042 und neues bias1: -0.35968390\n",
      "neues weight2: -3.21413781 und neues bias2: -2.49673464\n",
      "Wir haben 0.06579332246575682 und -1.1818181818181817 als Trainings Daten\n",
      "Ich sage y vorraus: -1.32094085\n",
      "\u001b[31mIch lag um -0.13912267 daneben\u001b[0m\n",
      "neues weight1: -0.36383008 und neues bias1: -0.36743028\n",
      "neues weight2: -3.21515569 und neues bias2: -2.49395218\n",
      "Wir haben 0.07054802310718646 und -1.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: -1.29139485\n",
      "\u001b[31mIch lag um -0.13987970 daneben\u001b[0m\n",
      "neues weight1: -0.36437587 und neues bias1: -0.37516665\n",
      "neues weight2: -3.21620206 und neues bias2: -2.49115459\n",
      "Wir haben 0.07564633275546291 und -1.121212121212121 als Trainings Daten\n",
      "Ich sage y vorraus: -1.26165609\n",
      "\u001b[31mIch lag um -0.14044397 daneben\u001b[0m\n",
      "neues weight1: -0.36495938 und neues bias1: -0.38288036\n",
      "neues weight2: -3.21727585 und neues bias2: -2.48834571\n",
      "Wir haben 0.08111308307896872 und -1.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -1.23174463\n",
      "\u001b[31mIch lag um -0.14083554 daneben\u001b[0m\n",
      "neues weight1: -0.36558230 und neues bias1: -0.39056004\n",
      "neues weight2: -3.21837600 und neues bias2: -2.48552900\n",
      "Wir haben 0.08697490026177834 und -1.0606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.20167433\n",
      "\u001b[31mIch lag um -0.14106827 daneben\u001b[0m\n",
      "neues weight1: -0.36624638 und neues bias1: -0.39819530\n",
      "neues weight2: -3.21950148 und neues bias2: -2.48270763\n",
      "Wir haben 0.093260334688322 und -1.0303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.17145404\n",
      "\u001b[31mIch lag um -0.14115101 daneben\u001b[0m\n",
      "neues weight1: -0.36695339 und neues bias1: -0.40577638\n",
      "neues weight2: -3.22065125 und neues bias2: -2.47988461\n",
      "Wir haben 0.1 und -1.0 als Trainings Daten\n",
      "Ich sage y vorraus: -1.14108840\n",
      "\u001b[31mIch lag um -0.14108840 daneben\u001b[0m\n",
      "neues weight1: -0.36770515 und neues bias1: -0.41329392\n",
      "neues weight2: -3.22182424 und neues bias2: -2.47706284\n",
      "Wir haben 0.10722672220103231 und -0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.11057859\n",
      "\u001b[31mIch lag um -0.14088162 daneben\u001b[0m\n",
      "neues weight1: -0.36850344 und neues bias1: -0.42073882\n",
      "neues weight2: -3.22301929 und neues bias2: -2.47424521\n",
      "Wir haben 0.11497569953977356 und -0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.07992282\n",
      "\u001b[31mIch lag um -0.14052888 daneben\u001b[0m\n",
      "neues weight1: -0.36935003 und neues bias1: -0.42810201\n",
      "neues weight2: -3.22423518 und neues bias2: -2.47143463\n",
      "Wir haben 0.12328467394420659 und -0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.04911673\n",
      "\u001b[31mIch lag um -0.14002582 daneben\u001b[0m\n",
      "neues weight1: -0.37024660 und neues bias1: -0.43537440\n",
      "neues weight2: -3.22547059 und neues bias2: -2.46863412\n",
      "Wir haben 0.13219411484660293 und -0.8787878787878787 als Trainings Daten\n",
      "Ich sage y vorraus: -1.01815374\n",
      "\u001b[31mIch lag um -0.13936586 daneben\u001b[0m\n",
      "neues weight1: -0.37119474 und neues bias1: -0.44254672\n",
      "neues weight2: -3.22672403 und neues bias2: -2.46584680\n",
      "Wir haben 0.14174741629268056 und -0.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -0.98702527\n",
      "\u001b[31mIch lag um -0.13854042 daneben\u001b[0m\n",
      "neues weight1: -0.37219586 und neues bias1: -0.44960943\n",
      "neues weight2: -3.22799390 und neues bias2: -2.46307599\n",
      "Wir haben 0.1519911082952934 und -0.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -0.95572092\n",
      "\u001b[31mIch lag um -0.13753910 daneben\u001b[0m\n",
      "neues weight1: -0.37325118 und neues bias1: -0.45655272\n",
      "neues weight2: -3.22927842 und neues bias2: -2.46032521\n",
      "Wir haben 0.16297508346206444 und -0.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -0.92422863\n",
      "\u001b[31mIch lag um -0.13634984 daneben\u001b[0m\n",
      "neues weight1: -0.37436163 und neues bias1: -0.46336637\n",
      "neues weight2: -3.23057559 und neues bias2: -2.45759821\n",
      "Wir haben 0.17475284000076838 und -0.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -0.89253482\n",
      "\u001b[31mIch lag um -0.13495906 daneben\u001b[0m\n",
      "neues weight1: -0.37552783 und neues bias1: -0.47003976\n",
      "neues weight2: -3.23188322 und neues bias2: -2.45489903\n",
      "Wir haben 0.1873817422860384 und -0.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -0.86062445\n",
      "\u001b[31mIch lag um -0.13335172 daneben\u001b[0m\n",
      "neues weight1: -0.37674994 und neues bias1: -0.47656182\n",
      "neues weight2: -3.23319886 und neues bias2: -2.45223200\n",
      "Wir haben 0.20092330025650468 und -0.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -0.82848118\n",
      "\u001b[31mIch lag um -0.13151148 daneben\u001b[0m\n",
      "neues weight1: -0.37802765 und neues bias1: -0.48292101\n",
      "neues weight2: -3.23451979 und neues bias2: -2.44960177\n",
      "Wir haben 0.21544346900318845 und -0.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -0.79608744\n",
      "\u001b[31mIch lag um -0.12942077 daneben\u001b[0m\n",
      "neues weight1: -0.37936002 und neues bias1: -0.48910533\n",
      "neues weight2: -3.23584301 und neues bias2: -2.44701335\n",
      "Wir haben 0.23101297000831605 und -0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -0.76342457\n",
      "\u001b[31mIch lag um -0.12706093 daneben\u001b[0m\n",
      "neues weight1: -0.38074540 und neues bias1: -0.49510230\n",
      "neues weight2: -3.23716519 und neues bias2: -2.44447213\n",
      "Wir haben 0.24770763559917114 und -0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -0.73047297\n",
      "\u001b[31mIch lag um -0.12441237 daneben\u001b[0m\n",
      "neues weight1: -0.38218130 und neues bias1: -0.50089903\n",
      "neues weight2: -3.23848265 und neues bias2: -2.44198389\n",
      "Wir haben 0.26560877829466867 und -0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -0.69721228\n",
      "\u001b[31mIch lag um -0.12145470 daneben\u001b[0m\n",
      "neues weight1: -0.38366424 und neues bias1: -0.50648222\n",
      "neues weight2: -3.23979136 und neues bias2: -2.43955479\n",
      "Wir haben 0.2848035868435802 und -0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -0.66362161\n",
      "\u001b[31mIch lag um -0.11816706 daneben\u001b[0m\n",
      "neues weight1: -0.38518965 und neues bias1: -0.51183824\n",
      "neues weight2: -3.24108685 und neues bias2: -2.43719145\n",
      "Wir haben 0.30538555088334157 und -0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -0.62967982\n",
      "\u001b[31mIch lag um -0.11452831 daneben\u001b[0m\n",
      "neues weight1: -0.38675169 und neues bias1: -0.51695322\n",
      "neues weight2: -3.24236427 und neues bias2: -2.43490089\n",
      "Wir haben 0.32745491628777285 und -0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: -0.59536592\n",
      "\u001b[31mIch lag um -0.11051744 daneben\u001b[0m\n",
      "neues weight1: -0.38834310 und neues bias1: -0.52181315\n",
      "neues weight2: -3.24361830 und neues bias2: -2.43269054\n",
      "Wir haben 0.3511191734215131 und -0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -0.56065947\n",
      "\u001b[31mIch lag um -0.10611402 daneben\u001b[0m\n",
      "neues weight1: -0.38995505 und neues bias1: -0.52640404\n",
      "neues weight2: -3.24484316 und neues bias2: -2.43056826\n",
      "Wir haben 0.37649358067924676 und -0.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.52554111\n",
      "\u001b[31mIch lag um -0.10129869 daneben\u001b[0m\n",
      "neues weight1: -0.39157701 und neues bias1: -0.53071210\n",
      "neues weight2: -3.24603260 und neues bias2: -2.42854228\n",
      "Wir haben 0.4037017258596556 und -0.3939393939393938 als Trainings Daten\n",
      "Ich sage y vorraus: -0.48999323\n",
      "\u001b[31mIch lag um -0.09605384 daneben\u001b[0m\n",
      "neues weight1: -0.39319659 und neues bias1: -0.53472392\n",
      "neues weight2: -3.24717987 und neues bias2: -2.42662121\n",
      "Wir haben 0.43287612810830595 und -0.36363636363636354 als Trainings Daten\n",
      "Ich sage y vorraus: -0.45400066\n",
      "\u001b[31mIch lag um -0.09036430 daneben\u001b[0m\n",
      "neues weight1: -0.39479946 und neues bias1: -0.53842676\n",
      "neues weight2: -3.24827778 und neues bias2: -2.42481392\n",
      "Wir haben 0.464158883361278 und -0.33333333333333326 als Trainings Daten\n",
      "Ich sage y vorraus: -0.41755155\n",
      "\u001b[31mIch lag um -0.08421822 daneben\u001b[0m\n",
      "neues weight1: -0.39636926 und neues bias1: -0.54180879\n",
      "neues weight2: -3.24931862 und neues bias2: -2.42312956\n",
      "Wir haben 0.49770235643321115 und -0.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -0.38063831\n",
      "\u001b[31mIch lag um -0.07760801 daneben\u001b[0m\n",
      "neues weight1: -0.39788758 und neues bias1: -0.54485945\n",
      "neues weight2: -3.25029430 und neues bias2: -2.42157740\n",
      "Wir haben 0.533669923120631 und -0.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -0.34325865\n",
      "\u001b[31mIch lag um -0.07053138 daneben\u001b[0m\n",
      "neues weight1: -0.39933400 und neues bias1: -0.54756978\n",
      "neues weight2: -3.25119629 und neues bias2: -2.42016677\n",
      "Wir haben 0.5722367659350217 und -0.24242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.30541667\n",
      "\u001b[31mIch lag um -0.06299243 daneben\u001b[0m\n",
      "neues weight1: -0.40068621 und neues bias1: -0.54993281\n",
      "neues weight2: -3.25201576 und neues bias2: -2.41890692\n",
      "Wir haben 0.6135907273413173 und -0.21212121212121215 als Trainings Daten\n",
      "Ich sage y vorraus: -0.26712402\n",
      "\u001b[31mIch lag um -0.05500280 daneben\u001b[0m\n",
      "neues weight1: -0.40192024 und neues bias1: -0.55194397\n",
      "neues weight2: -3.25274364 und neues bias2: -2.41780686\n",
      "Wir haben 0.6579332246575682 und -0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: -0.22840100\n",
      "\u001b[31mIch lag um -0.04658282 daneben\u001b[0m\n",
      "neues weight1: -0.40301075 und neues bias1: -0.55360145\n",
      "neues weight2: -3.25337073 und neues bias2: -2.41687521\n",
      "Wir haben 0.7054802310718645 und -0.15151515151515138 als Trainings Daten\n",
      "Ich sage y vorraus: -0.18927765\n",
      "\u001b[31mIch lag um -0.03776250 daneben\u001b[0m\n",
      "neues weight1: -0.40393152 und neues bias1: -0.55490661\n",
      "neues weight2: -3.25388786 und neues bias2: -2.41611996\n",
      "Wir haben 0.7564633275546291 und -0.12121212121212109 als Trainings Daten\n",
      "Ich sage y vorraus: -0.14979459\n",
      "\u001b[31mIch lag um -0.02858247 daneben\u001b[0m\n",
      "neues weight1: -0.40465601 und neues bias1: -0.55586435\n",
      "neues weight2: -3.25428601 und neues bias2: -2.41554831\n",
      "Wir haben 0.8111308307896873 und -0.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -0.11000371\n",
      "\u001b[31mIch lag um -0.01909462 daneben\u001b[0m\n",
      "neues weight1: -0.40515811 und neues bias1: -0.55648336\n",
      "neues weight2: -3.25455657 und neues bias2: -2.41516641\n",
      "Wir haben 0.8697490026177834 und -0.06060606060606055 als Trainings Daten\n",
      "Ich sage y vorraus: -0.06996851\n",
      "\u001b[31mIch lag um -0.00936245 daneben\u001b[0m\n",
      "neues weight1: -0.40541293 und neues bias1: -0.55677634\n",
      "neues weight2: -3.25469150 und neues bias2: -2.41497917\n",
      "Wir haben 0.9326033468832199 und -0.030303030303030297 als Trainings Daten\n",
      "Ich sage y vorraus: -0.02976408\n",
      "\u001b[31mIch lag um 0.00053895 daneben\u001b[0m\n",
      "neues weight1: -0.40539778 und neues bias1: -0.55676010\n",
      "neues weight2: -3.25468360 und neues bias2: -2.41498994\n",
      "Wir haben 1.0 und 0.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.01052345\n",
      "\u001b[31mIch lag um 0.01052345 daneben\u001b[0m\n",
      "neues weight1: -0.40509321 und neues bias1: -0.55645553\n",
      "neues weight2: -3.25452675 und neues bias2: -2.41520041\n",
      "Wir haben 1.072267222010323 und 0.030303030303030252 als Trainings Daten\n",
      "Ich sage y vorraus: 0.05079789\n",
      "\u001b[31mIch lag um 0.02049486 daneben\u001b[0m\n",
      "neues weight1: -0.40448403 und neues bias1: -0.55588741\n",
      "neues weight2: -3.25421616 und neues bias2: -2.41561031\n",
      "Wir haben 1.1497569953977356 und 0.060606060606060524 als Trainings Daten\n",
      "Ich sage y vorraus: 0.09095446\n",
      "\u001b[31mIch lag um 0.03034840 daneben\u001b[0m\n",
      "neues weight1: -0.40356039 und neues bias1: -0.55508406\n",
      "neues weight2: -3.25374864 und neues bias2: -2.41621728\n",
      "Wir haben 1.232846739442066 und 0.09090909090909083 als Trainings Daten\n",
      "Ich sage y vorraus: 0.13088172\n",
      "\u001b[31mIch lag um 0.03997263 daneben\u001b[0m\n",
      "neues weight1: -0.40231870 und neues bias1: -0.55407689\n",
      "neues weight2: -3.25312282 und neues bias2: -2.41701673\n",
      "Wir haben 1.3219411484660286 und 0.12121212121212108 als Trainings Daten\n",
      "Ich sage y vorraus: 0.17046386\n",
      "\u001b[31mIch lag um 0.04925173 daneben\u001b[0m\n",
      "neues weight1: -0.40076251 und neues bias1: -0.55289969\n",
      "neues weight2: -3.25233934 und neues bias2: -2.41800177\n",
      "Wir haben 1.4174741629268048 und 0.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: 0.20958308\n",
      "\u001b[31mIch lag um 0.05806793 daneben\u001b[0m\n",
      "neues weight1: -0.39890313 und neues bias1: -0.55158794\n",
      "neues weight2: -3.25140107 und neues bias2: -2.41916312\n",
      "Wir haben 1.5199110829529332 und 0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: 0.24812210\n",
      "\u001b[31mIch lag um 0.06630392 daneben\u001b[0m\n",
      "neues weight1: -0.39676004 und neues bias1: -0.55017793\n",
      "neues weight2: -3.25031322 und neues bias2: -2.42048920\n",
      "Wir haben 1.629750834620645 und 0.21212121212121235 als Trainings Daten\n",
      "Ich sage y vorraus: 0.28596634\n",
      "\u001b[31mIch lag um 0.07384513 daneben\u001b[0m\n",
      "neues weight1: -0.39436097 und neues bias1: -0.54870588\n",
      "neues weight2: -3.24908344 und neues bias2: -2.42196611\n",
      "Wir haben 1.7475284000076847 und 0.24242424242424265 als Trainings Daten\n",
      "Ich sage y vorraus: 0.32300570\n",
      "\u001b[31mIch lag um 0.08058146 daneben\u001b[0m\n",
      "neues weight1: -0.39174172 und neues bias1: -0.54720705\n",
      "neues weight2: -3.24772186 und neues bias2: -2.42357773\n",
      "Wir haben 1.873817422860385 und 0.27272727272727293 als Trainings Daten\n",
      "Ich sage y vorraus: 0.35913574\n",
      "\u001b[31mIch lag um 0.08640846 daneben\u001b[0m\n",
      "neues weight1: -0.38894568 und neues bias1: -0.54571489\n",
      "neues weight2: -3.24624113 und neues bias2: -2.42530590\n",
      "Wir haben 2.0092330025650478 und 0.30303030303030315 als Trainings Daten\n",
      "Ich sage y vorraus: 0.39425801\n",
      "\u001b[31mIch lag um 0.09122771 daneben\u001b[0m\n",
      "neues weight1: -0.38602291 und neues bias1: -0.54426022\n",
      "neues weight2: -3.24465639 und neues bias2: -2.42713046\n",
      "Wir haben 2.1544346900318843 und 0.3333333333333334 als Trainings Daten\n",
      "Ich sage y vorraus: 0.42827967\n",
      "\u001b[31mIch lag um 0.09494633 daneben\u001b[0m\n",
      "neues weight1: -0.38302905 und neues bias1: -0.54287059\n",
      "neues weight2: -3.24298527 und neues bias2: -2.42902938\n",
      "Wir haben 2.31012970008316 und 0.3636363636363637 als Trainings Daten\n",
      "Ich sage y vorraus: 0.46111216\n",
      "\u001b[31mIch lag um 0.09747580 daneben\u001b[0m\n",
      "neues weight1: -0.38002378 und neues bias1: -0.54156968\n",
      "neues weight2: -3.24124787 und neues bias2: -2.43097890\n",
      "Wir haben 2.4770763559917115 und 0.39393939393939403 als Trainings Daten\n",
      "Ich sage y vorraus: 0.49266941\n",
      "\u001b[31mIch lag um 0.09873001 daneben\u001b[0m\n",
      "neues weight1: -0.37706909 und neues bias1: -0.54037687\n",
      "neues weight2: -3.23946675 und neues bias2: -2.43295350\n",
      "Wir haben 2.656087782946687 und 0.42424242424242437 als Trainings Daten\n",
      "Ich sage y vorraus: 0.52286555\n",
      "\u001b[31mIch lag um 0.09862313 daneben\u001b[0m\n",
      "neues weight1: -0.37422713 und neues bias1: -0.53930689\n",
      "neues weight2: -3.23766700 und neues bias2: -2.43492596\n",
      "Wir haben 2.848035868435802 und 0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: 0.55161303\n",
      "\u001b[31mIch lag um 0.09706757 daneben\u001b[0m\n",
      "neues weight1: -0.37155784 und neues bias1: -0.53836965\n",
      "neues weight2: -3.23587623 und neues bias2: -2.43686732\n",
      "Wir haben 3.0538555088334154 und 0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.57882131\n",
      "\u001b[31mIch lag um 0.09397283 daneben\u001b[0m\n",
      "neues weight1: -0.36911626 und neues bias1: -0.53757014\n",
      "neues weight2: -3.23412466 und neues bias2: -2.43874677\n",
      "Wir haben 3.2745491628777286 und 0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: 0.60439739\n",
      "\u001b[31mIch lag um 0.08924588 daneben\u001b[0m\n",
      "neues weight1: -0.36694969 und neues bias1: -0.53690850\n",
      "neues weight2: -3.23244514 und neues bias2: -2.44053169\n",
      "Wir haben 3.511191734215131 und 0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: 0.62824857\n",
      "\u001b[31mIch lag um 0.08279402 daneben\u001b[0m\n",
      "neues weight1: -0.36509474 und neues bias1: -0.53638020\n",
      "neues weight2: -3.23087310 und neues bias2: -2.44218757\n",
      "Wir haben 3.7649358067924674 und 0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: 0.65028840\n",
      "\u001b[31mIch lag um 0.07453082 daneben\u001b[0m\n",
      "neues weight1: -0.36357462 und neues bias1: -0.53597645\n",
      "neues weight2: -3.22944634 und neues bias2: -2.44367819\n",
      "Wir haben 4.037017258596554 und 0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: 0.67044593\n",
      "\u001b[31mIch lag um 0.06438532 daneben\u001b[0m\n",
      "neues weight1: -0.36239702 und neues bias1: -0.53568475\n",
      "neues weight2: -3.22820462 und neues bias2: -2.44496589\n",
      "Wir haben 4.328761281083057 und 0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: 0.68867780\n",
      "\u001b[31mIch lag um 0.05231416 daneben\u001b[0m\n",
      "neues weight1: -0.36155301 und neues bias1: -0.53548977\n",
      "neues weight2: -3.22718898 und neues bias2: -2.44601218\n",
      "Wir haben 4.641588833612782 und 0.666666666666667 als Trainings Daten\n",
      "Ich sage y vorraus: 0.70498163\n",
      "\u001b[31mIch lag um 0.03831497 daneben\u001b[0m\n",
      "neues weight1: -0.36101738 und neues bias1: -0.53537437\n",
      "neues weight2: -3.22644077 und neues bias2: -2.44677848\n",
      "Wir haben 4.9770235643321135 und 0.6969696969696972 als Trainings Daten\n",
      "Ich sage y vorraus: 0.71940836\n",
      "\u001b[31mIch lag um 0.02243867 daneben\u001b[0m\n",
      "neues weight1: -0.36075073 und neues bias1: -0.53532080\n",
      "neues weight2: -3.22600038 und neues bias2: -2.44722725\n",
      "Wir haben 5.336699231206313 und 0.7272727272727275 als Trainings Daten\n",
      "Ich sage y vorraus: 0.73207039\n",
      "\u001b[31mIch lag um 0.00479767 daneben\u001b[0m\n",
      "neues weight1: -0.36070325 und neues bias1: -0.53531190\n",
      "neues weight2: -3.22590582 und neues bias2: -2.44732320\n",
      "Wir haben 5.72236765935022 und 0.7575757575757578 als Trainings Daten\n",
      "Ich sage y vorraus: 0.74314297\n",
      "\u001b[31mIch lag um -0.01443279 daneben\u001b[0m\n",
      "neues weight1: -0.36081968 und neues bias1: -0.53533225\n",
      "neues weight2: -3.22619130 und neues bias2: -2.44703455\n",
      "Wir haben 6.135907273413176 und 0.7878787878787881 als Trainings Daten\n",
      "Ich sage y vorraus: 0.75285742\n",
      "\u001b[31mIch lag um -0.03502137 daneben\u001b[0m\n",
      "neues weight1: -0.36104482 und neues bias1: -0.53536894\n",
      "neues weight2: -3.22688602 und neues bias2: -2.44633412\n",
      "Wir haben 6.5793322465756825 und 0.8181818181818183 als Trainings Daten\n",
      "Ich sage y vorraus: 0.76148687\n",
      "\u001b[31mIch lag um -0.05669495 daneben\u001b[0m\n",
      "neues weight1: -0.36132844 und neues bias1: -0.53541205\n",
      "neues weight2: -3.22801322 und neues bias2: -2.44520022\n",
      "Wir haben 7.054802310718645 und 0.8484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.76932699\n",
      "\u001b[31mIch lag um -0.07915786 daneben\u001b[0m\n",
      "neues weight1: -0.36162906 und neues bias1: -0.53545466\n",
      "neues weight2: -3.22958976 und neues bias2: -2.44361706\n",
      "Wir haben 7.56463327554629 und 0.8787878787878789 als Trainings Daten\n",
      "Ich sage y vorraus: 0.77667539\n",
      "\u001b[31mIch lag um -0.10211249 daneben\u001b[0m\n",
      "neues weight1: -0.36191591 und neues bias1: -0.53549258\n",
      "neues weight2: -3.23162613 und neues bias2: -2.44157481\n",
      "Wir haben 8.111308307896872 und 0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: 0.78381298\n",
      "\u001b[31mIch lag um -0.12527793 daneben\u001b[0m\n",
      "neues weight1: -0.36216923 und neues bias1: -0.53552381\n",
      "neues weight2: -3.23412686 und neues bias2: -2.43906925\n",
      "Wir haben 8.697490026177835 und 0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79098998\n",
      "\u001b[31mIch lag um -0.14840396 daneben\u001b[0m\n",
      "neues weight1: -0.36237911 und neues bias1: -0.53554794\n",
      "neues weight2: -3.23709120 und neues bias2: -2.43610118\n",
      "Wir haben 9.326033468832199 und 0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79841730\n",
      "\u001b[31mIch lag um -0.17127967 daneben\u001b[0m\n",
      "neues weight1: -0.36254343 und neues bias1: -0.53556556\n",
      "neues weight2: -3.24051407 und neues bias2: -2.43267558\n",
      "Wir haben 10.0 und 1.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.80626328\n",
      "\u001b[31mIch lag um -0.19373672 daneben\u001b[0m\n",
      "neues weight1: -0.36266547 und neues bias1: -0.53557776\n",
      "neues weight2: -3.24438692 und neues bias2: -2.42880085\n",
      "\n",
      "\n",
      "Ich trainiere in Epoche 2001\n",
      "Wir haben 0.01 und -2.0 als Trainings Daten\n",
      "Ich sage y vorraus: -0.83079905\n",
      "\u001b[31mIch lag um 1.16920095 daneben\u001b[0m\n",
      "neues weight1: -0.36139800 und neues bias1: -0.48108344\n",
      "neues weight2: -3.24720334 und neues bias2: -2.46628103\n",
      "Wir haben 0.010722672220103232 und -1.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.00437068\n",
      "\u001b[31mIch lag um 0.96532629 daneben\u001b[0m\n",
      "neues weight1: -0.36086202 und neues bias1: -0.43109802\n",
      "neues weight2: -3.23851143 und neues bias2: -2.48558756\n",
      "Wir haben 0.011497569953977356 und -1.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.15877913\n",
      "\u001b[31mIch lag um 0.78061481 daneben\u001b[0m\n",
      "neues weight1: -0.36037827 und neues bias1: -0.38902410\n",
      "neues weight2: -3.23211512 und neues bias2: -2.50119986\n",
      "Wir haben 0.012328467394420659 und -1.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.29127291\n",
      "\u001b[31mIch lag um 0.61781800 daneben\u001b[0m\n",
      "neues weight1: -0.35995491 und neues bias1: -0.35468349\n",
      "neues weight2: -3.22748958 und neues bias2: -2.51355622\n",
      "Wir haben 0.013219411484660288 und -1.878787878787879 als Trainings Daten\n",
      "Ich sage y vorraus: -1.40096847\n",
      "\u001b[31mIch lag um 0.47781940 daneben\u001b[0m\n",
      "neues weight1: -0.35959563 und neues bias1: -0.32750555\n",
      "neues weight2: -3.22419528 und neues bias2: -2.52311260\n",
      "Wir haben 0.014174741629268055 und -1.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -1.48860538\n",
      "\u001b[31mIch lag um 0.35987947 daneben\u001b[0m\n",
      "neues weight1: -0.35930055 und neues bias1: -0.30668820\n",
      "neues weight2: -3.22188587 und neues bias2: -2.53031019\n",
      "Wir haben 0.01519911082952934 und -1.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -1.55604062\n",
      "\u001b[31mIch lag um 0.26214120 daneben\u001b[0m\n",
      "neues weight1: -0.35906729 und neues bias1: -0.29134101\n",
      "neues weight2: -3.22030049 und neues bias2: -2.53555302\n",
      "Wir haben 0.016297508346206444 und -1.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -1.60571814\n",
      "\u001b[31mIch lag um 0.18216065 daneben\u001b[0m\n",
      "neues weight1: -0.35889202 und neues bias1: -0.28058691\n",
      "neues weight2: -3.21924854 und neues bias2: -2.53919623\n",
      "Wir haben 0.01747528400007684 und -1.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -1.64025018\n",
      "\u001b[31mIch lag um 0.11732558 daneben\u001b[0m\n",
      "neues weight1: -0.35877031 und neues bias1: -0.27362193\n",
      "neues weight2: -3.21859330 und neues bias2: -2.54154274\n",
      "Wir haben 0.01873817422860384 und -1.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -1.66214575\n",
      "\u001b[31mIch lag um 0.06512698 daneben\u001b[0m\n",
      "neues weight1: -0.35869761 und neues bias1: -0.26974255\n",
      "neues weight2: -3.21823742 und neues bias2: -2.54284528\n",
      "Wir haben 0.02009233002565047 und -1.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67366519\n",
      "\u001b[31mIch lag um 0.02330451 daneben\u001b[0m\n",
      "neues weight1: -0.35866967 und neues bias1: -0.26835198\n",
      "neues weight2: -3.21811154 und neues bias2: -2.54331137\n",
      "Wir haben 0.021544346900318846 und -1.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67676244\n",
      "\u001b[31mIch lag um -0.01009577 daneben\u001b[0m\n",
      "neues weight1: -0.35868266 und neues bias1: -0.26895465\n",
      "neues weight2: -3.21816591 und neues bias2: -2.54310946\n",
      "Wir haben 0.023101297000831605 und -1.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67308039\n",
      "\u001b[31mIch lag um -0.03671675 daneben\u001b[0m\n",
      "neues weight1: -0.35873326 und neues bias1: -0.27114514\n",
      "neues weight2: -3.21836443 und neues bias2: -2.54237512\n",
      "Wir haben 0.024770763559917114 und -1.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.66397408\n",
      "\u001b[31mIch lag um -0.05791347 daneben\u001b[0m\n",
      "neues weight1: -0.35881872 und neues bias1: -0.27459518\n",
      "neues weight2: -3.21868056 und neues bias2: -2.54121685\n",
      "Wir haben 0.026560877829466867 und -1.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -1.65054610\n",
      "\u001b[31mIch lag um -0.07478852 daneben\u001b[0m\n",
      "neues weight1: -0.35893680 und neues bias1: -0.27904093\n",
      "neues weight2: -3.21909447 und neues bias2: -2.53972108\n",
      "Wir haben 0.02848035868435802 und -1.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -1.63368482\n",
      "\u001b[31mIch lag um -0.08823028 daneben\u001b[0m\n",
      "neues weight1: -0.35908577 und neues bias1: -0.28427137\n",
      "neues weight2: -3.21959113 und neues bias2: -2.53795648\n",
      "Wir haben 0.030538555088334154 und -1.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -1.61410093\n",
      "\u001b[31mIch lag um -0.09894941 daneben\u001b[0m\n",
      "neues weight1: -0.35926432 und neues bias1: -0.29011828\n",
      "neues weight2: -3.22015900 und neues bias2: -2.53597749\n",
      "Wir haben 0.03274549162877728 und -1.4848484848484849 als Trainings Daten\n",
      "Ich sage y vorraus: -1.59235990\n",
      "\u001b[31mIch lag um -0.10751142 daneben\u001b[0m\n",
      "neues weight1: -0.35947159 und neues bias1: -0.29644779\n",
      "neues weight2: -3.22078909 und neues bias2: -2.53382726\n",
      "Wir haben 0.03511191734215131 und -1.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -1.56890985\n",
      "\u001b[31mIch lag um -0.11436440 daneben\u001b[0m\n",
      "neues weight1: -0.35970704 und neues bias1: -0.30315345\n",
      "neues weight2: -3.22147434 und neues bias2: -2.53153997\n",
      "Wir haben 0.037649358067924674 und -1.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -1.54410476\n",
      "\u001b[31mIch lag um -0.11986234 daneben\u001b[0m\n",
      "neues weight1: -0.35997047 und neues bias1: -0.31015056\n",
      "neues weight2: -3.22220914 und neues bias2: -2.52914272\n",
      "Wir haben 0.040370172585965536 und -1.393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.51822352\n",
      "\u001b[31mIch lag um -0.12428412 daneben\u001b[0m\n",
      "neues weight1: -0.36026199 und neues bias1: -0.31737159\n",
      "neues weight2: -3.22298898 und neues bias2: -2.52665704\n",
      "Wir haben 0.04328761281083057 und -1.3636363636363638 als Trainings Daten\n",
      "Ich sage y vorraus: -1.49148533\n",
      "\u001b[31mIch lag um -0.12784897 daneben\u001b[0m\n",
      "neues weight1: -0.36058192 und neues bias1: -0.32476256\n",
      "neues weight2: -3.22381024 und neues bias2: -2.52410006\n",
      "Wir haben 0.046415888336127795 und -1.3333333333333333 als Trainings Daten\n",
      "Ich sage y vorraus: -1.46406213\n",
      "\u001b[31mIch lag um -0.13072879 daneben\u001b[0m\n",
      "neues weight1: -0.36093086 und neues bias1: -0.33228013\n",
      "neues weight2: -3.22466995 und neues bias2: -2.52148549\n",
      "Wir haben 0.049770235643321115 und -1.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.43608838\n",
      "\u001b[31mIch lag um -0.13305808 daneben\u001b[0m\n",
      "neues weight1: -0.36130957 und neues bias1: -0.33988928\n",
      "neues weight2: -3.22556568 und neues bias2: -2.51882433\n",
      "Wir haben 0.0533669923120631 und -1.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -1.40766898\n",
      "\u001b[31mIch lag um -0.13494171 daneben\u001b[0m\n",
      "neues weight1: -0.36171901 und neues bias1: -0.34756150\n",
      "neues weight2: -3.22649538 und neues bias2: -2.51612549\n",
      "Wir haben 0.05722367659350217 und -1.2424242424242424 als Trainings Daten\n",
      "Ich sage y vorraus: -1.37888538\n",
      "\u001b[31mIch lag um -0.13646114 daneben\u001b[0m\n",
      "neues weight1: -0.36216031 und neues bias1: -0.35527334\n",
      "neues weight2: -3.22745735 und neues bias2: -2.51339627\n",
      "Wir haben 0.06135907273413173 und -1.2121212121212122 als Trainings Daten\n",
      "Ich sage y vorraus: -1.34980049\n",
      "\u001b[31mIch lag um -0.13767928 daneben\u001b[0m\n",
      "neues weight1: -0.36263474 und neues bias1: -0.36300525\n",
      "neues weight2: -3.22845010 und neues bias2: -2.51064268\n",
      "Wir haben 0.06579332246575682 und -1.1818181818181817 als Trainings Daten\n",
      "Ich sage y vorraus: -1.32046249\n",
      "\u001b[31mIch lag um -0.13864431 daneben\u001b[0m\n",
      "neues weight1: -0.36314368 und neues bias1: -0.37074074\n",
      "neues weight2: -3.22947234 und neues bias2: -2.50786980\n",
      "Wir haben 0.07054802310718646 und -1.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: -1.29090783\n",
      "\u001b[31mIch lag um -0.13939268 daneben\u001b[0m\n",
      "neues weight1: -0.36368865 und neues bias1: -0.37846556\n",
      "neues weight2: -3.23052288 und neues bias2: -2.50508194\n",
      "Wir haben 0.07564633275546291 und -1.121212121212121 als Trainings Daten\n",
      "Ich sage y vorraus: -1.26116363\n",
      "\u001b[31mIch lag um -0.13995151 daneben\u001b[0m\n",
      "neues weight1: -0.36427125 und neues bias1: -0.38616723\n",
      "neues weight2: -3.23160065 und neues bias2: -2.50228291\n",
      "Wir haben 0.08111308307896872 und -1.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -1.23124943\n",
      "\u001b[31mIch lag um -0.14034034 daneben\u001b[0m\n",
      "neues weight1: -0.36489317 und neues bias1: -0.39383454\n",
      "neues weight2: -3.23270461 und neues bias2: -2.49947611\n",
      "Wir haben 0.08697490026177834 und -1.0606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.20117871\n",
      "\u001b[31mIch lag um -0.14057265 daneben\u001b[0m\n",
      "neues weight1: -0.36555615 und neues bias1: -0.40145721\n",
      "neues weight2: -3.23383373 und neues bias2: -2.49666465\n",
      "Wir haben 0.093260334688322 und -1.0303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.17095996\n",
      "\u001b[31mIch lag um -0.14065693 daneben\u001b[0m\n",
      "neues weight1: -0.36626198 und neues bias1: -0.40902557\n",
      "neues weight2: -3.23498697 und neues bias2: -2.49385151\n",
      "Wir haben 0.1 und -1.0 als Trainings Daten\n",
      "Ich sage y vorraus: -1.14059756\n",
      "\u001b[31mIch lag um -0.14059756 daneben\u001b[0m\n",
      "neues weight1: -0.36701246 und neues bias1: -0.41653038\n",
      "neues weight2: -3.23616326 und neues bias2: -2.49103956\n",
      "Wir haben 0.10722672220103231 und -0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.11009244\n",
      "\u001b[31mIch lag um -0.14039547 daneben\u001b[0m\n",
      "neues weight1: -0.36780939 und neues bias1: -0.42396258\n",
      "neues weight2: -3.23736146 und neues bias2: -2.48823165\n",
      "Wir haben 0.11497569953977356 und -0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.07944264\n",
      "\u001b[31mIch lag um -0.14004870 daneben\u001b[0m\n",
      "neues weight1: -0.36865453 und neues bias1: -0.43131319\n",
      "neues weight2: -3.23858035 und neues bias2: -2.48543068\n",
      "Wir haben 0.12328467394420659 und -0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.04864365\n",
      "\u001b[31mIch lag um -0.13955274 daneben\u001b[0m\n",
      "neues weight1: -0.36954958 und neues bias1: -0.43857315\n",
      "neues weight2: -3.23981859 und neues bias2: -2.48263963\n",
      "Wir haben 0.13219411484660293 und -0.8787878787878787 als Trainings Daten\n",
      "Ich sage y vorraus: -1.01768877\n",
      "\u001b[31mIch lag um -0.13890089 daneben\u001b[0m\n",
      "neues weight1: -0.37049610 und neues bias1: -0.44573325\n",
      "neues weight2: -3.24107473 und neues bias2: -2.47986161\n",
      "Wir haben 0.14174741629268056 und -0.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -0.98656933\n",
      "\u001b[31mIch lag um -0.13808448 daneben\u001b[0m\n",
      "neues weight1: -0.37149552 und neues bias1: -0.45278399\n",
      "neues weight2: -3.24234715 und neues bias2: -2.47709992\n",
      "Wir haben 0.1519911082952934 und -0.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -0.95527485\n",
      "\u001b[31mIch lag um -0.13709303 daneben\u001b[0m\n",
      "neues weight1: -0.37254906 und neues bias1: -0.45971559\n",
      "neues weight2: -3.24363407 und neues bias2: -2.47435806\n",
      "Wir haben 0.16297508346206444 und -0.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -0.92379324\n",
      "\u001b[31mIch lag um -0.13591445 daneben\u001b[0m\n",
      "neues weight1: -0.37365767 und neues bias1: -0.46651787\n",
      "neues weight2: -3.24493350 und neues bias2: -2.47163977\n",
      "Wir haben 0.17475284000076838 und -0.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -0.89211085\n",
      "\u001b[31mIch lag um -0.13453509 daneben\u001b[0m\n",
      "neues weight1: -0.37482193 und neues bias1: -0.47318024\n",
      "neues weight2: -3.24624325 und neues bias2: -2.46894907\n",
      "Wir haben 0.1873817422860384 und -0.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -0.86021265\n",
      "\u001b[31mIch lag um -0.13293992 daneben\u001b[0m\n",
      "neues weight1: -0.37604205 und neues bias1: -0.47969165\n",
      "neues weight2: -3.24756086 und neues bias2: -2.46629027\n",
      "Wir haben 0.20092330025650468 und -0.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -0.82808227\n",
      "\u001b[31mIch lag um -0.13111257 daneben\u001b[0m\n",
      "neues weight1: -0.37731770 und neues bias1: -0.48604059\n",
      "neues weight2: -3.24888364 und neues bias2: -2.46366802\n",
      "Wir haben 0.21544346900318845 und -0.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -0.79570214\n",
      "\u001b[31mIch lag um -0.12903547 daneben\u001b[0m\n",
      "neues weight1: -0.37864796 und neues bias1: -0.49221508\n",
      "neues weight2: -3.25020857 und neues bias2: -2.46108731\n",
      "Wir haben 0.23101297000831605 und -0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -0.76305360\n",
      "\u001b[31mIch lag um -0.12668996 daneben\u001b[0m\n",
      "neues weight1: -0.38003117 und neues bias1: -0.49820268\n",
      "neues weight2: -3.25153232 und neues bias2: -2.45855351\n",
      "Wir haben 0.24770763559917114 und -0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -0.73011706\n",
      "\u001b[31mIch lag um -0.12405646 daneben\u001b[0m\n",
      "neues weight1: -0.38146486 und neues bias1: -0.50399050\n",
      "neues weight2: -3.25285123 und neues bias2: -2.45607238\n",
      "Wir haben 0.26560877829466867 und -0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -0.69687217\n",
      "\u001b[31mIch lag um -0.12111460 daneben\u001b[0m\n",
      "neues weight1: -0.38294556 und neues bias1: -0.50956527\n",
      "neues weight2: -3.25416125 und neues bias2: -2.45365009\n",
      "Wir haben 0.2848035868435802 und -0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -0.66329806\n",
      "\u001b[31mIch lag um -0.11784352 daneben\u001b[0m\n",
      "neues weight1: -0.38446873 und neues bias1: -0.51491338\n",
      "neues weight2: -3.25545793 und neues bias2: -2.45129322\n",
      "Wir haben 0.30538555088334157 und -0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -0.62937362\n",
      "\u001b[31mIch lag um -0.11422211 daneben\u001b[0m\n",
      "neues weight1: -0.38602851 und neues bias1: -0.52002098\n",
      "neues weight2: -3.25673642 und neues bias2: -2.44900877\n",
      "Wir haben 0.32745491628777285 und -0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: -0.59507786\n",
      "\u001b[31mIch lag um -0.11022937 daneben\u001b[0m\n",
      "neues weight1: -0.38761769 und neues bias1: -0.52487408\n",
      "neues weight2: -3.25799141 und neues bias2: -2.44680419\n",
      "Wir haben 0.3511191734215131 und -0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -0.56039035\n",
      "\u001b[31mIch lag um -0.10584490 daneben\u001b[0m\n",
      "neues weight1: -0.38922744 und neues bias1: -0.52945873\n",
      "neues weight2: -3.25921712 und neues bias2: -2.44468729\n",
      "Wir haben 0.37649358067924676 und -0.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.52529177\n",
      "\u001b[31mIch lag um -0.10104934 daneben\u001b[0m\n",
      "neues weight1: -0.39084727 und neues bias1: -0.53376113\n",
      "neues weight2: -3.26040730 und neues bias2: -2.44266630\n",
      "Wir haben 0.4037017258596556 und -0.3939393939393938 als Trainings Daten\n",
      "Ich sage y vorraus: -0.48976449\n",
      "\u001b[31mIch lag um -0.09582510 daneben\u001b[0m\n",
      "neues weight1: -0.39246481 und neues bias1: -0.53776789\n",
      "neues weight2: -3.26155524 und neues bias2: -2.44074980\n",
      "Wir haben 0.43287612810830595 und -0.36363636363636354 als Trainings Daten\n",
      "Ich sage y vorraus: -0.45379336\n",
      "\u001b[31mIch lag um -0.09015700 daneben\u001b[0m\n",
      "neues weight1: -0.39406576 und neues bias1: -0.54146630\n",
      "neues weight2: -3.26265372 und neues bias2: -2.43894666\n",
      "Wir haben 0.464158883361278 und -0.33333333333333326 als Trainings Daten\n",
      "Ich sage y vorraus: -0.41736652\n",
      "\u001b[31mIch lag um -0.08403319 daneben\u001b[0m\n",
      "neues weight1: -0.39563379 und neues bias1: -0.54484453\n",
      "neues weight2: -3.26369508 und neues bias2: -2.43726600\n",
      "Wir haben 0.49770235643321115 und -0.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -0.38047637\n",
      "\u001b[31mIch lag um -0.07744606 daneben\u001b[0m\n",
      "neues weight1: -0.39715054 und neues bias1: -0.54789203\n",
      "neues weight2: -3.26467122 und neues bias2: -2.43571707\n",
      "Wir haben 0.533669923120631 und -0.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -0.34312056\n",
      "\u001b[31mIch lag um -0.07039328 daneben\u001b[0m\n",
      "neues weight1: -0.39859563 und neues bias1: -0.55059986\n",
      "neues weight2: -3.26557363 und neues bias2: -2.43430921\n",
      "Wir haben 0.5722367659350217 und -0.24242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.30530315\n",
      "\u001b[31mIch lag um -0.06287891 daneben\u001b[0m\n",
      "neues weight1: -0.39994678 und neues bias1: -0.55296103\n",
      "neues weight2: -3.26639352 und neues bias2: -2.43305163\n",
      "Wir haben 0.6135907273413173 und -0.21212121212121215 als Trainings Daten\n",
      "Ich sage y vorraus: -0.26703574\n",
      "\u001b[31mIch lag um -0.05491453 daneben\u001b[0m\n",
      "neues weight1: -0.40118006 und neues bias1: -0.55497097\n",
      "neues weight2: -3.26712182 und neues bias2: -2.43195334\n",
      "Wir haben 0.6579332246575682 und -0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: -0.22833853\n",
      "\u001b[31mIch lag um -0.04652035 daneben\u001b[0m\n",
      "neues weight1: -0.40227018 und neues bias1: -0.55662786\n",
      "neues weight2: -3.26774936 und neues bias2: -2.43102293\n",
      "Wir haben 0.7054802310718645 und -0.15151515151515138 als Trainings Daten\n",
      "Ich sage y vorraus: -0.18924144\n",
      "\u001b[31mIch lag um -0.03772628 daneben\u001b[0m\n",
      "neues weight1: -0.40319097 und neues bias1: -0.55793305\n",
      "neues weight2: -3.26826699 und neues bias2: -2.43026841\n",
      "Wir haben 0.7564633275546291 und -0.12121212121212109 als Trainings Daten\n",
      "Ich sage y vorraus: -0.14978494\n",
      "\u001b[31mIch lag um -0.02857282 daneben\u001b[0m\n",
      "neues weight1: -0.40391592 und neues bias1: -0.55889140\n",
      "neues weight2: -3.26866573 und neues bias2: -2.42969695\n",
      "Wir haben 0.8111308307896873 und -0.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -0.11002077\n",
      "\u001b[31mIch lag um -0.01911168 daneben\u001b[0m\n",
      "neues weight1: -0.40441895 und neues bias1: -0.55951156\n",
      "neues weight2: -3.26893699 und neues bias2: -2.42931472\n",
      "Wir haben 0.8697490026177834 und -0.06060606060606055 als Trainings Daten\n",
      "Ich sage y vorraus: -0.07001224\n",
      "\u001b[31mIch lag um -0.00940618 daneben\u001b[0m\n",
      "neues weight1: -0.40467521 und neues bias1: -0.55980619\n",
      "neues weight2: -3.26907277 und neues bias2: -2.42912659\n",
      "Wir haben 0.9326033468832199 und -0.030303030303030297 als Trainings Daten\n",
      "Ich sage y vorraus: -0.02983421\n",
      "\u001b[31mIch lag um 0.00046882 daneben\u001b[0m\n",
      "neues weight1: -0.40466202 und neues bias1: -0.55979205\n",
      "neues weight2: -3.26906588 und neues bias2: -2.42913597\n",
      "Wir haben 1.0 und 0.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.01042744\n",
      "\u001b[31mIch lag um 0.01042744 daneben\u001b[0m\n",
      "neues weight1: -0.40435993 und neues bias1: -0.55948996\n",
      "neues weight2: -3.26891025 und neues bias2: -2.42934452\n",
      "Wir haben 1.072267222010323 und 0.030303030303030252 als Trainings Daten\n",
      "Ich sage y vorraus: 0.05067677\n",
      "\u001b[31mIch lag um 0.02037374 daneben\u001b[0m\n",
      "neues weight1: -0.40375375 und neues bias1: -0.55892463\n",
      "neues weight2: -3.26860111 und neues bias2: -2.42975199\n",
      "Wir haben 1.1497569953977356 und 0.060606060606060524 als Trainings Daten\n",
      "Ich sage y vorraus: 0.09080928\n",
      "\u001b[31mIch lag um 0.03020322 daneben\u001b[0m\n",
      "neues weight1: -0.40283358 und neues bias1: -0.55812431\n",
      "neues weight2: -3.26813529 und neues bias2: -2.43035606\n",
      "Wir haben 1.232846739442066 und 0.09090909090909083 als Trainings Daten\n",
      "Ich sage y vorraus: 0.13071382\n",
      "\u001b[31mIch lag um 0.03980473 daneben\u001b[0m\n",
      "neues weight1: -0.40159580 und neues bias1: -0.55712031\n",
      "neues weight2: -3.26751144 und neues bias2: -2.43115215\n",
      "Wir haben 1.3219411484660286 und 0.12121212121212108 als Trainings Daten\n",
      "Ich sage y vorraus: 0.17027488\n",
      "\u001b[31mIch lag um 0.04906275 daneben\u001b[0m\n",
      "neues weight1: -0.40004389 und neues bias1: -0.55594635\n",
      "neues weight2: -3.26673021 und neues bias2: -2.43213341\n",
      "Wir haben 1.4174741629268048 und 0.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: 0.20937495\n",
      "\u001b[31mIch lag um 0.05785980 daneben\u001b[0m\n",
      "neues weight1: -0.39818908 und neues bias1: -0.55463782\n",
      "neues weight2: -3.26579449 und neues bias2: -2.43329060\n",
      "Wir haben 1.5199110829529332 und 0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: 0.24789705\n",
      "\u001b[31mIch lag um 0.06607887 daneben\u001b[0m\n",
      "neues weight1: -0.39605071 und neues bias1: -0.55323091\n",
      "neues weight2: -3.26470949 und neues bias2: -2.43461218\n",
      "Wir haben 1.629750834620645 und 0.21212121212121235 als Trainings Daten\n",
      "Ich sage y vorraus: 0.28572687\n",
      "\u001b[31mIch lag um 0.07360566 daneben\u001b[0m\n",
      "neues weight1: -0.39365640 und neues bias1: -0.55176179\n",
      "neues weight2: -3.26348284 und neues bias2: -2.43608429\n",
      "Wir haben 1.7475284000076847 und 0.24242424242424265 als Trainings Daten\n",
      "Ich sage y vorraus: 0.32275462\n",
      "\u001b[31mIch lag um 0.08033038 daneben\u001b[0m\n",
      "neues weight1: -0.39104182 und neues bias1: -0.55026563\n",
      "neues weight2: -3.26212467 und neues bias2: -2.43769090\n",
      "Wir haben 1.873817422860385 und 0.27272727272727293 als Trainings Daten\n",
      "Ich sage y vorraus: 0.35887611\n",
      "\u001b[31mIch lag um 0.08614884 daneben\u001b[0m\n",
      "neues weight1: -0.38825019 und neues bias1: -0.54877582\n",
      "neues weight2: -3.26064759 und neues bias2: -2.43941388\n",
      "Wir haben 2.0092330025650478 und 0.30303030303030315 als Trainings Daten\n",
      "Ich sage y vorraus: 0.39399320\n",
      "\u001b[31mIch lag um 0.09096290 daneben\u001b[0m\n",
      "neues weight1: -0.38533142 und neues bias1: -0.54732314\n",
      "neues weight2: -3.25906671 und neues bias2: -2.44123314\n",
      "Wir haben 2.1544346900318843 und 0.3333333333333334 als Trainings Daten\n",
      "Ich sage y vorraus: 0.42801331\n",
      "\u001b[31mIch lag um 0.09467997 daneben\u001b[0m\n",
      "neues weight1: -0.38234099 und neues bias1: -0.54593511\n",
      "neues weight2: -3.25739960 und neues bias2: -2.44312674\n",
      "Wir haben 2.31012970008316 und 0.3636363636363637 als Trainings Daten\n",
      "Ich sage y vorraus: 0.46084818\n",
      "\u001b[31mIch lag um 0.09721182 daneben\u001b[0m\n",
      "neues weight1: -0.37933845 und neues bias1: -0.54463538\n",
      "neues weight2: -3.25566631 und neues bias2: -2.44507097\n",
      "Wir haben 2.4770763559917115 und 0.39393939393939403 als Trainings Daten\n",
      "Ich sage y vorraus: 0.49241204\n",
      "\u001b[31mIch lag um 0.09847264 daneben\u001b[0m\n",
      "neues weight1: -0.37638565 und neues bias1: -0.54344333\n",
      "neues weight2: -3.25388934 und neues bias2: -2.44704043\n",
      "Wir haben 2.656087782946687 und 0.42424242424242437 als Trainings Daten\n",
      "Ich sage y vorraus: 0.52261933\n",
      "\u001b[31mIch lag um 0.09837691 daneben\u001b[0m\n",
      "neues weight1: -0.37354466 und neues bias1: -0.54237371\n",
      "neues weight2: -3.25209367 und neues bias2: -2.44900796\n",
      "Wir haben 2.848035868435802 und 0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: 0.55138276\n",
      "\u001b[31mIch lag um 0.09683731 daneben\u001b[0m\n",
      "neues weight1: -0.37087537 und neues bias1: -0.54143647\n",
      "neues weight2: -3.25030682 und neues bias2: -2.45094471\n",
      "Wir haben 3.0538555088334154 und 0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.57861207\n",
      "\u001b[31mIch lag um 0.09376358 daneben\u001b[0m\n",
      "neues weight1: -0.36843284 und neues bias1: -0.54063666\n",
      "neues weight2: -3.24855891 und neues bias2: -2.45281998\n",
      "Wir haben 3.2745491628777286 und 0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: 0.60421435\n",
      "\u001b[31mIch lag um 0.08906284 daneben\u001b[0m\n",
      "neues weight1: -0.36626445 und neues bias1: -0.53997446\n",
      "neues weight2: -3.24688267 und neues bias2: -2.45460124\n",
      "Wir haben 3.511191734215131 und 0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: 0.62809690\n",
      "\u001b[31mIch lag um 0.08264236 daneben\u001b[0m\n",
      "neues weight1: -0.36440695 und neues bias1: -0.53944544\n",
      "neues weight2: -3.24531340 und neues bias2: -2.45625409\n",
      "Wir haben 3.7649358067924674 und 0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: 0.65017300\n",
      "\u001b[31mIch lag um 0.07441543 daneben\u001b[0m\n",
      "neues weight1: -0.36288379 und neues bias1: -0.53904087\n",
      "neues weight2: -3.24388879 und neues bias2: -2.45774239\n",
      "Wir haben 4.037017258596554 und 0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: 0.67037112\n",
      "\u001b[31mIch lag um 0.06431051 daneben\u001b[0m\n",
      "neues weight1: -0.36170293 und neues bias1: -0.53874837\n",
      "neues weight2: -3.24264848 und neues bias2: -2.45902860\n",
      "Wir haben 4.328761281083057 und 0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: 0.68864692\n",
      "\u001b[31mIch lag um 0.05228328 daneben\u001b[0m\n",
      "neues weight1: -0.36085574 und neues bias1: -0.53855266\n",
      "neues weight2: -3.24163344 und neues bias2: -2.46007427\n",
      "Wir haben 4.641588833612782 und 0.666666666666667 als Trainings Daten\n",
      "Ich sage y vorraus: 0.70499670\n",
      "\u001b[31mIch lag um 0.03833004 daneben\u001b[0m\n",
      "neues weight1: -0.36031732 und neues bias1: -0.53843666\n",
      "neues weight2: -3.24088495 und neues bias2: -2.46084087\n",
      "Wir haben 4.9770235643321135 und 0.6969696969696972 als Trainings Daten\n",
      "Ich sage y vorraus: 0.71946978\n",
      "\u001b[31mIch lag um 0.02250008 daneben\u001b[0m\n",
      "neues weight1: -0.36004852 und neues bias1: -0.53838265\n",
      "neues weight2: -3.24044336 und neues bias2: -2.46129087\n",
      "Wir haben 5.336699231206313 und 0.7272727272727275 als Trainings Daten\n",
      "Ich sage y vorraus: 0.73217675\n",
      "\u001b[31mIch lag um 0.00490402 daneben\u001b[0m\n",
      "neues weight1: -0.35999970 und neues bias1: -0.53837350\n",
      "neues weight2: -3.24034670 und neues bias2: -2.46138895\n",
      "Wir haben 5.72236765935022 und 0.7575757575757578 als Trainings Daten\n",
      "Ich sage y vorraus: 0.74329111\n",
      "\u001b[31mIch lag um -0.01428465 daneben\u001b[0m\n",
      "neues weight1: -0.36011568 und neues bias1: -0.53839377\n",
      "neues weight2: -3.24062925 und neues bias2: -2.46110326\n",
      "Wir haben 6.135907273413176 und 0.7878787878787881 als Trainings Daten\n",
      "Ich sage y vorraus: 0.75304267\n",
      "\u001b[31mIch lag um -0.03483612 daneben\u001b[0m\n",
      "neues weight1: -0.36034119 und neues bias1: -0.53843052\n",
      "neues weight2: -3.24132028 und neues bias2: -2.46040654\n",
      "Wir haben 6.5793322465756825 und 0.8181818181818183 als Trainings Daten\n",
      "Ich sage y vorraus: 0.76170346\n",
      "\u001b[31mIch lag um -0.05647836 daneben\u001b[0m\n",
      "neues weight1: -0.36062588 und neues bias1: -0.53847379\n",
      "neues weight2: -3.24244315 und neues bias2: -2.45927697\n",
      "Wir haben 7.054802310718645 und 0.8484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.76956856\n",
      "\u001b[31mIch lag um -0.07891629 daneben\u001b[0m\n",
      "neues weight1: -0.36092805 und neues bias1: -0.53851662\n",
      "neues weight2: -3.24401486 und neues bias2: -2.45769864\n",
      "Wir haben 7.56463327554629 und 0.8787878787878789 als Trainings Daten\n",
      "Ich sage y vorraus: 0.77693548\n",
      "\u001b[31mIch lag um -0.10185240 daneben\u001b[0m\n",
      "neues weight1: -0.36121674 und neues bias1: -0.53855479\n",
      "neues weight2: -3.24604601 und neues bias2: -2.45566160\n",
      "Wir haben 8.111308307896872 und 0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: 0.78408549\n",
      "\u001b[31mIch lag um -0.12500542 daneben\u001b[0m\n",
      "neues weight1: -0.36147197 und neues bias1: -0.53858625\n",
      "neues weight2: -3.24854127 und neues bias2: -2.45316149\n",
      "Wir haben 8.697490026177835 und 0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79126944\n",
      "\u001b[31mIch lag um -0.14812450 daneben\u001b[0m\n",
      "neues weight1: -0.36168365 und neues bias1: -0.53861059\n",
      "neues weight2: -3.25150001 und neues bias2: -2.45019900\n",
      "Wir haben 9.326033468832199 und 0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79869909\n",
      "\u001b[31mIch lag um -0.17099788 daneben\u001b[0m\n",
      "neues weight1: -0.36184956 und neues bias1: -0.53862838\n",
      "neues weight2: -3.25491723 und neues bias2: -2.44677904\n",
      "Wir haben 10.0 und 1.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.80654367\n",
      "\u001b[31mIch lag um -0.19345633 daneben\u001b[0m\n",
      "neues weight1: -0.36197291 und neues bias1: -0.53864071\n",
      "neues weight2: -3.25878446 und neues bias2: -2.44290991\n",
      "\n",
      "\n",
      "Ich trainiere in Epoche 3001\n",
      "Wir haben 0.01 und -2.0 als Trainings Daten\n",
      "Ich sage y vorraus: -0.83037384\n",
      "\u001b[31mIch lag um 1.16962616 daneben\u001b[0m\n",
      "neues weight1: -0.36082331 und neues bias1: -0.48355040\n",
      "neues weight2: -3.25923647 und neues bias2: -2.47813234\n",
      "Wir haben 0.010722672220103232 und -1.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.00441695\n",
      "\u001b[31mIch lag um 0.96528002 daneben\u001b[0m\n",
      "neues weight1: -0.36028657 und neues bias1: -0.43349344\n",
      "neues weight2: -3.25050714 und neues bias2: -2.49743794\n",
      "Wir haben 0.011497569953977356 und -1.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.15925975\n",
      "\u001b[31mIch lag um 0.78013419 daneben\u001b[0m\n",
      "neues weight1: -0.35980228 und neues bias1: -0.39137240\n",
      "neues weight2: -3.24408378 und neues bias2: -2.51304062\n",
      "Wir haben 0.012328467394420659 und -1.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.29210826\n",
      "\u001b[31mIch lag um 0.61698265 daneben\u001b[0m\n",
      "neues weight1: -0.35937866 und neues bias1: -0.35701168\n",
      "neues weight2: -3.23943967 und neues bias2: -2.52538027\n",
      "Wir haben 0.013219411484660288 und -1.878787878787879 als Trainings Daten\n",
      "Ich sage y vorraus: -1.40205435\n",
      "\u001b[31mIch lag um 0.47673353 daneben\u001b[0m\n",
      "neues weight1: -0.35901945 und neues bias1: -0.32983873\n",
      "neues weight2: -3.23613337 und neues bias2: -2.53491494\n",
      "Wir haben 0.014174741629268055 und -1.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -1.48983286\n",
      "\u001b[31mIch lag um 0.35865198 daneben\u001b[0m\n",
      "neues weight1: -0.35872473 und neues bias1: -0.30904672\n",
      "neues weight2: -3.23381690 und neues bias2: -2.54208798\n",
      "Wir haben 0.01519911082952934 und -1.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -1.55731160\n",
      "\u001b[31mIch lag um 0.26087022 daneben\u001b[0m\n",
      "neues weight1: -0.35849207 und neues bias1: -0.29373923\n",
      "neues weight2: -3.23222807 und neues bias2: -2.54730539\n",
      "Wir haben 0.016297508346206444 und -1.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -1.60695386\n",
      "\u001b[31mIch lag um 0.18092493 daneben\u001b[0m\n",
      "neues weight1: -0.35831759 und neues bias1: -0.28303336\n",
      "neues weight2: -3.23117534 und neues bias2: -2.55092389\n",
      "Wir haben 0.01747528400007684 und -1.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -1.64139373\n",
      "\u001b[31mIch lag um 0.11618202 daneben\u001b[0m\n",
      "neues weight1: -0.35819678 und neues bias1: -0.27612016\n",
      "neues weight2: -3.23052127 und neues bias2: -2.55324753\n",
      "Wir haben 0.01873817422860384 und -1.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -1.66316061\n",
      "\u001b[31mIch lag um 0.06411212 daneben\u001b[0m\n",
      "neues weight1: -0.35812506 und neues bias1: -0.27229231\n",
      "neues weight2: -3.23016798 und neues bias2: -2.55452977\n",
      "Wir haben 0.02009233002565047 und -1.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67453177\n",
      "\u001b[31mIch lag um 0.02243793 daneben\u001b[0m\n",
      "neues weight1: -0.35809809 und neues bias1: -0.27095033\n",
      "neues weight2: -3.23004573 und neues bias2: -2.55497853\n",
      "Wir haben 0.021544346900318846 und -1.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67747409\n",
      "\u001b[31mIch lag um -0.01080742 daneben\u001b[0m\n",
      "neues weight1: -0.35811202 und neues bias1: -0.27159697\n",
      "neues weight2: -3.23010445 und neues bias2: -2.55476238\n",
      "Wir haben 0.023101297000831605 und -1.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67363964\n",
      "\u001b[31mIch lag um -0.03727601 daneben\u001b[0m\n",
      "neues weight1: -0.35816351 und neues bias1: -0.27382589\n",
      "neues weight2: -3.23030781 und neues bias2: -2.55401686\n",
      "Wir haben 0.024770763559917114 und -1.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.66438950\n",
      "\u001b[31mIch lag um -0.05832890 daneben\u001b[0m\n",
      "neues weight1: -0.35824978 und neues bias1: -0.27730848\n",
      "neues weight2: -3.23062909 und neues bias2: -2.55285028\n",
      "Wir haben 0.026560877829466867 und -1.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -1.65082980\n",
      "\u001b[31mIch lag um -0.07507223 daneben\u001b[0m\n",
      "neues weight1: -0.35836857 und neues bias1: -0.28178095\n",
      "neues weight2: -3.23104831 und neues bias2: -2.55134884\n",
      "Wir haben 0.02848035868435802 und -1.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -1.63385067\n",
      "\u001b[31mIch lag um -0.08839612 daneben\u001b[0m\n",
      "neues weight1: -0.35851814 und neues bias1: -0.28703258\n",
      "neues weight2: -3.23155033 und neues bias2: -2.54958091\n",
      "Wir haben 0.030538555088334154 und -1.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -1.61416324\n",
      "\u001b[31mIch lag um -0.09901173 daneben\u001b[0m\n",
      "neues weight1: -0.35869719 und neues bias1: -0.29289562\n",
      "neues weight2: -3.23212354 und neues bias2: -2.54760068\n",
      "Wir haben 0.03274549162877728 und -1.4848484848484849 als Trainings Daten\n",
      "Ich sage y vorraus: -1.59233264\n",
      "\u001b[31mIch lag um -0.10748416 daneben\u001b[0m\n",
      "neues weight1: -0.35890483 und neues bias1: -0.29923674\n",
      "neues weight2: -3.23275889 und neues bias2: -2.54545100\n",
      "Wir haben 0.03511191734215131 und -1.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -1.56880611\n",
      "\u001b[31mIch lag um -0.11426066 daneben\u001b[0m\n",
      "neues weight1: -0.35914055 und neues bias1: -0.30595002\n",
      "neues weight2: -3.23344927 und neues bias2: -2.54316578\n",
      "Wir haben 0.037649358067924674 und -1.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -1.54393648\n",
      "\u001b[31mIch lag um -0.11969406 daneben\u001b[0m\n",
      "neues weight1: -0.35940414 und neues bias1: -0.31295131\n",
      "neues weight2: -3.23418905 und neues bias2: -2.54077190\n",
      "Wir haben 0.040370172585965536 und -1.393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.51800136\n",
      "\u001b[31mIch lag um -0.12406196 daneben\u001b[0m\n",
      "neues weight1: -0.35969571 und neues bias1: -0.32017358\n",
      "neues weight2: -3.23497371 und neues bias2: -2.53829066\n",
      "Wir haben 0.04328761281083057 und -1.3636363636363638 als Trainings Daten\n",
      "Ich sage y vorraus: -1.49121868\n",
      "\u001b[31mIch lag um -0.12758232 daneben\u001b[0m\n",
      "neues weight1: -0.36001559 und neues bias1: -0.32756331\n",
      "neues weight2: -3.23579961 und neues bias2: -2.53573902\n",
      "Wir haben 0.046415888336127795 und -1.3333333333333333 als Trainings Daten\n",
      "Ich sage y vorraus: -1.46375915\n",
      "\u001b[31mIch lag um -0.13042582 daneben\u001b[0m\n",
      "neues weight1: -0.36036437 und neues bias1: -0.33507758\n",
      "neues weight2: -3.23666377 und neues bias2: -2.53313050\n",
      "Wir haben 0.049770235643321115 und -1.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.43575612\n",
      "\u001b[31mIch lag um -0.13272582 daneben\u001b[0m\n",
      "neues weight1: -0.36074283 und neues bias1: -0.34268172\n",
      "neues weight2: -3.23756377 und neues bias2: -2.53047598\n",
      "Wir haben 0.0533669923120631 und -1.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -1.40731346\n",
      "\u001b[31mIch lag um -0.13458619 daneben\u001b[0m\n",
      "neues weight1: -0.36115194 und neues bias1: -0.35034753\n",
      "neues weight2: -3.23849758 und neues bias2: -2.52778426\n",
      "Wir haben 0.05722367659350217 und -1.2424242424242424 als Trainings Daten\n",
      "Ich sage y vorraus: -1.37851173\n",
      "\u001b[31mIch lag um -0.13608748 daneben\u001b[0m\n",
      "neues weight1: -0.36159280 und neues bias1: -0.35805184\n",
      "neues weight2: -3.23946347 und neues bias2: -2.52506251\n",
      "Wir haben 0.06135907273413173 und -1.2121212121212122 als Trainings Daten\n",
      "Ich sage y vorraus: -1.34941305\n",
      "\u001b[31mIch lag um -0.13729184 daneben\u001b[0m\n",
      "neues weight1: -0.36206671 und neues bias1: -0.36577534\n",
      "neues weight2: -3.24045997 und neues bias2: -2.52231667\n",
      "Wir haben 0.06579332246575682 und -1.1818181818181817 als Trainings Daten\n",
      "Ich sage y vorraus: -1.32006495\n",
      "\u001b[31mIch lag um -0.13824677 daneben\u001b[0m\n",
      "neues weight1: -0.36257505 und neues bias1: -0.37350170\n",
      "neues weight2: -3.24148580 und neues bias2: -2.51955174\n",
      "Wir haben 0.07054802310718646 und -1.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: -1.29050331\n",
      "\u001b[31mIch lag um -0.13898816 daneben\u001b[0m\n",
      "neues weight1: -0.36311934 und neues bias1: -0.38121687\n",
      "neues weight2: -3.24253978 und neues bias2: -2.51677197\n",
      "Wir haben 0.07564633275546291 und -1.121212121212121 als Trainings Daten\n",
      "Ich sage y vorraus: -1.26075475\n",
      "\u001b[31mIch lag um -0.13954263 daneben\u001b[0m\n",
      "neues weight1: -0.36370119 und neues bias1: -0.38890850\n",
      "neues weight2: -3.24362083 und neues bias2: -2.51398112\n",
      "Wir haben 0.08111308307896872 und -1.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -1.23083842\n",
      "\u001b[31mIch lag um -0.13992933 daneben\u001b[0m\n",
      "neues weight1: -0.36432227 und neues bias1: -0.39656549\n",
      "neues weight2: -3.24472792 und neues bias2: -2.51118254\n",
      "Wir haben 0.08697490026177834 und -1.0606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.20076746\n",
      "\u001b[31mIch lag um -0.14016140 daneben\u001b[0m\n",
      "neues weight1: -0.36498434 und neues bias1: -0.40417767\n",
      "neues weight2: -3.24586003 und neues bias2: -2.50837931\n",
      "Wir haben 0.093260334688322 und -1.0303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.17055009\n",
      "\u001b[31mIch lag um -0.14024706 daneben\u001b[0m\n",
      "neues weight1: -0.36568918 und neues bias1: -0.41173546\n",
      "neues weight2: -3.24701613 und neues bias2: -2.50557437\n",
      "Wir haben 0.1 und -1.0 als Trainings Daten\n",
      "Ich sage y vorraus: -1.14019044\n",
      "\u001b[31mIch lag um -0.14019044 daneben\u001b[0m\n",
      "neues weight1: -0.36643860 und neues bias1: -0.41922966\n",
      "neues weight2: -3.24819515 und neues bias2: -2.50277056\n",
      "Wir haben 0.10722672220103231 und -0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.10968927\n",
      "\u001b[31mIch lag um -0.13999231 daneben\u001b[0m\n",
      "neues weight1: -0.36723440 und neues bias1: -0.42665131\n",
      "neues weight2: -3.24939594 und neues bias2: -2.49997071\n",
      "Wir haben 0.11497569953977356 und -0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.07904447\n",
      "\u001b[31mIch lag um -0.13965053 daneben\u001b[0m\n",
      "neues weight1: -0.36807834 und neues bias1: -0.43399145\n",
      "neues weight2: -3.25061729 und neues bias2: -2.49717770\n",
      "Wir haben 0.12328467394420659 und -0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.04825140\n",
      "\u001b[31mIch lag um -0.13916049 daneben\u001b[0m\n",
      "neues weight1: -0.36897211 und neues bias1: -0.44124108\n",
      "neues weight2: -3.25185788 und neues bias2: -2.49439449\n",
      "Wir haben 0.13219411484660293 und -0.8787878787878787 als Trainings Daten\n",
      "Ich sage y vorraus: -1.01730327\n",
      "\u001b[31mIch lag um -0.13851539 daneben\u001b[0m\n",
      "neues weight1: -0.36991729 und neues bias1: -0.44839102\n",
      "neues weight2: -3.25311624 und neues bias2: -2.49162418\n",
      "Wir haben 0.14174741629268056 und -0.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -0.98619132\n",
      "\u001b[31mIch lag um -0.13770648 daneben\u001b[0m\n",
      "neues weight1: -0.37091530 und neues bias1: -0.45543182\n",
      "neues weight2: -3.25439075 und neues bias2: -2.48887005\n",
      "Wir haben 0.1519911082952934 und -0.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -0.95490504\n",
      "\u001b[31mIch lag um -0.13672323 daneben\u001b[0m\n",
      "neues weight1: -0.37196737 und neues bias1: -0.46235372\n",
      "neues weight2: -3.25567965 und neues bias2: -2.48613559\n",
      "Wir haben 0.16297508346206444 und -0.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -0.92343228\n",
      "\u001b[31mIch lag um -0.13555349 daneben\u001b[0m\n",
      "neues weight1: -0.37307443 und neues bias1: -0.46914656\n",
      "neues weight2: -3.25698094 und neues bias2: -2.48342452\n",
      "Wir haben 0.17475284000076838 und -0.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -0.89175936\n",
      "\u001b[31mIch lag um -0.13418361 daneben\u001b[0m\n",
      "neues weight1: -0.37423710 und neues bias1: -0.47579977\n",
      "neues weight2: -3.25829244 und neues bias2: -2.48074085\n",
      "Wir haben 0.1873817422860384 und -0.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -0.85987124\n",
      "\u001b[31mIch lag um -0.13259851 daneben\u001b[0m\n",
      "neues weight1: -0.37545556 und neues bias1: -0.48230233\n",
      "neues weight2: -3.25961169 und neues bias2: -2.47808888\n",
      "Wir haben 0.20092330025650468 und -0.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -0.82775153\n",
      "\u001b[31mIch lag um -0.13078184 daneben\u001b[0m\n",
      "neues weight1: -0.37672950 und neues bias1: -0.48864276\n",
      "neues weight2: -3.26093598 und neues bias2: -2.47547324\n",
      "Wir haben 0.21544346900318845 und -0.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -0.79538267\n",
      "\u001b[31mIch lag um -0.12871600 daneben\u001b[0m\n",
      "neues weight1: -0.37805800 und neues bias1: -0.49480909\n",
      "neues weight2: -3.26226231 und neues bias2: -2.47289892\n",
      "Wir haben 0.23101297000831605 und -0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -0.76274600\n",
      "\u001b[31mIch lag um -0.12638236 daneben\u001b[0m\n",
      "neues weight1: -0.37943941 und neues bias1: -0.50078890\n",
      "neues weight2: -3.26358736 und neues bias2: -2.47037127\n",
      "Wir haben 0.24770763559917114 und -0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -0.72982192\n",
      "\u001b[31mIch lag um -0.12376132 daneben\u001b[0m\n",
      "neues weight1: -0.38087126 und neues bias1: -0.50656932\n",
      "neues weight2: -3.26490746 und neues bias2: -2.46789605\n",
      "Wir haben 0.26560877829466867 und -0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -0.69659012\n",
      "\u001b[31mIch lag um -0.12083254 daneben\u001b[0m\n",
      "neues weight1: -0.38235011 und neues bias1: -0.51213710\n",
      "neues weight2: -3.26621856 und neues bias2: -2.46547940\n",
      "Wir haben 0.2848035868435802 und -0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -0.66302972\n",
      "\u001b[31mIch lag um -0.11757517 daneben\u001b[0m\n",
      "neues weight1: -0.38387141 und neues bias1: -0.51747864\n",
      "neues weight2: -3.26751623 und neues bias2: -2.46312789\n",
      "Wir haben 0.30538555088334157 und -0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -0.62911963\n",
      "\u001b[31mIch lag um -0.11396812 daneben\u001b[0m\n",
      "neues weight1: -0.38542932 und neues bias1: -0.52258012\n",
      "neues weight2: -3.26879560 und neues bias2: -2.46084853\n",
      "Wir haben 0.32745491628777285 und -0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: -0.59483888\n",
      "\u001b[31mIch lag um -0.10999040 daneben\u001b[0m\n",
      "neues weight1: -0.38701664 und neues bias1: -0.52742756\n",
      "neues weight2: -3.27005138 und neues bias2: -2.45864872\n",
      "Wir haben 0.3511191734215131 und -0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -0.56016706\n",
      "\u001b[31mIch lag um -0.10562161 daneben\u001b[0m\n",
      "neues weight1: -0.38862457 und neues bias1: -0.53200701\n",
      "neues weight2: -3.27127778 und neues bias2: -2.45653629\n",
      "Wir haben 0.37649358067924676 und -0.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.52508485\n",
      "\u001b[31mIch lag um -0.10084242 daneben\u001b[0m\n",
      "neues weight1: -0.39024263 und neues bias1: -0.53630470\n",
      "neues weight2: -3.27246859 und neues bias2: -2.45451944\n",
      "Wir haben 0.4037017258596556 und -0.3939393939393938 als Trainings Daten\n",
      "Ich sage y vorraus: -0.48957463\n",
      "\u001b[31mIch lag um -0.09563524 daneben\u001b[0m\n",
      "neues weight1: -0.39185847 und neues bias1: -0.54030727\n",
      "neues weight2: -3.27361706 und neues bias2: -2.45260674\n",
      "Wir haben 0.43287612810830595 und -0.36363636363636354 als Trainings Daten\n",
      "Ich sage y vorraus: -0.45362125\n",
      "\u001b[31mIch lag um -0.08998489 daneben\u001b[0m\n",
      "neues weight1: -0.39345783 und neues bias1: -0.54400199\n",
      "neues weight2: -3.27471602 und neues bias2: -2.45080704\n",
      "Wir haben 0.464158883361278 und -0.33333333333333326 als Trainings Daten\n",
      "Ich sage y vorraus: -0.41721286\n",
      "\u001b[31mIch lag um -0.08387952 daneben\u001b[0m\n",
      "neues weight1: -0.39502440 und neues bias1: -0.54737706\n",
      "neues weight2: -3.27575780 und neues bias2: -2.44912945\n",
      "Wir haben 0.49770235643321115 und -0.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -0.38034181\n",
      "\u001b[31mIch lag um -0.07731151 daneben\u001b[0m\n",
      "neues weight1: -0.39653984 und neues bias1: -0.55042194\n",
      "neues weight2: -3.27673432 und neues bias2: -2.44758322\n",
      "Wir haben 0.533669923120631 und -0.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -0.34300576\n",
      "\u001b[31mIch lag um -0.07027848 daneben\u001b[0m\n",
      "neues weight1: -0.39798381 und neues bias1: -0.55312767\n",
      "neues weight2: -3.27763709 und neues bias2: -2.44617765\n",
      "Wir haben 0.5722367659350217 und -0.24242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.30520872\n",
      "\u001b[31mIch lag um -0.06278447 daneben\u001b[0m\n",
      "neues weight1: -0.39933407 und neues bias1: -0.55548730\n",
      "neues weight2: -3.27845731 und neues bias2: -2.44492196\n",
      "Wir haben 0.6135907273413173 und -0.21212121212121215 als Trainings Daten\n",
      "Ich sage y vorraus: -0.26696221\n",
      "\u001b[31mIch lag um -0.05484099 daneben\u001b[0m\n",
      "neues weight1: -0.40056672 und neues bias1: -0.55749622\n",
      "neues weight2: -3.27918595 und neues bias2: -2.44382514\n",
      "Wir haben 0.6579332246575682 und -0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: -0.22828637\n",
      "\u001b[31mIch lag um -0.04646819 daneben\u001b[0m\n",
      "neues weight1: -0.40165652 und neues bias1: -0.55915261\n",
      "neues weight2: -3.27981387 und neues bias2: -2.44289578\n",
      "Wir haben 0.7054802310718645 und -0.15151515151515138 als Trainings Daten\n",
      "Ich sage y vorraus: -0.18921103\n",
      "\u001b[31mIch lag um -0.03769588 daneben\u001b[0m\n",
      "neues weight1: -0.40257731 und neues bias1: -0.56045781\n",
      "neues weight2: -3.28033191 und neues bias2: -2.44214186\n",
      "Wir haben 0.7564633275546291 und -0.12121212121212109 als Trainings Daten\n",
      "Ich sage y vorraus: -0.14977655\n",
      "\u001b[31mIch lag um -0.02856443 daneben\u001b[0m\n",
      "neues weight1: -0.40330264 und neues bias1: -0.56141665\n",
      "neues weight2: -3.28073114 und neues bias2: -2.44157057\n",
      "Wir haben 0.8111308307896873 und -0.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -0.11003452\n",
      "\u001b[31mIch lag um -0.01912543 daneben\u001b[0m\n",
      "neues weight1: -0.40380643 und neues bias1: -0.56203775\n",
      "neues weight2: -3.28100298 und neues bias2: -2.44118806\n",
      "Wir haben 0.8697490026177834 und -0.06060606060606055 als Trainings Daten\n",
      "Ich sage y vorraus: -0.07004809\n",
      "\u001b[31mIch lag um -0.00944203 daneben\u001b[0m\n",
      "neues weight1: -0.40406387 und neues bias1: -0.56233374\n",
      "neues weight2: -3.28113945 und neues bias2: -2.44099922\n",
      "Wir haben 0.9326033468832199 und -0.030303030303030297 als Trainings Daten\n",
      "Ich sage y vorraus: -0.02989194\n",
      "\u001b[31mIch lag um 0.00041109 daneben\u001b[0m\n",
      "neues weight1: -0.40405230 und neues bias1: -0.56232133\n",
      "neues weight2: -3.28113341 und neues bias2: -2.44100744\n",
      "Wir haben 1.0 und 0.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.01034825\n",
      "\u001b[31mIch lag um 0.01034825 daneben\u001b[0m\n",
      "neues weight1: -0.40375226 und neues bias1: -0.56202129\n",
      "neues weight2: -3.28097878 und neues bias2: -2.44121441\n",
      "Wir haben 1.072267222010323 und 0.030303030303030252 als Trainings Daten\n",
      "Ich sage y vorraus: 0.05057675\n",
      "\u001b[31mIch lag um 0.02027372 daneben\u001b[0m\n",
      "neues weight1: -0.40314855 und neues bias1: -0.56145827\n",
      "neues weight2: -3.28067084 und neues bias2: -2.44161988\n",
      "Wir haben 1.1497569953977356 und 0.060606060606060524 als Trainings Daten\n",
      "Ich sage y vorraus: 0.09068930\n",
      "\u001b[31mIch lag um 0.03008324 daneben\u001b[0m\n",
      "neues weight1: -0.40223126 und neues bias1: -0.56066045\n",
      "neues weight2: -3.28020642 und neues bias2: -2.44222155\n",
      "Wir haben 1.232846739442066 und 0.09090909090909083 als Trainings Daten\n",
      "Ich sage y vorraus: 0.13057500\n",
      "\u001b[31mIch lag um 0.03966591 daneben\u001b[0m\n",
      "neues weight1: -0.40099672 und neues bias1: -0.55965908\n",
      "neues weight2: -3.27958419 und neues bias2: -2.44301486\n",
      "Wir haben 1.3219411484660286 und 0.12121212121212108 als Trainings Daten\n",
      "Ich sage y vorraus: 0.17011855\n",
      "\u001b[31mIch lag um 0.04890643 daneben\u001b[0m\n",
      "neues weight1: -0.39944835 und neues bias1: -0.55848780\n",
      "neues weight2: -3.27880483 und neues bias2: -2.44399299\n",
      "Wir haben 1.4174741629268048 und 0.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: 0.20920273\n",
      "\u001b[31mIch lag um 0.05768757 daneben\u001b[0m\n",
      "neues weight1: -0.39759731 und neues bias1: -0.55718193\n",
      "neues weight2: -3.27787122 und neues bias2: -2.44514674\n",
      "Wir haben 1.5199110829529332 und 0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: 0.24771076\n",
      "\u001b[31mIch lag um 0.06589258 daneben\u001b[0m\n",
      "neues weight1: -0.39546286 und neues bias1: -0.55577760\n",
      "neues weight2: -3.27678857 und neues bias2: -2.44646460\n",
      "Wir haben 1.629750834620645 und 0.21212121212121235 als Trainings Daten\n",
      "Ich sage y vorraus: 0.28552861\n",
      "\u001b[31mIch lag um 0.07340740 daneben\u001b[0m\n",
      "neues weight1: -0.39307251 und neues bias1: -0.55431090\n",
      "neues weight2: -3.27556451 und neues bias2: -2.44793274\n",
      "Wir haben 1.7475284000076847 und 0.24242424242424265 als Trainings Daten\n",
      "Ich sage y vorraus: 0.32254668\n",
      "\u001b[31mIch lag um 0.08012244 daneben\u001b[0m\n",
      "neues weight1: -0.39046180 und neues bias1: -0.55281696\n",
      "neues weight2: -3.27420916 und neues bias2: -2.44953519\n",
      "Wir haben 1.873817422860385 und 0.27272727272727293 als Trainings Daten\n",
      "Ich sage y vorraus: 0.35866105\n",
      "\u001b[31mIch lag um 0.08593377 daneben\u001b[0m\n",
      "neues weight1: -0.38767383 und neues bias1: -0.55132910\n",
      "neues weight2: -3.27273510 und neues bias2: -2.45125387\n",
      "Wir haben 2.0092330025650478 und 0.30303030303030315 als Trainings Daten\n",
      "Ich sage y vorraus: 0.39377378\n",
      "\u001b[31mIch lag um 0.09074348 daneben\u001b[0m\n",
      "neues weight1: -0.38475839 und neues bias1: -0.54987808\n",
      "neues weight2: -3.27115741 und neues bias2: -2.45306874\n",
      "Wir haben 2.1544346900318843 und 0.3333333333333334 als Trainings Daten\n",
      "Ich sage y vorraus: 0.42779255\n",
      "\u001b[31mIch lag um 0.09445921 daneben\u001b[0m\n",
      "neues weight1: -0.38177082 und neues bias1: -0.54849137\n",
      "neues weight2: -3.26949364 und neues bias2: -2.45495792\n",
      "Wir haben 2.31012970008316 und 0.3636363636363637 als Trainings Daten\n",
      "Ich sage y vorraus: 0.46062934\n",
      "\u001b[31mIch lag um 0.09699297 daneben\u001b[0m\n",
      "neues weight1: -0.37877055 und neues bias1: -0.54719263\n",
      "neues weight2: -3.26776376 und neues bias2: -2.45689778\n",
      "Wir haben 2.4770763559917115 und 0.39393939393939403 als Trainings Daten\n",
      "Ich sage y vorraus: 0.49219860\n",
      "\u001b[31mIch lag um 0.09825921 daneben\u001b[0m\n",
      "neues weight1: -0.37581934 und neues bias1: -0.54600122\n",
      "neues weight2: -3.26599021 und neues bias2: -2.45886297\n",
      "Wir haben 2.656087782946687 und 0.42424242424242437 als Trainings Daten\n",
      "Ich sage y vorraus: 0.52241505\n",
      "\u001b[31mIch lag um 0.09817263 daneben\u001b[0m\n",
      "neues weight1: -0.37297917 und neues bias1: -0.54493191\n",
      "neues weight2: -3.26419793 und neues bias2: -2.46082642\n",
      "Wir haben 2.848035868435802 und 0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: 0.55119164\n",
      "\u001b[31mIch lag um 0.09664618 daneben\u001b[0m\n",
      "neues weight1: -0.37030990 und neues bias1: -0.54399468\n",
      "neues weight2: -3.26241433 und neues bias2: -2.46275934\n",
      "Wir haben 3.0538555088334154 und 0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.57843827\n",
      "\u001b[31mIch lag um 0.09358979 daneben\u001b[0m\n",
      "neues weight1: -0.36786661 und neues bias1: -0.54319462\n",
      "neues weight2: -3.26066946 und neues bias2: -2.46463114\n",
      "Wir haben 3.2745491628777286 und 0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: 0.60406220\n",
      "\u001b[31mIch lag um 0.08891068 daneben\u001b[0m\n",
      "neues weight1: -0.36569673 und neues bias1: -0.54253197\n",
      "neues weight2: -3.25899594 und neues bias2: -2.46640935\n",
      "Wir haben 3.511191734215131 und 0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: 0.62797067\n",
      "\u001b[31mIch lag um 0.08251612 daneben\u001b[0m\n",
      "neues weight1: -0.36383714 und neues bias1: -0.54200235\n",
      "neues weight2: -3.25742898 und neues bias2: -2.46805967\n",
      "Wir haben 3.7649358067924674 und 0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: 0.65007676\n",
      "\u001b[31mIch lag um 0.07431918 daneben\u001b[0m\n",
      "neues weight1: -0.36231148 und neues bias1: -0.54159712\n",
      "neues weight2: -3.25600615 und neues bias2: -2.46954606\n",
      "Wir haben 4.037017258596554 und 0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: 0.67030844\n",
      "\u001b[31mIch lag um 0.06424784 daneben\u001b[0m\n",
      "neues weight1: -0.36112792 und neues bias1: -0.54130394\n",
      "neues weight2: -3.25476703 und neues bias2: -2.47083101\n",
      "Wir haben 4.328761281083057 und 0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: 0.68862061\n",
      "\u001b[31mIch lag um 0.05225698 daneben\u001b[0m\n",
      "neues weight1: -0.36027811 und neues bias1: -0.54110762\n",
      "neues weight2: -3.25375250 und neues bias2: -2.47187615\n",
      "Wir haben 4.641588833612782 und 0.666666666666667 als Trainings Daten\n",
      "Ich sage y vorraus: 0.70500848\n",
      "\u001b[31mIch lag um 0.03834181 daneben\u001b[0m\n",
      "neues weight1: -0.35973737 und neues bias1: -0.54099113\n",
      "neues weight2: -3.25300378 und neues bias2: -2.47264299\n",
      "Wir haben 4.9770235643321135 und 0.6969696969696972 als Trainings Daten\n",
      "Ich sage y vorraus: 0.71951998\n",
      "\u001b[31mIch lag um 0.02255028 daneben\u001b[0m\n",
      "neues weight1: -0.35946679 und neues bias1: -0.54093676\n",
      "neues weight2: -3.25256121 und neues bias2: -2.47309400\n",
      "Wir haben 5.336699231206313 und 0.7272727272727275 als Trainings Daten\n",
      "Ich sage y vorraus: 0.73226425\n",
      "\u001b[31mIch lag um 0.00499153 daneben\u001b[0m\n",
      "neues weight1: -0.35941686 und neues bias1: -0.54092741\n",
      "neues weight2: -3.25246283 und neues bias2: -2.47319383\n",
      "Wir haben 5.72236765935022 und 0.7575757575757578 als Trainings Daten\n",
      "Ich sage y vorraus: 0.74341332\n",
      "\u001b[31mIch lag um -0.01416243 daneben\u001b[0m\n",
      "neues weight1: -0.35953245 und neues bias1: -0.54094761\n",
      "neues weight2: -3.25274296 und neues bias2: -2.47291058\n",
      "Wir haben 6.135907273413176 und 0.7878787878787881 als Trainings Daten\n",
      "Ich sage y vorraus: 0.75319575\n",
      "\u001b[31mIch lag um -0.03468304 daneben\u001b[0m\n",
      "neues weight1: -0.35975827 und neues bias1: -0.54098441\n",
      "neues weight2: -3.25343094 und neues bias2: -2.47221692\n",
      "Wir haben 6.5793322465756825 und 0.8181818181818183 als Trainings Daten\n",
      "Ich sage y vorraus: 0.76188263\n",
      "\u001b[31mIch lag um -0.05629919 daneben\u001b[0m\n",
      "neues weight1: -0.36004384 und neues bias1: -0.54102781\n",
      "neues weight2: -3.25455023 und neues bias2: -2.47109093\n",
      "Wir haben 7.054802310718645 und 0.8484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.76976856\n",
      "\u001b[31mIch lag um -0.07871628 daneben\u001b[0m\n",
      "neues weight1: -0.36034731 und neues bias1: -0.54107083\n",
      "neues weight2: -3.25611793 und neues bias2: -2.46951661\n",
      "Wir haben 7.56463327554629 und 0.8787878787878789 als Trainings Daten\n",
      "Ich sage y vorraus: 0.77715096\n",
      "\u001b[31mIch lag um -0.10163692 daneben\u001b[0m\n",
      "neues weight1: -0.36063753 und neues bias1: -0.54110919\n",
      "neues weight2: -3.25814477 und neues bias2: -2.46748387\n",
      "Wir haben 8.111308307896872 und 0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: 0.78431136\n",
      "\u001b[31mIch lag um -0.12477955 daneben\u001b[0m\n",
      "neues weight1: -0.36089434 und neues bias1: -0.54114085\n",
      "neues weight2: -3.26063550 und neues bias2: -2.46498828\n",
      "Wir haben 8.697490026177835 und 0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79150117\n",
      "\u001b[31mIch lag um -0.14789277 daneben\u001b[0m\n",
      "neues weight1: -0.36110752 und neues bias1: -0.54116537\n",
      "neues weight2: -3.26358959 und neues bias2: -2.46203042\n",
      "Wir haben 9.326033468832199 und 0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79893282\n",
      "\u001b[31mIch lag um -0.17076415 daneben\u001b[0m\n",
      "neues weight1: -0.36127476 und neues bias1: -0.54118330\n",
      "neues weight2: -3.26700213 und neues bias2: -2.45861514\n",
      "Wir haben 10.0 und 1.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.80677629\n",
      "\u001b[31mIch lag um -0.19322371 daneben\u001b[0m\n",
      "neues weight1: -0.36139922 und neues bias1: -0.54119574\n",
      "neues weight2: -3.27086470 und neues bias2: -2.45475067\n",
      "\n",
      "\n",
      "Ich trainiere in Epoche 4001\n",
      "Wir haben 0.01 und -2.0 als Trainings Daten\n",
      "Ich sage y vorraus: -0.83001917\n",
      "\u001b[31mIch lag um 1.16998083 daneben\u001b[0m\n",
      "neues weight1: -0.36034399 und neues bias1: -0.48562021\n",
      "neues weight2: -3.26938112 und neues bias2: -2.48812514\n",
      "Wir haben 0.010722672220103232 und -1.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.00445761\n",
      "\u001b[31mIch lag um 0.96523936 daneben\u001b[0m\n",
      "neues weight1: -0.35980660 und neues bias1: -0.43550339\n",
      "neues weight2: -3.26062048 und neues bias2: -2.50742992\n",
      "Wir haben 0.011497569953977356 und -1.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.15966473\n",
      "\u001b[31mIch lag um 0.77972921 daneben\u001b[0m\n",
      "neues weight1: -0.35932186 und neues bias1: -0.39334301\n",
      "neues weight2: -3.25417451 und neues bias2: -2.52302451\n",
      "Wir haben 0.012328467394420659 und -1.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.29281065\n",
      "\u001b[31mIch lag um 0.61628026 daneben\u001b[0m\n",
      "neues weight1: -0.35889804 und neues bias1: -0.35896564\n",
      "neues weight2: -3.24951492 und neues bias2: -2.53535011\n",
      "Wir haben 0.013219411484660288 und -1.878787878787879 als Trainings Daten\n",
      "Ich sage y vorraus: -1.40296643\n",
      "\u001b[31mIch lag um 0.47582145 daneben\u001b[0m\n",
      "neues weight1: -0.35853889 und neues bias1: -0.33179714\n",
      "neues weight2: -3.24619866 und neues bias2: -2.54486654\n",
      "Wir haben 0.014174741629268055 und -1.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -1.49086298\n",
      "\u001b[31mIch lag um 0.35762187 daneben\u001b[0m\n",
      "neues weight1: -0.35824447 und neues bias1: -0.31102664\n",
      "neues weight2: -3.24387634 und neues bias2: -2.55201898\n",
      "Wir haben 0.01519911082952934 und -1.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -1.55837722\n",
      "\u001b[31mIch lag um 0.25980459 daneben\u001b[0m\n",
      "neues weight1: -0.35801232 und neues bias1: -0.29575267\n",
      "neues weight2: -3.24228471 und neues bias2: -2.55721507\n",
      "Wir haben 0.016297508346206444 und -1.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -1.60798886\n",
      "\u001b[31mIch lag um 0.17988993 daneben\u001b[0m\n",
      "neues weight1: -0.35783851 und neues bias1: -0.28508741\n",
      "neues weight2: -3.24123140 und neues bias2: -2.56081287\n",
      "Wir haben 0.01747528400007684 und -1.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -1.64235044\n",
      "\u001b[31mIch lag um 0.11522532 daneben\u001b[0m\n",
      "neues weight1: -0.35771846 und neues bias1: -0.27821775\n",
      "neues weight2: -3.24057838 und neues bias2: -2.56311738\n",
      "Wir haben 0.01873817422860384 und -1.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -1.66400855\n",
      "\u001b[31mIch lag um 0.06326417 daneben\u001b[0m\n",
      "neues weight1: -0.35764754 und neues bias1: -0.27443314\n",
      "neues weight2: -3.24022732 und neues bias2: -2.56438266\n",
      "Wir haben 0.02009233002565047 und -1.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67525475\n",
      "\u001b[31mIch lag um 0.02171494 daneben\u001b[0m\n",
      "neues weight1: -0.35762139 und neues bias1: -0.27313187\n",
      "neues weight2: -3.24010815 und neues bias2: -2.56481696\n",
      "Wir haben 0.021544346900318846 und -1.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67806679\n",
      "\u001b[31mIch lag um -0.01140013 daneben\u001b[0m\n",
      "neues weight1: -0.35763612 und neues bias1: -0.27381529\n",
      "neues weight2: -3.24017055 und neues bias2: -2.56458896\n",
      "Wir haben 0.023101297000831605 und -1.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67410443\n",
      "\u001b[31mIch lag um -0.03774079 daneben\u001b[0m\n",
      "neues weight1: -0.35768835 und neues bias1: -0.27607630\n",
      "neues weight2: -3.24037799 und neues bias2: -2.56383414\n",
      "Wir haben 0.024770763559917114 und -1.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.66473375\n",
      "\u001b[31mIch lag um -0.05867314 daneben\u001b[0m\n",
      "neues weight1: -0.35777529 und neues bias1: -0.27958602\n",
      "neues weight2: -3.24070359 und neues bias2: -2.56266068\n",
      "Wir haben 0.026560877829466867 und -1.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -1.65106383\n",
      "\u001b[31mIch lag um -0.07530625 daneben\u001b[0m\n",
      "neues weight1: -0.35789467 und neues bias1: -0.28408071\n",
      "neues weight2: -3.24112726 und neues bias2: -2.56115455\n",
      "Wir haben 0.02848035868435802 und -1.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -1.63398621\n",
      "\u001b[31mIch lag um -0.08853166 daneben\u001b[0m\n",
      "neues weight1: -0.35804474 und neues bias1: -0.28934993\n",
      "neues weight2: -3.24163377 und neues bias2: -2.55938392\n",
      "Wir haben 0.030538555088334154 und -1.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -1.61421239\n",
      "\u001b[31mIch lag um -0.09906088 daneben\u001b[0m\n",
      "neues weight1: -0.35822420 und neues bias1: -0.29522632\n",
      "neues weight2: -3.24221144 und neues bias2: -2.55740270\n",
      "Wir haben 0.03274549162877728 und -1.4848484848484849 als Trainings Daten\n",
      "Ich sage y vorraus: -1.59230716\n",
      "\u001b[31mIch lag um -0.10745867 daneben\u001b[0m\n",
      "neues weight1: -0.35843215 und neues bias1: -0.30157699\n",
      "neues weight2: -3.24285118 und neues bias2: -2.55525353\n",
      "Wir haben 0.03511191734215131 und -1.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -1.56871698\n",
      "\u001b[31mIch lag um -0.11417152 daneben\u001b[0m\n",
      "neues weight1: -0.35866809 und neues bias1: -0.30829650\n",
      "neues weight2: -3.24354584 und neues bias2: -2.55297010\n",
      "Wir haben 0.037649358067924674 und -1.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -1.54379370\n",
      "\u001b[31mIch lag um -0.11955127 daneben\u001b[0m\n",
      "neues weight1: -0.35893181 und neues bias1: -0.31530115\n",
      "neues weight2: -3.24428977 und neues bias2: -2.55057907\n",
      "Wir haben 0.040370172585965536 und -1.393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.51781385\n",
      "\u001b[31mIch lag um -0.12387445 daneben\u001b[0m\n",
      "neues weight1: -0.35922341 und neues bias1: -0.32252433\n",
      "neues weight2: -3.24507843 und neues bias2: -2.54810158\n",
      "Wir haben 0.04328761281083057 und -1.3636363636363638 als Trainings Daten\n",
      "Ich sage y vorraus: -1.49099429\n",
      "\u001b[31mIch lag um -0.12735792 daneben\u001b[0m\n",
      "neues weight1: -0.35954324 und neues bias1: -0.32991292\n",
      "neues weight2: -3.24590819 und neues bias2: -2.54555442\n",
      "Wir haben 0.046415888336127795 und -1.3333333333333333 als Trainings Daten\n",
      "Ich sage y vorraus: -1.46350468\n",
      "\u001b[31mIch lag um -0.13017135 daneben\u001b[0m\n",
      "neues weight1: -0.35989189 und neues bias1: -0.33742432\n",
      "neues weight2: -3.24677606 und neues bias2: -2.54295100\n",
      "Wir haben 0.049770235643321115 und -1.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.43547743\n",
      "\u001b[31mIch lag um -0.13244713 daneben\u001b[0m\n",
      "neues weight1: -0.36027014 und neues bias1: -0.34502418\n",
      "neues weight2: -3.24767961 und neues bias2: -2.54030205\n",
      "Wir haben 0.0533669923120631 und -1.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -1.40701557\n",
      "\u001b[31mIch lag um -0.13428830 daneben\u001b[0m\n",
      "neues weight1: -0.36067895 und neues bias1: -0.35268457\n",
      "neues weight2: -3.24861682 und neues bias2: -2.53761629\n",
      "Wir haben 0.05722367659350217 und -1.2424242424242424 als Trainings Daten\n",
      "Ich sage y vorraus: -1.37819889\n",
      "\u001b[31mIch lag um -0.13577465 daneben\u001b[0m\n",
      "neues weight1: -0.36111946 und neues bias1: -0.36038252\n",
      "neues weight2: -3.24958597 und neues bias2: -2.53490080\n",
      "Wir haben 0.06135907273413173 und -1.2121212121212122 als Trainings Daten\n",
      "Ich sage y vorraus: -1.34908889\n",
      "\u001b[31mIch lag um -0.13696768 daneben\u001b[0m\n",
      "neues weight1: -0.36159293 und neues bias1: -0.36809892\n",
      "neues weight2: -3.25058559 und neues bias2: -2.53216144\n",
      "Wir haben 0.06579332246575682 und -1.1818181818181817 als Trainings Daten\n",
      "Ich sage y vorraus: -1.31973251\n",
      "\u001b[31mIch lag um -0.13791433 daneben\u001b[0m\n",
      "neues weight1: -0.36210077 und neues bias1: -0.37581761\n",
      "neues weight2: -3.25161440 und neues bias2: -2.52940316\n",
      "Wir haben 0.07054802310718646 und -1.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: -1.29016517\n",
      "\u001b[31mIch lag um -0.13865002 daneben\u001b[0m\n",
      "neues weight1: -0.36264448 und neues bias1: -0.38352467\n",
      "neues weight2: -3.25267123 und neues bias2: -2.52663016\n",
      "Wir haben 0.07564633275546291 und -1.121212121212121 als Trainings Daten\n",
      "Ich sage y vorraus: -1.26041308\n",
      "\u001b[31mIch lag um -0.13920096 daneben\u001b[0m\n",
      "neues weight1: -0.36322569 und neues bias1: -0.39120787\n",
      "neues weight2: -3.25375501 und neues bias2: -2.52384614\n",
      "Wir haben 0.08111308307896872 und -1.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -1.23049507\n",
      "\u001b[31mIch lag um -0.13958598 daneben\u001b[0m\n",
      "neues weight1: -0.36384607 und neues bias1: -0.39885622\n",
      "neues weight2: -3.25486470 und neues bias2: -2.52105442\n",
      "Wir haben 0.08697490026177834 und -1.0606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.20042399\n",
      "\u001b[31mIch lag um -0.13981793 daneben\u001b[0m\n",
      "neues weight1: -0.36450737 und neues bias1: -0.40645961\n",
      "neues weight2: -3.25599930 und neues bias2: -2.51825806\n",
      "Wir haben 0.093260334688322 und -1.0303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.17020782\n",
      "\u001b[31mIch lag um -0.13990479 daneben\u001b[0m\n",
      "neues weight1: -0.36521139 und neues bias1: -0.41400853\n",
      "neues weight2: -3.25715777 und neues bias2: -2.51545996\n",
      "Wir haben 0.1 und -1.0 als Trainings Daten\n",
      "Ich sage y vorraus: -1.13985052\n",
      "\u001b[31mIch lag um -0.13985052 daneben\u001b[0m\n",
      "neues weight1: -0.36595992 und neues bias1: -0.42149387\n",
      "neues weight2: -3.25833904 und neues bias2: -2.51266295\n",
      "Wir haben 0.10722672220103231 und -0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.10935270\n",
      "\u001b[31mIch lag um -0.13965573 daneben\u001b[0m\n",
      "neues weight1: -0.36675477 und neues bias1: -0.42890667\n",
      "neues weight2: -3.25954198 und neues bias2: -2.50986984\n",
      "Wir haben 0.11497569953977356 und -0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.07871209\n",
      "\u001b[31mIch lag um -0.13931815 daneben\u001b[0m\n",
      "neues weight1: -0.36759771 und neues bias1: -0.43623806\n",
      "neues weight2: -3.26076539 und neues bias2: -2.50708347\n",
      "Wir haben 0.12328467394420659 und -0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.04792399\n",
      "\u001b[31mIch lag um -0.13883308 daneben\u001b[0m\n",
      "neues weight1: -0.36849041 und neues bias1: -0.44347906\n",
      "neues weight2: -3.26200791 und neues bias2: -2.50430681\n",
      "Wir haben 0.13219411484660293 und -0.8787878787878787 als Trainings Daten\n",
      "Ich sage y vorraus: -1.01698150\n",
      "\u001b[31mIch lag um -0.13819363 daneben\u001b[0m\n",
      "neues weight1: -0.36943447 und neues bias1: -0.45062051\n",
      "neues weight2: -3.26326811 und neues bias2: -2.50154294\n",
      "Wir haben 0.14174741629268056 und -0.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -0.98587583\n",
      "\u001b[31mIch lag um -0.13739098 daneben\u001b[0m\n",
      "neues weight1: -0.37043130 und neues bias1: -0.45765299\n",
      "neues weight2: -3.26454437 und neues bias2: -2.49879512\n",
      "Wir haben 0.1519911082952934 und -0.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -0.95459639\n",
      "\u001b[31mIch lag um -0.13641458 daneben\u001b[0m\n",
      "neues weight1: -0.37148214 und neues bias1: -0.46456677\n",
      "neues weight2: -3.26583491 und neues bias2: -2.49606683\n",
      "Wir haben 0.16297508346206444 und -0.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -0.92313101\n",
      "\u001b[31mIch lag um -0.13525222 daneben\u001b[0m\n",
      "neues weight1: -0.37258791 und neues bias1: -0.47135172\n",
      "neues weight2: -3.26713775 und neues bias2: -2.49336178\n",
      "Wir haben 0.17475284000076838 und -0.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -0.89146600\n",
      "\u001b[31mIch lag um -0.13389025 daneben\u001b[0m\n",
      "neues weight1: -0.37374924 und neues bias1: -0.47799728\n",
      "neues weight2: -3.26845069 und neues bias2: -2.49068398\n",
      "Wir haben 0.1873817422860384 und -0.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -0.85958629\n",
      "\u001b[31mIch lag um -0.13231356 daneben\u001b[0m\n",
      "neues weight1: -0.37496632 und neues bias1: -0.48449245\n",
      "neues weight2: -3.26977130 und neues bias2: -2.48803771\n",
      "Wir haben 0.20092330025650468 und -0.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -0.82747548\n",
      "\u001b[31mIch lag um -0.13050578 daneben\u001b[0m\n",
      "neues weight1: -0.37623883 und neues bias1: -0.49082577\n",
      "neues weight2: -3.27109685 und neues bias2: -2.48542759\n",
      "Wir haben 0.21544346900318845 und -0.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -0.79511601\n",
      "\u001b[31mIch lag um -0.12844934 daneben\u001b[0m\n",
      "neues weight1: -0.37756586 und neues bias1: -0.49698528\n",
      "neues weight2: -3.27242435 und neues bias2: -2.48285861\n",
      "Wir haben 0.23101297000831605 und -0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -0.76248923\n",
      "\u001b[31mIch lag um -0.12612559 daneben\u001b[0m\n",
      "neues weight1: -0.37894577 und neues bias1: -0.50295858\n",
      "neues weight2: -3.27375048 und neues bias2: -2.48033609\n",
      "Wir haben 0.24770763559917114 und -0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -0.72957555\n",
      "\u001b[31mIch lag um -0.12351494 daneben\u001b[0m\n",
      "neues weight1: -0.38037609 und neues bias1: -0.50873282\n",
      "neues weight2: -3.27507157 und neues bias2: -2.47786580\n",
      "Wir haben 0.26560877829466867 und -0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -0.69635465\n",
      "\u001b[31mIch lag um -0.12059707 daneben\u001b[0m\n",
      "neues weight1: -0.38185339 und neues bias1: -0.51429475\n",
      "neues weight2: -3.27638357 und neues bias2: -2.47545385\n",
      "Wir haben 0.2848035868435802 und -0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -0.66280567\n",
      "\u001b[31mIch lag um -0.11735113 daneben\u001b[0m\n",
      "neues weight1: -0.38337312 und neues bias1: -0.51963081\n",
      "neues weight2: -3.27768205 und neues bias2: -2.47310683\n",
      "Wir haben 0.30538555088334157 und -0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -0.62890755\n",
      "\u001b[31mIch lag um -0.11375603 daneben\u001b[0m\n",
      "neues weight1: -0.38492947 und neues bias1: -0.52472716\n",
      "neues weight2: -3.27896216 und neues bias2: -2.47083171\n",
      "Wir haben 0.32745491628777285 und -0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: -0.59463932\n",
      "\u001b[31mIch lag um -0.10979083 daneben\u001b[0m\n",
      "neues weight1: -0.38651524 und neues bias1: -0.52956986\n",
      "neues weight2: -3.28021858 und neues bias2: -2.46863589\n",
      "Wir haben 0.3511191734215131 und -0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -0.55998057\n",
      "\u001b[31mIch lag um -0.10543512 daneben\u001b[0m\n",
      "neues weight1: -0.38812165 und neues bias1: -0.53414497\n",
      "neues weight2: -3.28144557 und neues bias2: -2.46652719\n",
      "Wir haben 0.37649358067924676 und -0.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.52491200\n",
      "\u001b[31mIch lag um -0.10066958 daneben\u001b[0m\n",
      "neues weight1: -0.38973822 und neues bias1: -0.53843873\n",
      "neues weight2: -3.28263688 und neues bias2: -2.46451380\n",
      "Wir haben 0.4037017258596556 und -0.3939393939393938 als Trainings Daten\n",
      "Ich sage y vorraus: -0.48941601\n",
      "\u001b[31mIch lag um -0.09547661 daneben\u001b[0m\n",
      "neues weight1: -0.39135265 und neues bias1: -0.54243779\n",
      "neues weight2: -3.28378581 und neues bias2: -2.46260427\n",
      "Wir haben 0.43287612810830595 und -0.36363636363636354 als Trainings Daten\n",
      "Ich sage y vorraus: -0.45347744\n",
      "\u001b[31mIch lag um -0.08984107 daneben\u001b[0m\n",
      "neues weight1: -0.39295067 und neues bias1: -0.54612942\n",
      "neues weight2: -3.28488516 und neues bias2: -2.46080745\n",
      "Wir haben 0.464158883361278 und -0.33333333333333326 als Trainings Daten\n",
      "Ich sage y vorraus: -0.41708441\n",
      "\u001b[31mIch lag um -0.08375108 daneben\u001b[0m\n",
      "neues weight1: -0.39451601 und neues bias1: -0.54950185\n",
      "neues weight2: -3.28592730 und neues bias2: -2.45913242\n",
      "Wir haben 0.49770235643321115 und -0.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -0.38022930\n",
      "\u001b[31mIch lag um -0.07719900 daneben\u001b[0m\n",
      "neues weight1: -0.39603036 und neues bias1: -0.55254453\n",
      "neues weight2: -3.28690412 und neues bias2: -2.45758844\n",
      "Wir haben 0.533669923120631 und -0.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -0.34290973\n",
      "\u001b[31mIch lag um -0.07018245 daneben\u001b[0m\n",
      "neues weight1: -0.39747339 und neues bias1: -0.55524851\n",
      "neues weight2: -3.28780718 und neues bias2: -2.45618480\n",
      "Wir haben 0.5722367659350217 und -0.24242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.30512966\n",
      "\u001b[31mIch lag um -0.06270542 daneben\u001b[0m\n",
      "neues weight1: -0.39882291 und neues bias1: -0.55760683\n",
      "neues weight2: -3.28862768 und neues bias2: -2.45493069\n",
      "Wir haben 0.6135907273413173 und -0.21212121212121215 als Trainings Daten\n",
      "Ich sage y vorraus: -0.26690059\n",
      "\u001b[31mIch lag um -0.05477938 daneben\u001b[0m\n",
      "neues weight1: -0.40005503 und neues bias1: -0.55961489\n",
      "neues weight2: -3.28935661 und neues bias2: -2.45383510\n",
      "Wir haben 0.6579332246575682 und -0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: -0.22824259\n",
      "\u001b[31mIch lag um -0.04642441 daneben\u001b[0m\n",
      "neues weight1: -0.40114455 und neues bias1: -0.56127086\n",
      "neues weight2: -3.28998483 und neues bias2: -2.45290661\n",
      "Wir haben 0.7054802310718645 und -0.15151515151515138 als Trainings Daten\n",
      "Ich sage y vorraus: -0.18918540\n",
      "\u001b[31mIch lag um -0.03767024 daneben\u001b[0m\n",
      "neues weight1: -0.40206534 und neues bias1: -0.56257605\n",
      "neues weight2: -3.29050322 und neues bias2: -2.45215321\n",
      "Wir haben 0.7564633275546291 und -0.12121212121212109 als Trainings Daten\n",
      "Ich sage y vorraus: -0.14976928\n",
      "\u001b[31mIch lag um -0.02855716 daneben\u001b[0m\n",
      "neues weight1: -0.40279097 und neues bias1: -0.56353530\n",
      "neues weight2: -3.29090286 und neues bias2: -2.45158206\n",
      "Wir haben 0.8111308307896873 und -0.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -0.11004572\n",
      "\u001b[31mIch lag um -0.01913663 daneben\u001b[0m\n",
      "neues weight1: -0.40329540 und neues bias1: -0.56415718\n",
      "neues weight2: -3.29117518 und neues bias2: -2.45119933\n",
      "Wir haben 0.8697490026177834 und -0.06060606060606055 als Trainings Daten\n",
      "Ich sage y vorraus: -0.07007773\n",
      "\u001b[31mIch lag um -0.00947167 daneben\u001b[0m\n",
      "neues weight1: -0.40355382 und neues bias1: -0.56445430\n",
      "neues weight2: -3.29131223 und neues bias2: -2.45100990\n",
      "Wir haben 0.9326033468832199 und -0.030303030303030297 als Trainings Daten\n",
      "Ich sage y vorraus: -0.02993984\n",
      "\u001b[31mIch lag um 0.00036319 daneben\u001b[0m\n",
      "neues weight1: -0.40354359 und neues bias1: -0.56444333\n",
      "neues weight2: -3.29130689 und neues bias2: -2.45101716\n",
      "Wir haben 1.0 und 0.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.01028243\n",
      "\u001b[31mIch lag um 0.01028243 daneben\u001b[0m\n",
      "neues weight1: -0.40324526 und neues bias1: -0.56414500\n",
      "neues weight2: -3.29115310 und neues bias2: -2.45122281\n",
      "Wir haben 1.072267222010323 und 0.030303030303030252 als Trainings Daten\n",
      "Ich sage y vorraus: 0.05049355\n",
      "\u001b[31mIch lag um 0.02019052 daneben\u001b[0m\n",
      "neues weight1: -0.40264361 und neues bias1: -0.56358390\n",
      "neues weight2: -3.29084615 und neues bias2: -2.45162662\n",
      "Wir haben 1.1497569953977356 und 0.060606060606060524 als Trainings Daten\n",
      "Ich sage y vorraus: 0.09058944\n",
      "\u001b[31mIch lag um 0.02998338 daneben\u001b[0m\n",
      "neues weight1: -0.40172871 und neues bias1: -0.56278816\n",
      "neues weight2: -3.29038290 und neues bias2: -2.45222629\n",
      "Wir haben 1.232846739442066 und 0.09090909090909083 als Trainings Daten\n",
      "Ich sage y vorraus: 0.13045939\n",
      "\u001b[31mIch lag um 0.03955030 daneben\u001b[0m\n",
      "neues weight1: -0.40049687 und neues bias1: -0.56178898\n",
      "neues weight2: -3.28976202 und neues bias2: -2.45301729\n",
      "Wir haben 1.3219411484660286 und 0.12121212121212108 als Trainings Daten\n",
      "Ich sage y vorraus: 0.16998833\n",
      "\u001b[31mIch lag um 0.04877620 daneben\u001b[0m\n",
      "neues weight1: -0.39895146 und neues bias1: -0.56061993\n",
      "neues weight2: -3.28898422 und neues bias2: -2.45399282\n",
      "Wir haben 1.4174741629268048 und 0.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: 0.20905921\n",
      "\u001b[31mIch lag um 0.05754406 daneben\u001b[0m\n",
      "neues weight1: -0.39710356 und neues bias1: -0.55931628\n",
      "neues weight2: -3.28805236 und neues bias2: -2.45514370\n",
      "Wir haben 1.5199110829529332 und 0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: 0.24755549\n",
      "\u001b[31mIch lag um 0.06573731 daneben\u001b[0m\n",
      "neues weight1: -0.39497238 und neues bias1: -0.55791411\n",
      "neues weight2: -3.28697167 und neues bias2: -2.45645845\n",
      "Wir haben 1.629750834620645 und 0.21212121212121235 als Trainings Daten\n",
      "Ich sage y vorraus: 0.28536331\n",
      "\u001b[31mIch lag um 0.07324210 daneben\u001b[0m\n",
      "neues weight1: -0.39258533 und neues bias1: -0.55644943\n",
      "neues weight2: -3.28574978 und neues bias2: -2.45792329\n",
      "Wir haben 1.7475284000076847 und 0.24242424242424265 als Trainings Daten\n",
      "Ich sage y vorraus: 0.32237329\n",
      "\u001b[31mIch lag um 0.07994905 daneben\u001b[0m\n",
      "neues weight1: -0.38997785 und neues bias1: -0.55495734\n",
      "neues weight2: -3.28439677 und neues bias2: -2.45952227\n",
      "Wir haben 1.873817422860385 und 0.27272727272727293 als Trainings Daten\n",
      "Ich sage y vorraus: 0.35848167\n",
      "\u001b[31mIch lag um 0.08575440 daneben\u001b[0m\n",
      "neues weight1: -0.38719295 und neues bias1: -0.55347112\n",
      "neues weight2: -3.28292523 und neues bias2: -2.46123736\n",
      "Wir haben 2.0092330025650478 und 0.30303030303030315 als Trainings Daten\n",
      "Ich sage y vorraus: 0.39359074\n",
      "\u001b[31mIch lag um 0.09056044 daneben\u001b[0m\n",
      "neues weight1: -0.38428029 und neues bias1: -0.55202148\n",
      "neues weight2: -3.28135020 und neues bias2: -2.46304856\n",
      "Wir haben 2.1544346900318843 und 0.3333333333333334 als Trainings Daten\n",
      "Ich sage y vorraus: 0.42760835\n",
      "\u001b[31mIch lag um 0.09427501 daneben\u001b[0m\n",
      "neues weight1: -0.38129511 und neues bias1: -0.55063589\n",
      "neues weight2: -3.27968920 und neues bias2: -2.46493406\n",
      "Wir haben 2.31012970008316 und 0.3636363636363637 als Trainings Daten\n",
      "Ich sage y vorraus: 0.46044668\n",
      "\u001b[31mIch lag um 0.09681032 daneben\u001b[0m\n",
      "neues weight1: -0.37829676 und neues bias1: -0.54933797\n",
      "neues weight2: -3.27796216 und neues bias2: -2.46687027\n",
      "Wir haben 2.4770763559917115 und 0.39393939393939403 als Trainings Daten\n",
      "Ich sage y vorraus: 0.49202041\n",
      "\u001b[31mIch lag um 0.09808102 daneben\u001b[0m\n",
      "neues weight1: -0.37534688 und neues bias1: -0.54814710\n",
      "neues weight2: -3.27619148 und neues bias2: -2.46883189\n",
      "Wir haben 2.656087782946687 und 0.42424242424242437 als Trainings Daten\n",
      "Ich sage y vorraus: 0.52224446\n",
      "\u001b[31mIch lag um 0.09800204 daneben\u001b[0m\n",
      "neues weight1: -0.37250741 und neues bias1: -0.54707806\n",
      "neues weight2: -3.27440202 und neues bias2: -2.47079193\n",
      "Wir haben 2.848035868435802 und 0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: 0.55103196\n",
      "\u001b[31mIch lag um 0.09648650 daneben\u001b[0m\n",
      "neues weight1: -0.36983818 und neues bias1: -0.54614084\n",
      "neues weight2: -3.27262114 und neues bias2: -2.47272166\n",
      "Wir haben 3.0538555088334154 und 0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.57829300\n",
      "\u001b[31mIch lag um 0.09344452 daneben\u001b[0m\n",
      "neues weight1: -0.36739427 und neues bias1: -0.54534057\n",
      "neues weight2: -3.27087880 und neues bias2: -2.47459055\n",
      "Wir haben 3.2745491628777286 und 0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: 0.60393492\n",
      "\u001b[31mIch lag um 0.08878340 daneben\u001b[0m\n",
      "neues weight1: -0.36522316 und neues bias1: -0.54467754\n",
      "neues weight2: -3.26920756 und neues bias2: -2.47636622\n",
      "Wir haben 3.511191734215131 und 0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: 0.62786496\n",
      "\u001b[31mIch lag um 0.08241042 daneben\u001b[0m\n",
      "neues weight1: -0.36336185 und neues bias1: -0.54414743\n",
      "neues weight2: -3.26764253 und neues bias2: -2.47801443\n",
      "Wir haben 3.7649358067924674 und 0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: 0.64999602\n",
      "\u001b[31mIch lag um 0.07423845 daneben\u001b[0m\n",
      "neues weight1: -0.36183410 und neues bias1: -0.54374165\n",
      "neues weight2: -3.26622120 und neues bias2: -2.47949920\n",
      "Wir haben 4.037017258596554 und 0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: 0.67025568\n",
      "\u001b[31mIch lag um 0.06419508 daneben\u001b[0m\n",
      "neues weight1: -0.36064830 und neues bias1: -0.54344792\n",
      "neues weight2: -3.26498308 und neues bias2: -2.48078310\n",
      "Wir haben 4.328761281083057 und 0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: 0.68859818\n",
      "\u001b[31mIch lag um 0.05223454 daneben\u001b[0m\n",
      "neues weight1: -0.35979630 und neues bias1: -0.54325109\n",
      "neues weight2: -3.26396898 und neues bias2: -2.48182779\n",
      "Wir haben 4.641588833612782 und 0.666666666666667 als Trainings Daten\n",
      "Ich sage y vorraus: 0.70501780\n",
      "\u001b[31mIch lag um 0.03835114 daneben\u001b[0m\n",
      "neues weight1: -0.35925363 und neues bias1: -0.54313418\n",
      "neues weight2: -3.26322008 und neues bias2: -2.48259481\n",
      "Wir haben 4.9770235643321135 und 0.6969696969696972 als Trainings Daten\n",
      "Ich sage y vorraus: 0.71956139\n",
      "\u001b[31mIch lag um 0.02259169 daneben\u001b[0m\n",
      "neues weight1: -0.35898156 und neues bias1: -0.54307952\n",
      "neues weight2: -3.26277670 und neues bias2: -2.48304665\n",
      "Wir haben 5.336699231206313 und 0.7272727272727275 als Trainings Daten\n",
      "Ich sage y vorraus: 0.73233682\n",
      "\u001b[31mIch lag um 0.00506409 daneben\u001b[0m\n",
      "neues weight1: -0.35893070 und neues bias1: -0.54306999\n",
      "neues weight2: -3.26267689 und neues bias2: -2.48314793\n",
      "Wir haben 5.72236765935022 und 0.7575757575757578 als Trainings Daten\n",
      "Ich sage y vorraus: 0.74351491\n",
      "\u001b[31mIch lag um -0.01406085 daneben\u001b[0m\n",
      "neues weight1: -0.35904597 und neues bias1: -0.54309013\n",
      "neues weight2: -3.26295501 und neues bias2: -2.48286671\n",
      "Wir haben 6.135907273413176 und 0.7878787878787881 als Trainings Daten\n",
      "Ich sage y vorraus: 0.75332316\n",
      "\u001b[31mIch lag um -0.03455563 daneben\u001b[0m\n",
      "neues weight1: -0.35927204 und neues bias1: -0.54312697\n",
      "neues weight2: -3.26364045 und neues bias2: -2.48217560\n",
      "Wir haben 6.5793322465756825 und 0.8181818181818183 als Trainings Daten\n",
      "Ich sage y vorraus: 0.76203190\n",
      "\u001b[31mIch lag um -0.05614991 daneben\u001b[0m\n",
      "neues weight1: -0.35955835 und neues bias1: -0.54317049\n",
      "neues weight2: -3.26475676 und neues bias2: -2.48105260\n",
      "Wir haben 7.054802310718645 und 0.8484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.76993530\n",
      "\u001b[31mIch lag um -0.07854955 daneben\u001b[0m\n",
      "neues weight1: -0.35986291 und neues bias1: -0.54321366\n",
      "neues weight2: -3.26632113 und neues bias2: -2.47948161\n",
      "Wir haben 7.56463327554629 und 0.8787878787878789 als Trainings Daten\n",
      "Ich sage y vorraus: 0.77733069\n",
      "\u001b[31mIch lag um -0.10145719 daneben\u001b[0m\n",
      "neues weight1: -0.36015440 und neues bias1: -0.54325219\n",
      "neues weight2: -3.26834436 und neues bias2: -2.47745247\n",
      "Wir haben 8.111308307896872 und 0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: 0.78449984\n",
      "\u001b[31mIch lag um -0.12459107 daneben\u001b[0m\n",
      "neues weight1: -0.36041254 und neues bias1: -0.54328402\n",
      "neues weight2: -3.27083131 und neues bias2: -2.47496065\n",
      "Wir haben 8.697490026177835 und 0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79169459\n",
      "\u001b[31mIch lag um -0.14769935 daneben\u001b[0m\n",
      "neues weight1: -0.36062699 und neues bias1: -0.54330867\n",
      "neues weight2: -3.27378153 und neues bias2: -2.47200666\n",
      "Wir haben 9.326033468832199 und 0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79912797\n",
      "\u001b[31mIch lag um -0.17056900 daneben\u001b[0m\n",
      "neues weight1: -0.36079534 und neues bias1: -0.54332673\n",
      "neues weight2: -3.27719015 und neues bias2: -2.46859528\n",
      "Wir haben 10.0 und 1.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.80697056\n",
      "\u001b[31mIch lag um -0.19302944 daneben\u001b[0m\n",
      "neues weight1: -0.36092072 und neues bias1: -0.54333926\n",
      "neues weight2: -3.28104882 und neues bias2: -2.46473469\n",
      "\n",
      "\n",
      "Ich trainiere in Epoche 5001\n",
      "Wir haben 0.01 und -2.0 als Trainings Daten\n",
      "Ich sage y vorraus: -0.82972167\n",
      "\u001b[31mIch lag um 1.17027833 daneben\u001b[0m\n",
      "neues weight1: -0.35994196 und neues bias1: -0.48736492\n",
      "neues weight2: -3.27796706 und neues bias2: -2.49658356\n",
      "Wir haben 0.010722672220103232 und -1.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.00449319\n",
      "\u001b[31mIch lag um 0.96520378 daneben\u001b[0m\n",
      "neues weight1: -0.35940403 und neues bias1: -0.43719776\n",
      "neues weight2: -3.26918008 und neues bias2: -2.51588764\n",
      "Wir haben 0.011497569953977356 und -1.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.16000731\n",
      "\u001b[31mIch lag um 0.77938663 daneben\u001b[0m\n",
      "neues weight1: -0.35891891 und neues bias1: -0.39500436\n",
      "neues weight2: -3.26271512 und neues bias2: -2.53147537\n",
      "Wir haben 0.012328467394420659 und -1.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.29340378\n",
      "\u001b[31mIch lag um 0.61568713 daneben\u001b[0m\n",
      "neues weight1: -0.35849492 und neues bias1: -0.36061311\n",
      "neues weight2: -3.25804255 und neues bias2: -2.54378912\n",
      "Wir haben 0.013219411484660288 und -1.878787878787879 als Trainings Daten\n",
      "Ich sage y vorraus: -1.40373597\n",
      "\u001b[31mIch lag um 0.47505191 daneben\u001b[0m\n",
      "neues weight1: -0.35813582 und neues bias1: -0.33344855\n",
      "neues weight2: -3.25471795 und neues bias2: -2.55329015\n",
      "Wir haben 0.014174741629268055 und -1.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -1.49173146\n",
      "\u001b[31mIch lag um 0.35675339 daneben\u001b[0m\n",
      "neues weight1: -0.35784166 und neues bias1: -0.31269634\n",
      "neues weight2: -3.25239077 und neues bias2: -2.56042522\n",
      "Wir haben 0.01519911082952934 und -1.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -1.55927493\n",
      "\u001b[31mIch lag um 0.25890689 daneben\u001b[0m\n",
      "neues weight1: -0.35760994 und neues bias1: -0.29745078\n",
      "neues weight2: -3.25079684 und neues bias2: -2.56560336\n",
      "Wir haben 0.016297508346206444 und -1.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -1.60886000\n",
      "\u001b[31mIch lag um 0.17901879 daneben\u001b[0m\n",
      "neues weight1: -0.35743669 und neues bias1: -0.28681987\n",
      "neues weight2: -3.24974310 und neues bias2: -2.56918373\n",
      "Wir haben 0.01747528400007684 und -1.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -1.64315490\n",
      "\u001b[31mIch lag um 0.11442086 daneben\u001b[0m\n",
      "neues weight1: -0.35731728 und neues bias1: -0.27998696\n",
      "neues weight2: -3.24909100 und neues bias2: -2.57147215\n",
      "Wir haben 0.01873817422860384 und -1.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -1.66472079\n",
      "\u001b[31mIch lag um 0.06255194 daneben\u001b[0m\n",
      "neues weight1: -0.35724705 und neues bias1: -0.27623880\n",
      "neues weight2: -3.24874186 und neues bias2: -2.57272319\n",
      "Wir haben 0.02009233002565047 und -1.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67586127\n",
      "\u001b[31mIch lag um 0.02110842 daneben\u001b[0m\n",
      "neues weight1: -0.35722159 und neues bias1: -0.27497181\n",
      "neues weight2: -3.24862532 und neues bias2: -2.57314536\n",
      "Wir haben 0.021544346900318846 und -1.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67856329\n",
      "\u001b[31mIch lag um -0.01189663 daneben\u001b[0m\n",
      "neues weight1: -0.35723698 und neues bias1: -0.27568615\n",
      "neues weight2: -3.24869084 und neues bias2: -2.57290743\n",
      "Wir haben 0.023101297000831605 und -1.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67449306\n",
      "\u001b[31mIch lag um -0.03812942 daneben\u001b[0m\n",
      "neues weight1: -0.35728983 und neues bias1: -0.27797410\n",
      "neues weight2: -3.24890173 und neues bias2: -2.57214484\n",
      "Wir haben 0.024770763559917114 und -1.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.66502088\n",
      "\u001b[31mIch lag um -0.05896027 daneben\u001b[0m\n",
      "neues weight1: -0.35737734 und neues bias1: -0.28150656\n",
      "neues weight2: -3.24923097 und neues bias2: -2.57096563\n",
      "Wir haben 0.026560877829466867 und -1.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -1.65125827\n",
      "\u001b[31mIch lag um -0.07550069 daneben\u001b[0m\n",
      "neues weight1: -0.35749721 und neues bias1: -0.28601984\n",
      "neues weight2: -3.24965839 und neues bias2: -2.56945562\n",
      "Wir haben 0.02848035868435802 und -1.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -1.63409792\n",
      "\u001b[31mIch lag um -0.08864338 daneben\u001b[0m\n",
      "neues weight1: -0.35764770 und neues bias1: -0.29130375\n",
      "neues weight2: -3.25016868 und neues bias2: -2.56768275\n",
      "Wir haben 0.030538555088334154 und -1.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -1.61425162\n",
      "\u001b[31mIch lag um -0.09910011 daneben\u001b[0m\n",
      "neues weight1: -0.35782750 und neues bias1: -0.29719125\n",
      "neues weight2: -3.25075010 und neues bias2: -2.56570075\n",
      "Wir haben 0.03274549162877728 und -1.4848484848484849 als Trainings Daten\n",
      "Ich sage y vorraus: -1.59228384\n",
      "\u001b[31mIch lag um -0.10743535 daneben\u001b[0m\n",
      "neues weight1: -0.35803571 und neues bias1: -0.30354985\n",
      "neues weight2: -3.25139351 und neues bias2: -2.56355204\n",
      "Wir haben 0.03511191734215131 und -1.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -1.56864037\n",
      "\u001b[31mIch lag um -0.11409492 daneben\u001b[0m\n",
      "neues weight1: -0.35827183 und neues bias1: -0.31027450\n",
      "neues weight2: -3.25209176 und neues bias2: -2.56127014\n",
      "Wir haben 0.037649358067924674 und -1.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -1.54367223\n",
      "\u001b[31mIch lag um -0.11942980 daneben\u001b[0m\n",
      "neues weight1: -0.35853565 und neues bias1: -0.31728188\n",
      "neues weight2: -3.25283917 und neues bias2: -2.55888155\n",
      "Wir haben 0.040370172585965536 und -1.393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.51765502\n",
      "\u001b[31mIch lag um -0.12371563 daneben\u001b[0m\n",
      "neues weight1: -0.35882728 und neues bias1: -0.32450574\n",
      "neues weight2: -3.25363119 und neues bias2: -2.55640724\n",
      "Wir haben 0.04328761281083057 und -1.3636363636363638 als Trainings Daten\n",
      "Ich sage y vorraus: -1.49080468\n",
      "\u001b[31mIch lag um -0.12716832 daneben\u001b[0m\n",
      "neues weight1: -0.35914707 und neues bias1: -0.33189329\n",
      "neues weight2: -3.25446417 und neues bias2: -2.55386387\n",
      "Wir haben 0.046415888336127795 und -1.3333333333333333 als Trainings Daten\n",
      "Ich sage y vorraus: -1.46329000\n",
      "\u001b[31mIch lag um -0.12995667 daneben\u001b[0m\n",
      "neues weight1: -0.35949560 und neues bias1: -0.33940222\n",
      "neues weight2: -3.25533514 und neues bias2: -2.55126474\n",
      "Wir haben 0.049770235643321115 und -1.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.43524260\n",
      "\u001b[31mIch lag um -0.13221229 daneben\u001b[0m\n",
      "neues weight1: -0.35987367 und neues bias1: -0.34699843\n",
      "neues weight2: -3.25624167 und neues bias2: -2.54862049\n",
      "Wir haben 0.0533669923120631 und -1.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -1.40676476\n",
      "\u001b[31mIch lag um -0.13403749 daneben\u001b[0m\n",
      "neues weight1: -0.36028223 und neues bias1: -0.35465419\n",
      "neues weight2: -3.25718172 und neues bias2: -2.54593974\n",
      "Wir haben 0.05722367659350217 und -1.2424242424242424 als Trainings Daten\n",
      "Ich sage y vorraus: -1.37793568\n",
      "\u001b[31mIch lag um -0.13551144 daneben\u001b[0m\n",
      "neues weight1: -0.36072243 und neues bias1: -0.36234675\n",
      "neues weight2: -3.25815359 und neues bias2: -2.54322951\n",
      "Wir haben 0.06135907273413173 und -1.2121212121212122 als Trainings Daten\n",
      "Ich sage y vorraus: -1.34881629\n",
      "\u001b[31mIch lag um -0.13669508 daneben\u001b[0m\n",
      "neues weight1: -0.36119553 und neues bias1: -0.37005715\n",
      "neues weight2: -3.25915581 und neues bias2: -2.54049561\n",
      "Wir haben 0.06579332246575682 und -1.1818181818181817 als Trainings Daten\n",
      "Ich sage y vorraus: -1.31945306\n",
      "\u001b[31mIch lag um -0.13763488 daneben\u001b[0m\n",
      "neues weight1: -0.36170294 und neues bias1: -0.37776936\n",
      "neues weight2: -3.26018711 und neues bias2: -2.53774291\n",
      "Wir haben 0.07054802310718646 und -1.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: -1.28988103\n",
      "\u001b[31mIch lag um -0.13836588 daneben\u001b[0m\n",
      "neues weight1: -0.36224618 und neues bias1: -0.38546959\n",
      "neues weight2: -3.26124633 und neues bias2: -2.53497559\n",
      "Wir haben 0.07564633275546291 und -1.121212121212121 als Trainings Daten\n",
      "Ich sage y vorraus: -1.26012606\n",
      "\u001b[31mIch lag um -0.13891394 daneben\u001b[0m\n",
      "neues weight1: -0.36282685 und neues bias1: -0.39314568\n",
      "neues weight2: -3.26233238 und neues bias2: -2.53219732\n",
      "Wir haben 0.08111308307896872 und -1.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -1.23020670\n",
      "\u001b[31mIch lag um -0.13929761 daneben\u001b[0m\n",
      "neues weight1: -0.36344664 und neues bias1: -0.40078674\n",
      "neues weight2: -3.26344425 und neues bias2: -2.52941136\n",
      "Wir haben 0.08697490026177834 und -1.0606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.20013557\n",
      "\u001b[31mIch lag um -0.13952951 daneben\u001b[0m\n",
      "neues weight1: -0.36410730 und neues bias1: -0.40838273\n",
      "neues weight2: -3.26458092 und neues bias2: -2.52662077\n",
      "Wir haben 0.093260334688322 und -1.0303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.16992046\n",
      "\u001b[31mIch lag um -0.13961743 daneben\u001b[0m\n",
      "neues weight1: -0.36481062 und neues bias1: -0.41592419\n",
      "neues weight2: -3.26574137 und neues bias2: -2.52382842\n",
      "Wir haben 0.1 und -1.0 als Trainings Daten\n",
      "Ich sage y vorraus: -1.13956517\n",
      "\u001b[31mIch lag um -0.13956517 daneben\u001b[0m\n",
      "neues weight1: -0.36555840 und neues bias1: -0.42340206\n",
      "neues weight2: -3.26692453 und neues bias2: -2.52103712\n",
      "Wir haben 0.10722672220103231 und -0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.10907018\n",
      "\u001b[31mIch lag um -0.13937321 daneben\u001b[0m\n",
      "neues weight1: -0.36635246 und neues bias1: -0.43080744\n",
      "neues weight2: -3.26812927 und neues bias2: -2.51824966\n",
      "Wir haben 0.11497569953977356 und -0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.07843312\n",
      "\u001b[31mIch lag um -0.13903918 daneben\u001b[0m\n",
      "neues weight1: -0.36719454 und neues bias1: -0.43813147\n",
      "neues weight2: -3.26935438 und neues bias2: -2.51546887\n",
      "Wir haben 0.12328467394420659 und -0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.04764920\n",
      "\u001b[31mIch lag um -0.13855829 daneben\u001b[0m\n",
      "neues weight1: -0.36808635 und neues bias1: -0.44536520\n",
      "neues weight2: -3.27059853 und neues bias2: -2.51269771\n",
      "Wir haben 0.13219411484660293 und -0.8787878787878787 als Trainings Daten\n",
      "Ich sage y vorraus: -1.01671146\n",
      "\u001b[31mIch lag um -0.13792359 daneben\u001b[0m\n",
      "neues weight1: -0.36902947 und neues bias1: -0.45249951\n",
      "neues weight2: -3.27186027 und neues bias2: -2.50993924\n",
      "Wir haben 0.14174741629268056 und -0.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -0.98561106\n",
      "\u001b[31mIch lag um -0.13712621 daneben\u001b[0m\n",
      "neues weight1: -0.37002531 und neues bias1: -0.45952501\n",
      "neues weight2: -3.27313799 und neues bias2: -2.50719671\n",
      "Wir haben 0.1519911082952934 und -0.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -0.95433737\n",
      "\u001b[31mIch lag um -0.13615555 daneben\u001b[0m\n",
      "neues weight1: -0.37107511 und neues bias1: -0.46643197\n",
      "neues weight2: -3.27442990 und neues bias2: -2.50447360\n",
      "Wir haben 0.16297508346206444 und -0.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -0.92287819\n",
      "\u001b[31mIch lag um -0.13499940 daneben\u001b[0m\n",
      "neues weight1: -0.37217980 und neues bias1: -0.47321028\n",
      "neues weight2: -3.27573403 und neues bias2: -2.50177361\n",
      "Wir haben 0.17475284000076838 und -0.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -0.89121981\n",
      "\u001b[31mIch lag um -0.13364405 daneben\u001b[0m\n",
      "neues weight1: -0.37334001 und neues bias1: -0.47984941\n",
      "neues weight2: -3.27704819 und neues bias2: -2.49910073\n",
      "Wir haben 0.1873817422860384 und -0.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -0.85934714\n",
      "\u001b[31mIch lag um -0.13207442 daneben\u001b[0m\n",
      "neues weight1: -0.37455593 und neues bias1: -0.48633838\n",
      "neues weight2: -3.27836992 und neues bias2: -2.49645924\n",
      "Wir haben 0.20092330025650468 und -0.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -0.82724380\n",
      "\u001b[31mIch lag um -0.13027410 daneben\u001b[0m\n",
      "neues weight1: -0.37582724 und neues bias1: -0.49266572\n",
      "neues weight2: -3.27969653 und neues bias2: -2.49385376\n",
      "Wir haben 0.21544346900318845 und -0.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -0.79489221\n",
      "\u001b[31mIch lag um -0.12822554 daneben\u001b[0m\n",
      "neues weight1: -0.37715303 und neues bias1: -0.49881950\n",
      "neues weight2: -3.28102501 und neues bias2: -2.49128925\n",
      "Wir haben 0.23101297000831605 und -0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -0.76227371\n",
      "\u001b[31mIch lag um -0.12591008 daneben\u001b[0m\n",
      "neues weight1: -0.37853167 und neues bias1: -0.50478733\n",
      "neues weight2: -3.28235203 und neues bias2: -2.48877105\n",
      "Wir haben 0.24770763559917114 und -0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -0.72936875\n",
      "\u001b[31mIch lag um -0.12330814 daneben\u001b[0m\n",
      "neues weight1: -0.37996071 und neues bias1: -0.51055638\n",
      "neues weight2: -3.28367394 und neues bias2: -2.48630489\n",
      "Wir haben 0.26560877829466867 und -0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -0.69615699\n",
      "\u001b[31mIch lag um -0.12039941 daneben\u001b[0m\n",
      "neues weight1: -0.38143670 und neues bias1: -0.51611340\n",
      "neues weight2: -3.28498670 und neues bias2: -2.48389690\n",
      "Wir haben 0.2848035868435802 und -0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -0.66261759\n",
      "\u001b[31mIch lag um -0.11716305 daneben\u001b[0m\n",
      "neues weight1: -0.38295512 und neues bias1: -0.52144484\n",
      "neues weight2: -3.28628586 und neues bias2: -2.48155364\n",
      "Wir haben 0.30538555088334157 und -0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -0.62872950\n",
      "\u001b[31mIch lag um -0.11357798 daneben\u001b[0m\n",
      "neues weight1: -0.38451016 und neues bias1: -0.52653690\n",
      "neues weight2: -3.28756658 und neues bias2: -2.47928208\n",
      "Wir haben 0.32745491628777285 und -0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: -0.59447176\n",
      "\u001b[31mIch lag um -0.10962327 daneben\u001b[0m\n",
      "neues weight1: -0.38609462 und neues bias1: -0.53137562\n",
      "neues weight2: -3.28882355 und neues bias2: -2.47708961\n",
      "Wir haben 0.3511191734215131 und -0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -0.55982397\n",
      "\u001b[31mIch lag um -0.10527852 daneben\u001b[0m\n",
      "neues weight1: -0.38769975 und neues bias1: -0.53594708\n",
      "neues weight2: -3.29005102 und neues bias2: -2.47498404\n",
      "Wir haben 0.37649358067924676 und -0.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.52476685\n",
      "\u001b[31mIch lag um -0.10052442 daneben\u001b[0m\n",
      "neues weight1: -0.38931508 und neues bias1: -0.54023753\n",
      "neues weight2: -3.29124276 und neues bias2: -2.47297355\n",
      "Wir haben 0.4037017258596556 und -0.3939393939393938 als Trainings Daten\n",
      "Ich sage y vorraus: -0.48928278\n",
      "\u001b[31mIch lag um -0.09534338 daneben\u001b[0m\n",
      "neues weight1: -0.39092831 und neues bias1: -0.54423364\n",
      "neues weight2: -3.29239206 und neues bias2: -2.47106669\n",
      "Wir haben 0.43287612810830595 und -0.36363636363636354 als Trainings Daten\n",
      "Ich sage y vorraus: -0.45335662\n",
      "\u001b[31mIch lag um -0.08972025 daneben\u001b[0m\n",
      "neues weight1: -0.39252521 und neues bias1: -0.54792268\n",
      "neues weight2: -3.29349175 und neues bias2: -2.46927228\n",
      "Wir haben 0.464158883361278 und -0.33333333333333326 als Trainings Daten\n",
      "Ich sage y vorraus: -0.41697649\n",
      "\u001b[31mIch lag um -0.08364315 daneben\u001b[0m\n",
      "neues weight1: -0.39408952 und neues bias1: -0.55129288\n",
      "neues weight2: -3.29453417 und neues bias2: -2.46759942\n",
      "Wir haben 0.49770235643321115 und -0.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -0.38013474\n",
      "\u001b[31mIch lag um -0.07710444 daneben\u001b[0m\n",
      "neues weight1: -0.39560294 und neues bias1: -0.55433370\n",
      "neues weight2: -3.29551126 und neues bias2: -2.46605733\n",
      "Wir haben 0.533669923120631 und -0.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -0.34282898\n",
      "\u001b[31mIch lag um -0.07010171 daneben\u001b[0m\n",
      "neues weight1: -0.39704519 und neues bias1: -0.55703621\n",
      "neues weight2: -3.29641456 und neues bias2: -2.46465529\n",
      "Wir haben 0.5722367659350217 und -0.24242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.30506316\n",
      "\u001b[31mIch lag um -0.06263891 daneben\u001b[0m\n",
      "neues weight1: -0.39839408 und neues bias1: -0.55939343\n",
      "neues weight2: -3.29723530 und neues bias2: -2.46340252\n",
      "Wir haben 0.6135907273413173 und -0.21212121212121215 als Trainings Daten\n",
      "Ich sage y vorraus: -0.26684871\n",
      "\u001b[31mIch lag um -0.05472750 daneben\u001b[0m\n",
      "neues weight1: -0.39962576 und neues bias1: -0.56140076\n",
      "neues weight2: -3.29796447 und neues bias2: -2.46230797\n",
      "Wir haben 0.6579332246575682 und -0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: -0.22820567\n",
      "\u001b[31mIch lag um -0.04638748 daneben\u001b[0m\n",
      "neues weight1: -0.40071504 und neues bias1: -0.56305637\n",
      "neues weight2: -3.29859294 und neues bias2: -2.46138022\n",
      "Wir haben 0.7054802310718645 und -0.15151515151515138 als Trainings Daten\n",
      "Ich sage y vorraus: -0.18916370\n",
      "\u001b[31mIch lag um -0.03764854 daneben\u001b[0m\n",
      "neues weight1: -0.40163582 und neues bias1: -0.56436156\n",
      "neues weight2: -3.29911162 und neues bias2: -2.46062725\n",
      "Wir haben 0.7564633275546291 und -0.12121212121212109 als Trainings Daten\n",
      "Ich sage y vorraus: -0.14976299\n",
      "\u001b[31mIch lag um -0.02855087 daneben\u001b[0m\n",
      "neues weight1: -0.40236171 und neues bias1: -0.56532113\n",
      "neues weight2: -3.29951159 und neues bias2: -2.46005623\n",
      "Wir haben 0.8111308307896873 und -0.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -0.11005493\n",
      "\u001b[31mIch lag um -0.01914584 daneben\u001b[0m\n",
      "neues weight1: -0.40286667 und neues bias1: -0.56594367\n",
      "neues weight2: -3.29978431 und neues bias2: -2.45967331\n",
      "Wir haben 0.8697490026177834 und -0.06060606060606055 als Trainings Daten\n",
      "Ich sage y vorraus: -0.07010241\n",
      "\u001b[31mIch lag um -0.00949635 daneben\u001b[0m\n",
      "neues weight1: -0.40312591 und neues bias1: -0.56624173\n",
      "neues weight2: -3.29992185 und neues bias2: -2.45948338\n",
      "Wir haben 0.9326033468832199 und -0.030303030303030297 als Trainings Daten\n",
      "Ich sage y vorraus: -0.02997985\n",
      "\u001b[31mIch lag um 0.00032318 daneben\u001b[0m\n",
      "neues weight1: -0.40311680 und neues bias1: -0.56623197\n",
      "neues weight2: -3.29991709 und neues bias2: -2.45948985\n",
      "Wir haben 1.0 und 0.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.01022739\n",
      "\u001b[31mIch lag um 0.01022739 daneben\u001b[0m\n",
      "neues weight1: -0.40281989 und neues bias1: -0.56593506\n",
      "neues weight2: -3.29976401 und neues bias2: -2.45969440\n",
      "Wir haben 1.072267222010323 und 0.030303030303030252 als Trainings Daten\n",
      "Ich sage y vorraus: 0.05042392\n",
      "\u001b[31mIch lag um 0.02012089 daneben\u001b[0m\n",
      "neues weight1: -0.40221996 und neues bias1: -0.56537557\n",
      "neues weight2: -3.29945789 und neues bias2: -2.46009681\n",
      "Wir haben 1.1497569953977356 und 0.060606060606060524 als Trainings Daten\n",
      "Ich sage y vorraus: 0.09050581\n",
      "\u001b[31mIch lag um 0.02989975 daneben\u001b[0m\n",
      "neues weight1: -0.40130707 und neues bias1: -0.56458158\n",
      "neues weight2: -3.29899562 und neues bias2: -2.46069481\n",
      "Wir haben 1.232846739442066 und 0.09090909090909083 als Trainings Daten\n",
      "Ich sage y vorraus: 0.13036255\n",
      "\u001b[31mIch lag um 0.03945346 daneben\u001b[0m\n",
      "neues weight1: -0.40007749 und neues bias1: -0.56358423\n",
      "neues weight2: -3.29837587 und neues bias2: -2.46148388\n",
      "Wir haben 1.3219411484660286 und 0.12121212121212108 als Trainings Daten\n",
      "Ich sage y vorraus: 0.16987921\n",
      "\u001b[31mIch lag um 0.04866709 daneben\u001b[0m\n",
      "neues weight1: -0.39853456 und neues bias1: -0.56241706\n",
      "neues weight2: -3.29759937 und neues bias2: -2.46245722\n",
      "Wir haben 1.4174741629268048 und 0.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: 0.20893893\n",
      "\u001b[31mIch lag um 0.05742378 daneben\u001b[0m\n",
      "neues weight1: -0.39668931 und neues bias1: -0.56111527\n",
      "neues weight2: -3.29666898 und neues bias2: -2.46360569\n",
      "Wir haben 1.5199110829529332 und 0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: 0.24742533\n",
      "\u001b[31mIch lag um 0.06560715 daneben\u001b[0m\n",
      "neues weight1: -0.39456087 und neues bias1: -0.55971490\n",
      "neues weight2: -3.29558994 und neues bias2: -2.46491784\n",
      "Wir haben 1.629750834620645 und 0.21212121212121235 als Trainings Daten\n",
      "Ich sage y vorraus: 0.28522472\n",
      "\u001b[31mIch lag um 0.07310351 daneben\u001b[0m\n",
      "neues weight1: -0.39217658 und neues bias1: -0.55825192\n",
      "neues weight2: -3.29436985 und neues bias2: -2.46637991\n",
      "Wir haben 1.7475284000076847 und 0.24242424242424265 als Trainings Daten\n",
      "Ich sage y vorraus: 0.32222788\n",
      "\u001b[31mIch lag um 0.07980364 daneben\u001b[0m\n",
      "neues weight1: -0.38957183 und neues bias1: -0.55676138\n",
      "neues weight2: -3.29301881 und neues bias2: -2.46797598\n",
      "Wir haben 1.873817422860385 und 0.27272727272727293 als Trainings Daten\n",
      "Ich sage y vorraus: 0.35833122\n",
      "\u001b[31mIch lag um 0.08560395 daneben\u001b[0m\n",
      "neues weight1: -0.38678949 und neues bias1: -0.55527654\n",
      "neues weight2: -3.29154938 und neues bias2: -2.46968806\n",
      "Wir haben 2.0092330025650478 und 0.30303030303030315 als Trainings Daten\n",
      "Ich sage y vorraus: 0.39343719\n",
      "\u001b[31mIch lag um 0.09040688 daneben\u001b[0m\n",
      "neues weight1: -0.38387917 und neues bias1: -0.55382806\n",
      "neues weight2: -3.28997659 und neues bias2: -2.47149620\n",
      "Wir haben 2.1544346900318843 und 0.3333333333333334 als Trainings Daten\n",
      "Ich sage y vorraus: 0.42745379\n",
      "\u001b[31mIch lag um 0.09412046 daneben\u001b[0m\n",
      "neues weight1: -0.38089602 und neues bias1: -0.55244341\n",
      "neues weight2: -3.28831791 und neues bias2: -2.47337861\n",
      "Wir haben 2.31012970008316 und 0.3636363636363637 als Trainings Daten\n",
      "Ich sage y vorraus: 0.46029340\n",
      "\u001b[31mIch lag um 0.09665704 daneben\u001b[0m\n",
      "neues weight1: -0.37789927 und neues bias1: -0.55114618\n",
      "neues weight2: -3.28659326 und neues bias2: -2.47531175\n",
      "Wir haben 2.4770763559917115 und 0.39393939393939403 als Trainings Daten\n",
      "Ich sage y vorraus: 0.49187084\n",
      "\u001b[31mIch lag um 0.09793145 daneben\u001b[0m\n",
      "neues weight1: -0.37495052 und neues bias1: -0.54995577\n",
      "neues weight2: -3.28482498 und neues bias2: -2.47727038\n",
      "Wir haben 2.656087782946687 und 0.42424242424242437 als Trainings Daten\n",
      "Ich sage y vorraus: 0.52210122\n",
      "\u001b[31mIch lag um 0.09785880 daneben\u001b[0m\n",
      "neues weight1: -0.37211166 und neues bias1: -0.54888696\n",
      "neues weight2: -3.28303789 und neues bias2: -2.47922755\n",
      "Wir haben 2.848035868435802 und 0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: 0.55089784\n",
      "\u001b[31mIch lag um 0.09635239 daneben\u001b[0m\n",
      "neues weight1: -0.36944247 und neues bias1: -0.54794975\n",
      "neues weight2: -3.28125929 und neues bias2: -2.48115460\n",
      "Wir haben 3.0538555088334154 und 0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.57817093\n",
      "\u001b[31mIch lag um 0.09332245 daneben\u001b[0m\n",
      "neues weight1: -0.36699804 und neues bias1: -0.54714931\n",
      "neues weight2: -3.27951908 und neues bias2: -2.48302105\n",
      "Wir haben 3.2745491628777286 und 0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: 0.60382791\n",
      "\u001b[31mIch lag um 0.08867639 daneben\u001b[0m\n",
      "neues weight1: -0.36482591 und neues bias1: -0.54648598\n",
      "neues weight2: -3.27784975 und neues bias2: -2.48479458\n",
      "Wir haben 3.511191734215131 und 0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: 0.62777601\n",
      "\u001b[31mIch lag um 0.08232147 daneben\u001b[0m\n",
      "neues weight1: -0.36296316 und neues bias1: -0.54595546\n",
      "neues weight2: -3.27628634 und neues bias2: -2.48644101\n",
      "Wir haben 3.7649358067924674 und 0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: 0.64992799\n",
      "\u001b[31mIch lag um 0.07417041 daneben\u001b[0m\n",
      "neues weight1: -0.36143367 und neues bias1: -0.54554921\n",
      "neues weight2: -3.27486628 und neues bias2: -2.48792441\n",
      "Wir haben 4.037017258596554 und 0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: 0.67021109\n",
      "\u001b[31mIch lag um 0.06415049 daneben\u001b[0m\n",
      "neues weight1: -0.36024600 und neues bias1: -0.54525502\n",
      "neues weight2: -3.27362900 und neues bias2: -2.48920742\n",
      "Wir haben 4.328761281083057 und 0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: 0.68857901\n",
      "\u001b[31mIch lag um 0.05221537 daneben\u001b[0m\n",
      "neues weight1: -0.35939216 und neues bias1: -0.54505777\n",
      "neues weight2: -3.27261527 und neues bias2: -2.49025173\n",
      "Wir haben 4.641588833612782 und 0.666666666666667 als Trainings Daten\n",
      "Ich sage y vorraus: 0.70502528\n",
      "\u001b[31mIch lag um 0.03835861 daneben\u001b[0m\n",
      "neues weight1: -0.35884788 und neues bias1: -0.54494051\n",
      "neues weight2: -3.27186623 und neues bias2: -2.49101890\n",
      "Wir haben 4.9770235643321135 und 0.6969696969696972 als Trainings Daten\n",
      "Ich sage y vorraus: 0.71959579\n",
      "\u001b[31mIch lag um 0.02262609 daneben\u001b[0m\n",
      "neues weight1: -0.35857456 und neues bias1: -0.54488559\n",
      "neues weight2: -3.27142218 und neues bias2: -2.49147143\n",
      "Wir haben 5.336699231206313 und 0.7272727272727275 als Trainings Daten\n",
      "Ich sage y vorraus: 0.73239739\n",
      "\u001b[31mIch lag um 0.00512467 daneben\u001b[0m\n",
      "neues weight1: -0.35852292 und neues bias1: -0.54487591\n",
      "neues weight2: -3.27132117 und neues bias2: -2.49157392\n",
      "Wir haben 5.72236765935022 und 0.7575757575757578 als Trainings Daten\n",
      "Ich sage y vorraus: 0.74359987\n",
      "\u001b[31mIch lag um -0.01397588 daneben\u001b[0m\n",
      "neues weight1: -0.35863791 und neues bias1: -0.54489601\n",
      "neues weight2: -3.27159760 und neues bias2: -2.49129440\n",
      "Wir haben 6.135907273413176 und 0.7878787878787881 als Trainings Daten\n",
      "Ich sage y vorraus: 0.75342984\n",
      "\u001b[31mIch lag um -0.03444895 daneben\u001b[0m\n",
      "neues weight1: -0.35886420 und neues bias1: -0.54493289\n",
      "neues weight2: -3.27228092 und neues bias2: -2.49060542\n",
      "Wir haben 6.5793322465756825 und 0.8181818181818183 als Trainings Daten\n",
      "Ich sage y vorraus: 0.76215698\n",
      "\u001b[31mIch lag um -0.05602484 daneben\u001b[0m\n",
      "neues weight1: -0.35915112 und neues bias1: -0.54497650\n",
      "neues weight2: -3.27339473 und neues bias2: -2.48948493\n",
      "Wir haben 7.054802310718645 und 0.8484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.77007509\n",
      "\u001b[31mIch lag um -0.07840976 daneben\u001b[0m\n",
      "neues weight1: -0.35945659 und neues bias1: -0.54501980\n",
      "neues weight2: -3.27495630 und neues bias2: -2.48791673\n",
      "Wir haben 7.56463327554629 und 0.8787878787878789 als Trainings Daten\n",
      "Ich sage y vorraus: 0.77748144\n",
      "\u001b[31mIch lag um -0.10130644 daneben\u001b[0m\n",
      "neues weight1: -0.35974916 und neues bias1: -0.54505847\n",
      "neues weight2: -3.27697652 und neues bias2: -2.48589060\n",
      "Wir haben 8.111308307896872 und 0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: 0.78465798\n",
      "\u001b[31mIch lag um -0.12443293 daneben\u001b[0m\n",
      "neues weight1: -0.36000842 und neues bias1: -0.54509044\n",
      "neues weight2: -3.27946029 und neues bias2: -2.48340194\n",
      "Wir haben 8.697490026177835 und 0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79185694\n",
      "\u001b[31mIch lag um -0.14753700 daneben\u001b[0m\n",
      "neues weight1: -0.36022393 und neues bias1: -0.54511521\n",
      "neues weight2: -3.28240725 und neues bias2: -2.48045120\n",
      "Wir haben 9.326033468832199 und 0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79929180\n",
      "\u001b[31mIch lag um -0.17040517 daneben\u001b[0m\n",
      "neues weight1: -0.36039322 und neues bias1: -0.54513337\n",
      "neues weight2: -3.28581259 und neues bias2: -2.47704310\n",
      "Wir haben 10.0 und 1.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.80713368\n",
      "\u001b[31mIch lag um -0.19286632 daneben\u001b[0m\n",
      "neues weight1: -0.36051939 und neues bias1: -0.54514598\n",
      "neues weight2: -3.28966800 und neues bias2: -2.47318577\n",
      "\n",
      "\n",
      "Ich trainiere in Epoche 6001\n",
      "Wir haben 0.01 und -2.0 als Trainings Daten\n",
      "Ich sage y vorraus: -0.82947098\n",
      "\u001b[31mIch lag um 1.17052902 daneben\u001b[0m\n",
      "neues weight1: -0.35960319 und neues bias1: -0.48884127\n",
      "neues weight2: -3.28525725 und neues bias2: -2.50376621\n",
      "Wir haben 0.010722672220103232 und -1.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.00452421\n",
      "\u001b[31mIch lag um 0.96517276 daneben\u001b[0m\n",
      "neues weight1: -0.35906481 und neues bias1: -0.43863160\n",
      "neues weight2: -3.27644803 und neues bias2: -2.52306967\n",
      "Wir haben 0.011497569953977356 und -1.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.16029806\n",
      "\u001b[31mIch lag um 0.77909588 daneben\u001b[0m\n",
      "neues weight1: -0.35857937 und neues bias1: -0.39641036\n",
      "neues weight2: -3.26996705 und neues bias2: -2.53865158\n",
      "Wir haben 0.012328467394420659 und -1.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.29390642\n",
      "\u001b[31mIch lag um 0.61518449 daneben\u001b[0m\n",
      "neues weight1: -0.35815523 und neues bias1: -0.36200749\n",
      "neues weight2: -3.26528353 und neues bias2: -2.55095527\n",
      "Wir haben 0.013219411484660288 und -1.878787878787879 als Trainings Daten\n",
      "Ich sage y vorraus: -1.40438764\n",
      "\u001b[31mIch lag um 0.47440024 daneben\u001b[0m\n",
      "neues weight1: -0.35779618 und neues bias1: -0.33484637\n",
      "neues weight2: -3.26195192 und neues bias2: -2.56044328\n",
      "Wir haben 0.014174741629268055 und -1.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -1.49246645\n",
      "\u001b[31mIch lag um 0.35601840 daneben\u001b[0m\n",
      "neues weight1: -0.35750224 und neues bias1: -0.31410978\n",
      "neues weight2: -3.25962068 und neues bias2: -2.56756365\n",
      "Wir haben 0.01519911082952934 und -1.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -1.56003415\n",
      "\u001b[31mIch lag um 0.25814766 daneben\u001b[0m\n",
      "neues weight1: -0.35727089 und neues bias1: -0.29888837\n",
      "neues weight2: -3.25802484 und neues bias2: -2.57272660\n",
      "Wir haben 0.016297508346206444 und -1.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -1.60959622\n",
      "\u001b[31mIch lag um 0.17828257 daneben\u001b[0m\n",
      "neues weight1: -0.35709811 und neues bias1: -0.28828659\n",
      "neues weight2: -3.25697077 und neues bias2: -2.57629225\n",
      "Wir haben 0.01747528400007684 und -1.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -1.64383420\n",
      "\u001b[31mIch lag um 0.11374155 daneben\u001b[0m\n",
      "neues weight1: -0.35697925 und neues bias1: -0.28148482\n",
      "neues weight2: -3.25631950 und neues bias2: -2.57856708\n",
      "Wir haben 0.01873817422860384 und -1.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -1.66532166\n",
      "\u001b[31mIch lag um 0.06195107 daneben\u001b[0m\n",
      "neues weight1: -0.35690959 und neues bias1: -0.27776751\n",
      "neues weight2: -3.25597201 und neues bias2: -2.57980610\n",
      "Wir haben 0.02009233002565047 und -1.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67637242\n",
      "\u001b[31mIch lag um 0.02059728 daneben\u001b[0m\n",
      "neues weight1: -0.35688471 und neues bias1: -0.27652950\n",
      "neues weight2: -3.25585771 und neues bias2: -2.58021805\n",
      "Wir haben 0.021544346900318846 und -1.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67898120\n",
      "\u001b[31mIch lag um -0.01231453 daneben\u001b[0m\n",
      "neues weight1: -0.35690067 und neues bias1: -0.27726994\n",
      "neues weight2: -3.25592588 und neues bias2: -2.57997176\n",
      "Wir haben 0.023101297000831605 und -1.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67481967\n",
      "\u001b[31mIch lag um -0.03845604 daneben\u001b[0m\n",
      "neues weight1: -0.35695405 und neues bias1: -0.27958060\n",
      "neues weight2: -3.25613970 und neues bias2: -2.57920264\n",
      "Wir haben 0.024770763559917114 und -1.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.66526168\n",
      "\u001b[31mIch lag um -0.05920107 daneben\u001b[0m\n",
      "neues weight1: -0.35704202 und neues bias1: -0.28313221\n",
      "neues weight2: -3.25647203 und neues bias2: -2.57801862\n",
      "Wir haben 0.026560877829466867 und -1.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -1.65142078\n",
      "\u001b[31mIch lag um -0.07566321 daneben\u001b[0m\n",
      "neues weight1: -0.35716231 und neues bias1: -0.28766113\n",
      "neues weight2: -3.25690262 und neues bias2: -2.57650535\n",
      "Wir haben 0.02848035868435802 und -1.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -1.63419065\n",
      "\u001b[31mIch lag um -0.08873611 daneben\u001b[0m\n",
      "neues weight1: -0.35731315 und neues bias1: -0.29295737\n",
      "neues weight2: -3.25741610 und neues bias2: -2.57473063\n",
      "Wir haben 0.030538555088334154 und -1.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -1.61428326\n",
      "\u001b[31mIch lag um -0.09913175 daneben\u001b[0m\n",
      "neues weight1: -0.35749323 und neues bias1: -0.29885418\n",
      "neues weight2: -3.25800068 und neues bias2: -2.57274800\n",
      "Wir haben 0.03274549162877728 und -1.4848484848484849 als Trainings Daten\n",
      "Ich sage y vorraus: -1.59226280\n",
      "\u001b[31mIch lag um -0.10741432 daneben\u001b[0m\n",
      "neues weight1: -0.35770167 und neues bias1: -0.30521940\n",
      "neues weight2: -3.25864720 und neues bias2: -2.57059971\n",
      "Wir haben 0.03511191734215131 und -1.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -1.56857452\n",
      "\u001b[31mIch lag um -0.11402906 daneben\u001b[0m\n",
      "neues weight1: -0.35793793 und neues bias1: -0.31194832\n",
      "neues weight2: -3.25934847 und neues bias2: -2.56831913\n",
      "Wir haben 0.037649358067924674 und -1.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -1.54356867\n",
      "\u001b[31mIch lag um -0.11932624 daneben\u001b[0m\n",
      "neues weight1: -0.35820184 und neues bias1: -0.31895793\n",
      "neues weight2: -3.26009880 und neues bias2: -2.56593260\n",
      "Wir haben 0.040370172585965536 und -1.393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.51752008\n",
      "\u001b[31mIch lag um -0.12358069 daneben\u001b[0m\n",
      "neues weight1: -0.35849349 und neues bias1: -0.32618231\n",
      "neues weight2: -3.26089364 und neues bias2: -2.56346099\n",
      "Wir haben 0.04328761281083057 und -1.3636363636363638 als Trainings Daten\n",
      "Ich sage y vorraus: -1.49064392\n",
      "\u001b[31mIch lag um -0.12700755 daneben\u001b[0m\n",
      "neues weight1: -0.35881324 und neues bias1: -0.33356892\n",
      "neues weight2: -3.26172934 und neues bias2: -2.56092084\n",
      "Wir haben 0.046415888336127795 und -1.3333333333333333 als Trainings Daten\n",
      "Ich sage y vorraus: -1.46310823\n",
      "\u001b[31mIch lag um -0.12977489 daneben\u001b[0m\n",
      "neues weight1: -0.35916167 und neues bias1: -0.34107571\n",
      "neues weight2: -3.26260292 und neues bias2: -2.55832534\n",
      "Wir haben 0.049770235643321115 und -1.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.43504394\n",
      "\u001b[31mIch lag um -0.13201364 daneben\u001b[0m\n",
      "neues weight1: -0.35953958 und neues bias1: -0.34866879\n",
      "neues weight2: -3.26351194 und neues bias2: -2.55568507\n",
      "Wir haben 0.0533669923120631 und -1.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -1.40655275\n",
      "\u001b[31mIch lag um -0.13382548 daneben\u001b[0m\n",
      "neues weight1: -0.35994794 und neues bias1: -0.35632062\n",
      "neues weight2: -3.26445438 und neues bias2: -2.55300856\n",
      "Wir haben 0.05722367659350217 und -1.2424242424242424 als Trainings Daten\n",
      "Ich sage y vorraus: -1.37771331\n",
      "\u001b[31mIch lag um -0.13528907 daneben\u001b[0m\n",
      "neues weight1: -0.36038787 und neues bias1: -0.36400860\n",
      "neues weight2: -3.26542853 und neues bias2: -2.55030278\n",
      "Wir haben 0.06135907273413173 und -1.2121212121212122 als Trainings Daten\n",
      "Ich sage y vorraus: -1.34858608\n",
      "\u001b[31mIch lag um -0.13646487 daneben\u001b[0m\n",
      "neues weight1: -0.36086066 und neues bias1: -0.37171390\n",
      "neues weight2: -3.26643295 und neues bias2: -2.54757348\n",
      "Wir haben 0.06579332246575682 und -1.1818181818181817 als Trainings Daten\n",
      "Ich sage y vorraus: -1.31921716\n",
      "\u001b[31mIch lag um -0.13739898 daneben\u001b[0m\n",
      "neues weight1: -0.36136771 und neues bias1: -0.37942062\n",
      "neues weight2: -3.26746634 und neues bias2: -2.54482550\n",
      "Wir haben 0.07054802310718646 und -1.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: -1.28964123\n",
      "\u001b[31mIch lag um -0.13812608 daneben\u001b[0m\n",
      "neues weight1: -0.36191054 und neues bias1: -0.38711505\n",
      "neues weight2: -3.26852755 und neues bias2: -2.54206298\n",
      "Wir haben 0.07564633275546291 und -1.121212121212121 als Trainings Daten\n",
      "Ich sage y vorraus: -1.25988389\n",
      "\u001b[31mIch lag um -0.13867177 daneben\u001b[0m\n",
      "neues weight1: -0.36249075 und neues bias1: -0.39478514\n",
      "neues weight2: -3.26961551 und neues bias2: -2.53928954\n",
      "Wir haben 0.08111308307896872 und -1.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -1.22996344\n",
      "\u001b[31mIch lag um -0.13905435 daneben\u001b[0m\n",
      "neues weight1: -0.36311004 und neues bias1: -0.40242004\n",
      "neues weight2: -3.27072921 und neues bias2: -2.53650846\n",
      "Wir haben 0.08697490026177834 und -1.0606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.19989231\n",
      "\u001b[31mIch lag um -0.13928625 daneben\u001b[0m\n",
      "neues weight1: -0.36377016 und neues bias1: -0.41000976\n",
      "neues weight2: -3.27186762 und neues bias2: -2.53372273\n",
      "Wir haben 0.093260334688322 und -1.0303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.16967813\n",
      "\u001b[31mIch lag um -0.13937510 daneben\u001b[0m\n",
      "neues weight1: -0.36447289 und neues bias1: -0.41754493\n",
      "neues weight2: -3.27302973 und neues bias2: -2.53093523\n",
      "Wir haben 0.1 und -1.0 als Trainings Daten\n",
      "Ich sage y vorraus: -1.13932456\n",
      "\u001b[31mIch lag um -0.13932456 daneben\u001b[0m\n",
      "neues weight1: -0.36522005 und neues bias1: -0.42501649\n",
      "neues weight2: -3.27421448 und neues bias2: -2.52814874\n",
      "Wir haben 0.10722672220103231 und -0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.10883197\n",
      "\u001b[31mIch lag um -0.13913500 daneben\u001b[0m\n",
      "neues weight1: -0.36601343 und neues bias1: -0.43241559\n",
      "neues weight2: -3.27542073 und neues bias2: -2.52536604\n",
      "Wir haben 0.11497569953977356 und -0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.07819791\n",
      "\u001b[31mIch lag um -0.13880397 daneben\u001b[0m\n",
      "neues weight1: -0.36685480 und neues bias1: -0.43973340\n",
      "neues weight2: -3.27664728 und neues bias2: -2.52258996\n",
      "Wir haben 0.12328467394420659 und -0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.04741753\n",
      "\u001b[31mIch lag um -0.13832662 daneben\u001b[0m\n",
      "neues weight1: -0.36774585 und neues bias1: -0.44696100\n",
      "neues weight2: -3.27789279 und neues bias2: -2.51982343\n",
      "Wir haben 0.13219411484660293 und -0.8787878787878787 als Trainings Daten\n",
      "Ich sage y vorraus: -1.01648381\n",
      "\u001b[31mIch lag um -0.13769593 daneben\u001b[0m\n",
      "neues weight1: -0.36868817 und neues bias1: -0.45408929\n",
      "neues weight2: -3.27915582 und neues bias2: -2.51706951\n",
      "Wir haben 0.14174741629268056 und -0.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -0.98538785\n",
      "\u001b[31mIch lag um -0.13690300 daneben\u001b[0m\n",
      "neues weight1: -0.36968318 und neues bias1: -0.46110889\n",
      "neues weight2: -3.28043476 und neues bias2: -2.51433145\n",
      "Wir haben 0.1519911082952934 und -0.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -0.95411900\n",
      "\u001b[31mIch lag um -0.13593719 daneben\u001b[0m\n",
      "neues weight1: -0.37073210 und neues bias1: -0.46801010\n",
      "neues weight2: -3.28172783 und neues bias2: -2.51161270\n",
      "Wir haben 0.16297508346206444 und -0.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -0.92266505\n",
      "\u001b[31mIch lag um -0.13478626 daneben\u001b[0m\n",
      "neues weight1: -0.37183588 und neues bias1: -0.47478281\n",
      "neues weight2: -3.28303304 und neues bias2: -2.50891698\n",
      "Wir haben 0.17475284000076838 und -0.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -0.89101226\n",
      "\u001b[31mIch lag um -0.13343650 daneben\u001b[0m\n",
      "neues weight1: -0.37299514 und neues bias1: -0.48141652\n",
      "neues weight2: -3.28434821 und neues bias2: -2.50624825\n",
      "Wir haben 0.1873817422860384 und -0.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -0.85914553\n",
      "\u001b[31mIch lag um -0.13187281 daneben\u001b[0m\n",
      "neues weight1: -0.37421007 und neues bias1: -0.48790024\n",
      "neues weight2: -3.28567090 und neues bias2: -2.50361079\n",
      "Wir haben 0.20092330025650468 und -0.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -0.82704848\n",
      "\u001b[31mIch lag um -0.13007878 daneben\u001b[0m\n",
      "neues weight1: -0.37548037 und neues bias1: -0.49422254\n",
      "neues weight2: -3.28699839 und neues bias2: -2.50100922\n",
      "Wir haben 0.21544346900318845 und -0.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -0.79470352\n",
      "\u001b[31mIch lag um -0.12803685 daneben\u001b[0m\n",
      "neues weight1: -0.37680512 und neues bias1: -0.50037149\n",
      "neues weight2: -3.28832769 und neues bias2: -2.49844848\n",
      "Wir haben 0.23101297000831605 und -0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -0.76209201\n",
      "\u001b[31mIch lag um -0.12572837 daneben\u001b[0m\n",
      "neues weight1: -0.37818270 und neues bias1: -0.50633471\n",
      "neues weight2: -3.28965547 und neues bias2: -2.49593391\n",
      "Wir haben 0.24770763559917114 und -0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -0.72919438\n",
      "\u001b[31mIch lag um -0.12313377 daneben\u001b[0m\n",
      "neues weight1: -0.37961065 und neues bias1: -0.51209937\n",
      "neues weight2: -3.29097808 und neues bias2: -2.49347124\n",
      "Wir haben 0.26560877829466867 und -0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -0.69599032\n",
      "\u001b[31mIch lag um -0.12023274 daneben\u001b[0m\n",
      "neues weight1: -0.38108555 und neues bias1: -0.51765225\n",
      "neues weight2: -3.29229146 und neues bias2: -2.49106658\n",
      "Wir haben 0.2848035868435802 und -0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -0.66245900\n",
      "\u001b[31mIch lag um -0.11700445 daneben\u001b[0m\n",
      "neues weight1: -0.38260285 und neues bias1: -0.52297980\n",
      "neues weight2: -3.29359120 und neues bias2: -2.48872649\n",
      "Wir haben 0.30538555088334157 und -0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -0.62857935\n",
      "\u001b[31mIch lag um -0.11342783 daneben\u001b[0m\n",
      "neues weight1: -0.38415678 und neues bias1: -0.52806822\n",
      "neues weight2: -3.29487243 und neues bias2: -2.48645794\n",
      "Wir haben 0.32745491628777285 und -0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: -0.59433044\n",
      "\u001b[31mIch lag um -0.10948196 daneben\u001b[0m\n",
      "neues weight1: -0.38574015 und neues bias1: -0.53290358\n",
      "neues weight2: -3.29612986 und neues bias2: -2.48426830\n",
      "Wir haben 0.3511191734215131 und -0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -0.55969189\n",
      "\u001b[31mIch lag um -0.10514644 daneben\u001b[0m\n",
      "neues weight1: -0.38734419 und neues bias1: -0.53747196\n",
      "neues weight2: -3.29735774 und neues bias2: -2.48216537\n",
      "Wir haben 0.37649358067924676 und -0.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.52464440\n",
      "\u001b[31mIch lag um -0.10040198 daneben\u001b[0m\n",
      "neues weight1: -0.38895847 und neues bias1: -0.54175963\n",
      "neues weight2: -3.29854984 und neues bias2: -2.48015733\n",
      "Wir haben 0.4037017258596556 und -0.3939393939393938 als Trainings Daten\n",
      "Ich sage y vorraus: -0.48917038\n",
      "\u001b[31mIch lag um -0.09523099 daneben\u001b[0m\n",
      "neues weight1: -0.39057070 und neues bias1: -0.54575324\n",
      "neues weight2: -3.29969945 und neues bias2: -2.47825271\n",
      "Wir haben 0.43287612810830595 und -0.36363636363636354 als Trainings Daten\n",
      "Ich sage y vorraus: -0.45325467\n",
      "\u001b[31mIch lag um -0.08961831 daneben\u001b[0m\n",
      "neues weight1: -0.39216665 und neues bias1: -0.54944009\n",
      "neues weight2: -3.30079941 und neues bias2: -2.47646034\n",
      "Wir haben 0.464158883361278 und -0.33333333333333326 als Trainings Daten\n",
      "Ich sage y vorraus: -0.41688541\n",
      "\u001b[31mIch lag um -0.08355207 daneben\u001b[0m\n",
      "neues weight1: -0.39373008 und neues bias1: -0.55280841\n",
      "neues weight2: -3.30184208 und neues bias2: -2.47478930\n",
      "Wir haben 0.49770235643321115 und -0.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -0.38005492\n",
      "\u001b[31mIch lag um -0.07702462 daneben\u001b[0m\n",
      "neues weight1: -0.39524273 und neues bias1: -0.55584767\n",
      "neues weight2: -3.30281939 und neues bias2: -2.47324881\n",
      "Wir haben 0.533669923120631 und -0.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -0.34276080\n",
      "\u001b[31mIch lag um -0.07003353 daneben\u001b[0m\n",
      "neues weight1: -0.39668431 und neues bias1: -0.55854892\n",
      "neues weight2: -3.30372290 und neues bias2: -2.47184814\n",
      "Wir haben 0.5722367659350217 und -0.24242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.30500697\n",
      "\u001b[31mIch lag um -0.06258273 daneben\u001b[0m\n",
      "neues weight1: -0.39803266 und neues bias1: -0.56090521\n",
      "neues weight2: -3.30454383 und neues bias2: -2.47059648\n",
      "Wir haben 0.6135907273413173 und -0.21212121212121215 als Trainings Daten\n",
      "Ich sage y vorraus: -0.26680485\n",
      "\u001b[31mIch lag um -0.05468364 daneben\u001b[0m\n",
      "neues weight1: -0.39926396 und neues bias1: -0.56291193\n",
      "neues weight2: -3.30527320 und neues bias2: -2.46950281\n",
      "Wir haben 0.6579332246575682 und -0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: -0.22817441\n",
      "\u001b[31mIch lag um -0.04635623 daneben\u001b[0m\n",
      "neues weight1: -0.40035304 und neues bias1: -0.56456723\n",
      "neues weight2: -3.30590189 und neues bias2: -2.46857569\n",
      "Wir haben 0.7054802310718645 und -0.15151515151515138 als Trainings Daten\n",
      "Ich sage y vorraus: -0.18914527\n",
      "\u001b[31mIch lag um -0.03763012 daneben\u001b[0m\n",
      "neues weight1: -0.40127382 und neues bias1: -0.56587241\n",
      "neues weight2: -3.30642081 und neues bias2: -2.46782308\n",
      "Wir haben 0.7564633275546291 und -0.12121212121212109 als Trainings Daten\n",
      "Ich sage y vorraus: -0.14975755\n",
      "\u001b[31mIch lag um -0.02854543 daneben\u001b[0m\n",
      "neues weight1: -0.40199992 und neues bias1: -0.56683226\n",
      "neues weight2: -3.30682106 und neues bias2: -2.46725218\n",
      "Wir haben 0.8111308307896873 und -0.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -0.11006255\n",
      "\u001b[31mIch lag um -0.01915346 daneben\u001b[0m\n",
      "neues weight1: -0.40250532 und neues bias1: -0.56745535\n",
      "neues weight2: -3.30709412 und neues bias2: -2.46686911\n",
      "Wir haben 0.8697490026177834 und -0.06060606060606055 als Trainings Daten\n",
      "Ich sage y vorraus: -0.07012308\n",
      "\u001b[31mIch lag um -0.00951702 daneben\u001b[0m\n",
      "neues weight1: -0.40276525 und neues bias1: -0.56775420\n",
      "neues weight2: -3.30723207 und neues bias2: -2.46667877\n",
      "Wir haben 0.9326033468832199 und -0.030303030303030297 als Trainings Daten\n",
      "Ich sage y vorraus: -0.03001343\n",
      "\u001b[31mIch lag um 0.00028960 daneben\u001b[0m\n",
      "neues weight1: -0.40275708 und neues bias1: -0.56774544\n",
      "neues weight2: -3.30722780 und neues bias2: -2.46668456\n",
      "Wir haben 1.0 und 0.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.01018113\n",
      "\u001b[31mIch lag um 0.01018113 daneben\u001b[0m\n",
      "neues weight1: -0.40246137 und neues bias1: -0.56744973\n",
      "neues weight2: -3.30707530 und neues bias2: -2.46688818\n",
      "Wir haben 1.072267222010323 und 0.030303030303030252 als Trainings Daten\n",
      "Ich sage y vorraus: 0.05036535\n",
      "\u001b[31mIch lag um 0.02006232 daneben\u001b[0m\n",
      "neues weight1: -0.40186290 und neues bias1: -0.56689159\n",
      "neues weight2: -3.30676988 und neues bias2: -2.46728943\n",
      "Wir haben 1.1497569953977356 und 0.060606060606060524 als Trainings Daten\n",
      "Ich sage y vorraus: 0.09043545\n",
      "\u001b[31mIch lag um 0.02982939 daneben\u001b[0m\n",
      "neues weight1: -0.40095169 und neues bias1: -0.56609908\n",
      "neues weight2: -3.30630844 und neues bias2: -2.46788601\n",
      "Wir haben 1.232846739442066 und 0.09090909090909083 als Trainings Daten\n",
      "Ich sage y vorraus: 0.13028103\n",
      "\u001b[31mIch lag um 0.03937194 daneben\u001b[0m\n",
      "neues weight1: -0.39972402 und neues bias1: -0.56510327\n",
      "neues weight2: -3.30568965 und neues bias2: -2.46867345\n",
      "Wir haben 1.3219411484660286 und 0.12121212121212108 als Trainings Daten\n",
      "Ich sage y vorraus: 0.16978734\n",
      "\u001b[31mIch lag um 0.04857522 daneben\u001b[0m\n",
      "neues weight1: -0.39818317 und neues bias1: -0.56393768\n",
      "neues weight2: -3.30491424 und neues bias2: -2.46964496\n",
      "Wir haben 1.4174741629268048 und 0.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: 0.20883764\n",
      "\u001b[31mIch lag um 0.05732249 daneben\u001b[0m\n",
      "neues weight1: -0.39634014 und neues bias1: -0.56263746\n",
      "neues weight2: -3.30398509 und neues bias2: -2.47079141\n",
      "Wir haben 1.5199110829529332 und 0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: 0.24731570\n",
      "\u001b[31mIch lag um 0.06549752 daneben\u001b[0m\n",
      "neues weight1: -0.39421402 und neues bias1: -0.56123861\n",
      "neues weight2: -3.30290743 und neues bias2: -2.47210136\n",
      "Wir haben 1.629750834620645 und 0.21212121212121235 als Trainings Daten\n",
      "Ich sage y vorraus: 0.28510797\n",
      "\u001b[31mIch lag um 0.07298676 daneben\u001b[0m\n",
      "neues weight1: -0.39183207 und neues bias1: -0.55977707\n",
      "neues weight2: -3.30168887 und neues bias2: -2.47356109\n",
      "Wir haben 1.7475284000076847 und 0.24242424242424265 als Trainings Daten\n",
      "Ich sage y vorraus: 0.32210537\n",
      "\u001b[31mIch lag um 0.07968113 daneben\u001b[0m\n",
      "neues weight1: -0.38922960 und neues bias1: -0.55828784\n",
      "neues weight2: -3.30033948 und neues bias2: -2.47515472\n",
      "Wir haben 1.873817422860385 und 0.27272727272727293 als Trainings Daten\n",
      "Ich sage y vorraus: 0.35820445\n",
      "\u001b[31mIch lag um 0.08547718 daneben\u001b[0m\n",
      "neues weight1: -0.38644944 und neues bias1: -0.55680415\n",
      "neues weight2: -3.29887183 und neues bias2: -2.47686426\n",
      "Wir haben 2.0092330025650478 und 0.30303030303030315 als Trainings Daten\n",
      "Ich sage y vorraus: 0.39330778\n",
      "\u001b[31mIch lag um 0.09027748 daneben\u001b[0m\n",
      "neues weight1: -0.38354110 und neues bias1: -0.55535666\n",
      "neues weight2: -3.29730092 und neues bias2: -2.47866981\n",
      "Wir haben 2.1544346900318843 und 0.3333333333333334 als Trainings Daten\n",
      "Ich sage y vorraus: 0.42732352\n",
      "\u001b[31mIch lag um 0.09399019 daneben\u001b[0m\n",
      "neues weight1: -0.38055965 und neues bias1: -0.55397280\n",
      "neues weight2: -3.29564420 und neues bias2: -2.48054961\n",
      "Wir haben 2.31012970008316 und 0.3636363636363637 als Trainings Daten\n",
      "Ich sage y vorraus: 0.46016418\n",
      "\u001b[31mIch lag um 0.09652781 daneben\u001b[0m\n",
      "neues weight1: -0.37756426 und neues bias1: -0.55267617\n",
      "neues weight2: -3.29392156 und neues bias2: -2.48248017\n",
      "Wir haben 2.4770763559917115 und 0.39393939393939403 als Trainings Daten\n",
      "Ich sage y vorraus: 0.49174473\n",
      "\u001b[31mIch lag um 0.09780533 daneben\u001b[0m\n",
      "neues weight1: -0.37461648 und neues bias1: -0.55148614\n",
      "neues weight2: -3.29215531 und neues bias2: -2.48443628\n",
      "Wir haben 2.656087782946687 und 0.42424242424242437 als Trainings Daten\n",
      "Ich sage y vorraus: 0.52198042\n",
      "\u001b[31mIch lag um 0.09773800 daneben\u001b[0m\n",
      "neues weight1: -0.37177812 und neues bias1: -0.55041752\n",
      "neues weight2: -3.29037021 und neues bias2: -2.48639104\n",
      "Wir haben 2.848035868435802 und 0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: 0.55078470\n",
      "\u001b[31mIch lag um 0.09623924 daneben\u001b[0m\n",
      "neues weight1: -0.36910898 und neues bias1: -0.54948033\n",
      "neues weight2: -3.28859354 und neues bias2: -2.48831582\n",
      "Wir haben 3.0538555088334154 und 0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.57806791\n",
      "\u001b[31mIch lag um 0.09321943 daneben\u001b[0m\n",
      "neues weight1: -0.36666413 und neues bias1: -0.54867975\n",
      "neues weight2: -3.28685512 und neues bias2: -2.49018021\n",
      "Wir haben 3.2745491628777286 und 0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: 0.60373755\n",
      "\u001b[31mIch lag um 0.08858604 daneben\u001b[0m\n",
      "neues weight1: -0.36449115 und neues bias1: -0.54801615\n",
      "neues weight2: -3.28518740 und neues bias2: -2.49195193\n",
      "Wir haben 3.511191734215131 und 0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: 0.62770085\n",
      "\u001b[31mIch lag um 0.08224631 daneben\u001b[0m\n",
      "neues weight1: -0.36262718 und neues bias1: -0.54748529\n",
      "neues weight2: -3.28362536 und neues bias2: -2.49359686\n",
      "Wir haben 3.7649358067924674 und 0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: 0.64987043\n",
      "\u001b[31mIch lag um 0.07411286 daneben\u001b[0m\n",
      "neues weight1: -0.36109623 und neues bias1: -0.54707866\n",
      "neues weight2: -3.28220637 und neues bias2: -2.49507911\n",
      "Wir haben 4.037017258596554 und 0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: 0.67017327\n",
      "\u001b[31mIch lag um 0.06411267 daneben\u001b[0m\n",
      "neues weight1: -0.35990699 und neues bias1: -0.54678407\n",
      "neues weight2: -3.28096981 und neues bias2: -2.49636137\n",
      "Wir haben 4.328761281083057 und 0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: 0.68856261\n",
      "\u001b[31mIch lag um 0.05219897 daneben\u001b[0m\n",
      "neues weight1: -0.35905160 und neues bias1: -0.54658647\n",
      "neues weight2: -3.27995639 und neues bias2: -2.49740535\n",
      "Wir haben 4.641588833612782 und 0.666666666666667 als Trainings Daten\n",
      "Ich sage y vorraus: 0.70503133\n",
      "\u001b[31mIch lag um 0.03836467 daneben\u001b[0m\n",
      "neues weight1: -0.35850596 und neues bias1: -0.54646891\n",
      "neues weight2: -3.27920723 und neues bias2: -2.49817264\n",
      "Wir haben 4.9770235643321135 und 0.6969696969696972 als Trainings Daten\n",
      "Ich sage y vorraus: 0.71962455\n",
      "\u001b[31mIch lag um 0.02265485 daneben\u001b[0m\n",
      "neues weight1: -0.35823159 und neues bias1: -0.54641378\n",
      "neues weight2: -3.27876262 und neues bias2: -2.49862574\n",
      "Wir haben 5.336699231206313 und 0.7272727272727275 als Trainings Daten\n",
      "Ich sage y vorraus: 0.73244823\n",
      "\u001b[31mIch lag um 0.00517550 daneben\u001b[0m\n",
      "neues weight1: -0.35817928 und neues bias1: -0.54640398\n",
      "neues weight2: -3.27866062 und neues bias2: -2.49872925\n",
      "Wir haben 5.72236765935022 und 0.7575757575757578 als Trainings Daten\n",
      "Ich sage y vorraus: 0.74367129\n",
      "\u001b[31mIch lag um -0.01390447 daneben\u001b[0m\n",
      "neues weight1: -0.35829405 und neues bias1: -0.54642404\n",
      "neues weight2: -3.27893563 und neues bias2: -2.49845116\n",
      "Wir haben 6.135907273413176 und 0.7878787878787881 als Trainings Daten\n",
      "Ich sage y vorraus: 0.75351960\n",
      "\u001b[31mIch lag um -0.03435918 daneben\u001b[0m\n",
      "neues weight1: -0.35852051 und neues bias1: -0.54646095\n",
      "neues weight2: -3.27961716 und neues bias2: -2.49776397\n",
      "Wir haben 6.5793322465756825 und 0.8181818181818183 als Trainings Daten\n",
      "Ich sage y vorraus: 0.76226229\n",
      "\u001b[31mIch lag um -0.05591953 daneben\u001b[0m\n",
      "neues weight1: -0.35880796 und neues bias1: -0.54650464\n",
      "neues weight2: -3.28072887 und neues bias2: -2.49664558\n",
      "Wir haben 7.054802310718645 und 0.8484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.77019284\n",
      "\u001b[31mIch lag um -0.07829200 daneben\u001b[0m\n",
      "neues weight1: -0.35911420 und neues bias1: -0.54654804\n",
      "neues weight2: -3.28228808 und neues bias2: -2.49507974\n",
      "Wir haben 7.56463327554629 und 0.8787878787878789 als Trainings Daten\n",
      "Ich sage y vorraus: 0.77760847\n",
      "\u001b[31mIch lag um -0.10117941 daneben\u001b[0m\n",
      "neues weight1: -0.35940767 und neues bias1: -0.54658684\n",
      "neues weight2: -3.28430575 und neues bias2: -2.49305615\n",
      "Wir haben 8.111308307896872 und 0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: 0.78479129\n",
      "\u001b[31mIch lag um -0.12429962 daneben\u001b[0m\n",
      "neues weight1: -0.35966787 und neues bias1: -0.54661892\n",
      "neues weight2: -3.28678686 und neues bias2: -2.49057016\n",
      "Wir haben 8.697490026177835 und 0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79199381\n",
      "\u001b[31mIch lag um -0.14740012 daneben\u001b[0m\n",
      "neues weight1: -0.35988428 und neues bias1: -0.54664380\n",
      "neues weight2: -3.28973107 und neues bias2: -2.48762216\n",
      "Wir haben 9.326033468832199 und 0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79942995\n",
      "\u001b[31mIch lag um -0.17026702 daneben\u001b[0m\n",
      "neues weight1: -0.36005437 und neues bias1: -0.54666204\n",
      "neues weight2: -3.29313364 und neues bias2: -2.48421682\n",
      "Wir haben 10.0 und 1.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.80727126\n",
      "\u001b[31mIch lag um -0.19272874 daneben\u001b[0m\n",
      "neues weight1: -0.36018120 und neues bias1: -0.54667472\n",
      "neues weight2: -3.29698629 und neues bias2: -2.48036224\n",
      "\n",
      "\n",
      "Ich trainiere in Epoche 7001\n",
      "Wir haben 0.01 und -2.0 als Trainings Daten\n",
      "Ich sage y vorraus: -0.82925891\n",
      "\u001b[31mIch lag um 1.17074109 daneben\u001b[0m\n",
      "neues weight1: -0.35931663 und neues bias1: -0.49009456\n",
      "neues weight2: -3.29146390 und neues bias2: -2.50988183\n",
      "Wir haben 0.010722672220103232 und -1.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.00455121\n",
      "\u001b[31mIch lag um 0.96514576 daneben\u001b[0m\n",
      "neues weight1: -0.35877786 und neues bias1: -0.43984887\n",
      "neues weight2: -3.28263584 und neues bias2: -2.52918474\n",
      "Wir haben 0.011497569953977356 und -1.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.16054549\n",
      "\u001b[31mIch lag um 0.77884845 daneben\u001b[0m\n",
      "neues weight1: -0.35829215 und neues bias1: -0.39760405\n",
      "neues weight2: -3.27614128 und neues bias2: -2.54476171\n",
      "Wir haben 0.012328467394420659 und -1.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.29433365\n",
      "\u001b[31mIch lag um 0.61475726 daneben\u001b[0m\n",
      "neues weight1: -0.35786789 und neues bias1: -0.36319140\n",
      "neues weight2: -3.27144851 und neues bias2: -2.55705686\n",
      "Wir haben 0.013219411484660288 und -1.878787878787879 als Trainings Daten\n",
      "Ich sage y vorraus: -1.40494120\n",
      "\u001b[31mIch lag um 0.47384668 daneben\u001b[0m\n",
      "neues weight1: -0.35750888 und neues bias1: -0.33603331\n",
      "neues weight2: -3.26811099 und neues bias2: -2.56653379\n",
      "Wir haben 0.014174741629268055 und -1.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -1.49309045\n",
      "\u001b[31mIch lag um 0.35539440 daneben\u001b[0m\n",
      "neues weight1: -0.35721513 und neues bias1: -0.31531006\n",
      "neues weight2: -3.26577633 und neues bias2: -2.57364168\n",
      "Wir haben 0.01519911082952934 und -1.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -1.56067837\n",
      "\u001b[31mIch lag um 0.25750345 daneben\u001b[0m\n",
      "neues weight1: -0.35698409 und neues bias1: -0.30010922\n",
      "neues weight2: -3.26417891 und neues bias2: -2.57879175\n",
      "Wir haben 0.016297508346206444 und -1.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -1.61022052\n",
      "\u001b[31mIch lag um 0.17765827 daneben\u001b[0m\n",
      "neues weight1: -0.35681171 und neues bias1: -0.28953224\n",
      "neues weight2: -3.26312459 und neues bias2: -2.58234491\n",
      "Wir haben 0.01747528400007684 und -1.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -1.64440984\n",
      "\u001b[31mIch lag um 0.11316591 daneben\u001b[0m\n",
      "neues weight1: -0.35669331 und neues bias1: -0.28275693\n",
      "neues weight2: -3.26247403 und neues bias2: -2.58460823\n",
      "Wir haben 0.01873817422860384 und -1.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -1.66583043\n",
      "\u001b[31mIch lag um 0.06144230 daneben\u001b[0m\n",
      "neues weight1: -0.35662415 und neues bias1: -0.27906581\n",
      "neues weight2: -3.26212796 und neues bias2: -2.58583708\n",
      "Wir haben 0.02009233002565047 und -1.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67680484\n",
      "\u001b[31mIch lag um 0.02016486 daneben\u001b[0m\n",
      "neues weight1: -0.35659977 und neues bias1: -0.27785236\n",
      "neues weight2: -3.26201558 und neues bias2: -2.58624038\n",
      "Wir haben 0.021544346900318846 und -1.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67933436\n",
      "\u001b[31mIch lag um -0.01266770 daneben\u001b[0m\n",
      "neues weight1: -0.35661620 und neues bias1: -0.27861493\n",
      "neues weight2: -3.26208602 und neues bias2: -2.58598702\n",
      "Wir haben 0.023101297000831605 und -1.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67509532\n",
      "\u001b[31mIch lag um -0.03873168 daneben\u001b[0m\n",
      "neues weight1: -0.35667002 und neues bias1: -0.28094482\n",
      "neues weight2: -3.26230232 und neues bias2: -2.58521239\n",
      "Wir haben 0.024770763559917114 und -1.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.66546454\n",
      "\u001b[31mIch lag um -0.05940393 daneben\u001b[0m\n",
      "neues weight1: -0.35675840 und neues bias1: -0.28451261\n",
      "neues weight2: -3.26263728 und neues bias2: -2.58402431\n",
      "Wir haben 0.026560877829466867 und -1.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -1.65155730\n",
      "\u001b[31mIch lag um -0.07579973 daneben\u001b[0m\n",
      "neues weight1: -0.35687904 und neues bias1: -0.28905474\n",
      "neues weight2: -3.26307055 und neues bias2: -2.58250831\n",
      "Wir haben 0.02848035868435802 und -1.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -1.63426808\n",
      "\u001b[31mIch lag um -0.08881353 daneben\u001b[0m\n",
      "neues weight1: -0.35703017 und neues bias1: -0.29436137\n",
      "neues weight2: -3.26358673 und neues bias2: -2.58073204\n",
      "Wir haben 0.030538555088334154 und -1.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -1.61430901\n",
      "\u001b[31mIch lag um -0.09915749 daneben\u001b[0m\n",
      "neues weight1: -0.35721049 und neues bias1: -0.30026602\n",
      "neues weight2: -3.26417399 und neues bias2: -2.57874889\n",
      "Wir haben 0.03274549162877728 und -1.4848484848484849 als Trainings Daten\n",
      "Ich sage y vorraus: -1.59224402\n",
      "\u001b[31mIch lag um -0.10739554 daneben\u001b[0m\n",
      "neues weight1: -0.35741911 und neues bias1: -0.30663678\n",
      "neues weight2: -3.26482314 und neues bias2: -2.57660098\n",
      "Wir haben 0.03511191734215131 und -1.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -1.56851787\n",
      "\u001b[31mIch lag um -0.11397242 daneben\u001b[0m\n",
      "neues weight1: -0.35765550 und neues bias1: -0.31336926\n",
      "neues weight2: -3.26552696 und neues bias2: -2.57432153\n",
      "Wir haben 0.037649358067924674 und -1.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -1.54348020\n",
      "\u001b[31mIch lag um -0.11923777 daneben\u001b[0m\n",
      "neues weight1: -0.35791947 und neues bias1: -0.32038072\n",
      "neues weight2: -3.26627977 und neues bias2: -2.57193678\n",
      "Wir haben 0.040370172585965536 und -1.393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.51740515\n",
      "\u001b[31mIch lag um -0.12346576 daneben\u001b[0m\n",
      "neues weight1: -0.35821114 und neues bias1: -0.32760550\n",
      "neues weight2: -3.26707700 und neues bias2: -2.56946746\n",
      "Wir haben 0.04328761281083057 und -1.3636363636363638 als Trainings Daten\n",
      "Ich sage y vorraus: -1.49050723\n",
      "\u001b[31mIch lag um -0.12687086 daneben\u001b[0m\n",
      "neues weight1: -0.35853085 und neues bias1: -0.33499128\n",
      "neues weight2: -3.26791498 und neues bias2: -2.56693005\n",
      "Wir haben 0.046415888336127795 und -1.3333333333333333 als Trainings Daten\n",
      "Ich sage y vorraus: -1.46295384\n",
      "\u001b[31mIch lag um -0.12962051 daneben\u001b[0m\n",
      "neues weight1: -0.35887920 und neues bias1: -0.34249622\n",
      "neues weight2: -3.26879076 und neues bias2: -2.56433764\n",
      "Wir haben 0.049770235643321115 und -1.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.43487535\n",
      "\u001b[31mIch lag um -0.13184505 daneben\u001b[0m\n",
      "neues weight1: -0.35925698 und neues bias1: -0.35008661\n",
      "neues weight2: -3.26970189 und neues bias2: -2.56170074\n",
      "Wir haben 0.0533669923120631 und -1.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -1.40637294\n",
      "\u001b[31mIch lag um -0.13364567 daneben\u001b[0m\n",
      "neues weight1: -0.35966515 und neues bias1: -0.35773509\n",
      "neues weight2: -3.27064634 und neues bias2: -2.55902782\n",
      "Wir haben 0.05722367659350217 und -1.2424242424242424 als Trainings Daten\n",
      "Ich sage y vorraus: -1.37752480\n",
      "\u001b[31mIch lag um -0.13510055 daneben\u001b[0m\n",
      "neues weight1: -0.36010486 und neues bias1: -0.36541916\n",
      "neues weight2: -3.27162243 und neues bias2: -2.55632581\n",
      "Wir haben 0.06135907273413173 und -1.2121212121212122 als Trainings Daten\n",
      "Ich sage y vorraus: -1.34839100\n",
      "\u001b[31mIch lag um -0.13626979 daneben\u001b[0m\n",
      "neues weight1: -0.36057739 und neues bias1: -0.37312013\n",
      "neues weight2: -3.27262869 und neues bias2: -2.55360042\n",
      "Wir haben 0.06579332246575682 und -1.1818181818181817 als Trainings Daten\n",
      "Ich sage y vorraus: -1.31901732\n",
      "\u001b[31mIch lag um -0.13719913 daneben\u001b[0m\n",
      "neues weight1: -0.36108413 und neues bias1: -0.38082218\n",
      "neues weight2: -3.27366384 und neues bias2: -2.55085643\n",
      "Wir haben 0.07054802310718646 und -1.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: -1.28943814\n",
      "\u001b[31mIch lag um -0.13792298 daneben\u001b[0m\n",
      "neues weight1: -0.36162661 und neues bias1: -0.38851169\n",
      "neues weight2: -3.27472674 und neues bias2: -2.54809797\n",
      "Wir haben 0.07564633275546291 und -1.121212121212121 als Trainings Daten\n",
      "Ich sage y vorraus: -1.25967882\n",
      "\u001b[31mIch lag um -0.13846670 daneben\u001b[0m\n",
      "neues weight1: -0.36220644 und neues bias1: -0.39617668\n",
      "neues weight2: -3.27581632 und neues bias2: -2.54532864\n",
      "Wir haben 0.08111308307896872 und -1.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -1.22975748\n",
      "\u001b[31mIch lag um -0.13884839 daneben\u001b[0m\n",
      "neues weight1: -0.36282531 und neues bias1: -0.40380634\n",
      "neues weight2: -3.27693155 und neues bias2: -2.54255167\n",
      "Wir haben 0.08697490026177834 und -1.0606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.19968638\n",
      "\u001b[31mIch lag um -0.13908032 daneben\u001b[0m\n",
      "neues weight1: -0.36348496 und neues bias1: -0.41139077\n",
      "neues weight2: -3.27807143 und neues bias2: -2.53977007\n",
      "Wir haben 0.093260334688322 und -1.0303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.16947300\n",
      "\u001b[31mIch lag um -0.13916996 daneben\u001b[0m\n",
      "neues weight1: -0.36418719 und neues bias1: -0.41892059\n",
      "neues weight2: -3.27923495 und neues bias2: -2.53698667\n",
      "Wir haben 0.1 und -1.0 als Trainings Daten\n",
      "Ich sage y vorraus: -1.13912090\n",
      "\u001b[31mIch lag um -0.13912090 daneben\u001b[0m\n",
      "neues weight1: -0.36493382 und neues bias1: -0.42638680\n",
      "neues weight2: -3.28042103 und neues bias2: -2.53420425\n",
      "Wir haben 0.10722672220103231 und -0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.10863036\n",
      "\u001b[31mIch lag um -0.13893339 daneben\u001b[0m\n",
      "neues weight1: -0.36572663 und neues bias1: -0.43378059\n",
      "neues weight2: -3.28162856 und neues bias2: -2.53142558\n",
      "Wir haben 0.11497569953977356 und -0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.07799886\n",
      "\u001b[31mIch lag um -0.13860492 daneben\u001b[0m\n",
      "neues weight1: -0.36656739 und neues bias1: -0.44109313\n",
      "neues weight2: -3.28285632 und neues bias2: -2.52865348\n",
      "Wir haben 0.12328467394420659 und -0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.04722147\n",
      "\u001b[31mIch lag um -0.13813056 daneben\u001b[0m\n",
      "neues weight1: -0.36745780 und neues bias1: -0.44831554\n",
      "neues weight2: -3.28410298 und neues bias2: -2.52589087\n",
      "Wir haben 0.13219411484660293 und -0.8787878787878787 als Trainings Daten\n",
      "Ich sage y vorraus: -1.01629115\n",
      "\u001b[31mIch lag um -0.13750327 daneben\u001b[0m\n",
      "neues weight1: -0.36839945 und neues bias1: -0.45543872\n",
      "neues weight2: -3.28536710 und neues bias2: -2.52314081\n",
      "Wir haben 0.14174741629268056 und -0.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -0.98519896\n",
      "\u001b[31mIch lag um -0.13671411 daneben\u001b[0m\n",
      "neues weight1: -0.36939375 und neues bias1: -0.46245332\n",
      "neues weight2: -3.28664707 und neues bias2: -2.52040652\n",
      "Wir haben 0.1519911082952934 und -0.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -0.95393422\n",
      "\u001b[31mIch lag um -0.13575240 daneben\u001b[0m\n",
      "neues weight1: -0.37044193 und neues bias1: -0.46934965\n",
      "neues weight2: -3.28794111 und neues bias2: -2.51769147\n",
      "Wir haben 0.16297508346206444 und -0.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -0.92248469\n",
      "\u001b[31mIch lag um -0.13460590 daneben\u001b[0m\n",
      "neues weight1: -0.37154494 und neues bias1: -0.47611763\n",
      "neues weight2: -3.28924724 und neues bias2: -2.51499936\n",
      "Wir haben 0.17475284000076838 und -0.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -0.89083662\n",
      "\u001b[31mIch lag um -0.13326087 daneben\u001b[0m\n",
      "neues weight1: -0.37270340 und neues bias1: -0.48274674\n",
      "neues weight2: -3.29056327 und neues bias2: -2.51233414\n",
      "Wir haben 0.1873817422860384 und -0.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -0.85897492\n",
      "\u001b[31mIch lag um -0.13170220 daneben\u001b[0m\n",
      "neues weight1: -0.37391750 und neues bias1: -0.48922603\n",
      "neues weight2: -3.29188676 und neues bias2: -2.50970010\n",
      "Wir haben 0.20092330025650468 und -0.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -0.82688319\n",
      "\u001b[31mIch lag um -0.12991349 daneben\u001b[0m\n",
      "neues weight1: -0.37518694 und neues bias1: -0.49554405\n",
      "neues weight2: -3.29321499 und neues bias2: -2.50710183\n",
      "Wir haben 0.21544346900318845 und -0.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -0.79454384\n",
      "\u001b[31mIch lag um -0.12787717 daneben\u001b[0m\n",
      "neues weight1: -0.37651080 und neues bias1: -0.50168891\n",
      "neues weight2: -3.29454498 und neues bias2: -2.50454428\n",
      "Wir haben 0.23101297000831605 und -0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -0.76193823\n",
      "\u001b[31mIch lag um -0.12557460 daneben\u001b[0m\n",
      "neues weight1: -0.37788748 und neues bias1: -0.50764822\n",
      "neues weight2: -3.29587340 und neues bias2: -2.50203279\n",
      "Wir haben 0.24770763559917114 und -0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -0.72904681\n",
      "\u001b[31mIch lag um -0.12298620 daneben\u001b[0m\n",
      "neues weight1: -0.37931451 und neues bias1: -0.51340917\n",
      "neues weight2: -3.29719659 und neues bias2: -2.49957307\n",
      "Wir haben 0.26560877829466867 und -0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -0.69584926\n",
      "\u001b[31mIch lag um -0.12009168 daneben\u001b[0m\n",
      "neues weight1: -0.38078848 und neues bias1: -0.51895854\n",
      "neues weight2: -3.29851051 und neues bias2: -2.49717123\n",
      "Wir haben 0.2848035868435802 und -0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -0.66232476\n",
      "\u001b[31mIch lag um -0.11687021 daneben\u001b[0m\n",
      "neues weight1: -0.38230484 und neues bias1: -0.52428279\n",
      "neues weight2: -3.29981073 und neues bias2: -2.49483383\n",
      "Wir haben 0.30538555088334157 und -0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -0.62845225\n",
      "\u001b[31mIch lag um -0.11330074 daneben\u001b[0m\n",
      "neues weight1: -0.38385783 und neues bias1: -0.52936814\n",
      "neues weight2: -3.30109239 und neues bias2: -2.49256781\n",
      "Wir haben 0.32745491628777285 und -0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: -0.59421082\n",
      "\u001b[31mIch lag um -0.10936233 daneben\u001b[0m\n",
      "neues weight1: -0.38544026 und neues bias1: -0.53420065\n",
      "neues weight2: -3.30235021 und neues bias2: -2.49038057\n",
      "Wir haben 0.3511191734215131 und -0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -0.55958008\n",
      "\u001b[31mIch lag um -0.10503462 daneben\u001b[0m\n",
      "neues weight1: -0.38704339 und neues bias1: -0.53876643\n",
      "neues weight2: -3.30357843 und neues bias2: -2.48827987\n",
      "Wir haben 0.37649358067924676 und -0.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.52454074\n",
      "\u001b[31mIch lag um -0.10029831 daneben\u001b[0m\n",
      "neues weight1: -0.38865678 und neues bias1: -0.54305173\n",
      "neues weight2: -3.30477083 und neues bias2: -2.48627391\n",
      "Wir haben 0.4037017258596556 und -0.3939393939393938 als Trainings Daten\n",
      "Ich sage y vorraus: -0.48907521\n",
      "\u001b[31mIch lag um -0.09513581 daneben\u001b[0m\n",
      "neues weight1: -0.39026815 und neues bias1: -0.54704322\n",
      "neues weight2: -3.30592072 und neues bias2: -2.48437119\n",
      "Wir haben 0.43287612810830595 und -0.36363636363636354 als Trainings Daten\n",
      "Ich sage y vorraus: -0.45316834\n",
      "\u001b[31mIch lag um -0.08953198 daneben\u001b[0m\n",
      "neues weight1: -0.39186330 und neues bias1: -0.55072822\n",
      "neues weight2: -3.30702091 und neues bias2: -2.48258055\n",
      "Wir haben 0.464158883361278 und -0.33333333333333326 als Trainings Daten\n",
      "Ich sage y vorraus: -0.41680826\n",
      "\u001b[31mIch lag um -0.08347493 daneben\u001b[0m\n",
      "neues weight1: -0.39342600 und neues bias1: -0.55409494\n",
      "neues weight2: -3.30806378 und neues bias2: -2.48091105\n",
      "Wir haben 0.49770235643321115 und -0.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -0.37998730\n",
      "\u001b[31mIch lag um -0.07695699 daneben\u001b[0m\n",
      "neues weight1: -0.39493798 und neues bias1: -0.55713288\n",
      "neues weight2: -3.30904128 und neues bias2: -2.47937191\n",
      "Wir haben 0.533669923120631 und -0.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -0.34270302\n",
      "\u001b[31mIch lag um -0.06997575 daneben\u001b[0m\n",
      "neues weight1: -0.39637900 und neues bias1: -0.55983307\n",
      "neues weight2: -3.30994495 und neues bias2: -2.47797240\n",
      "Wir haben 0.5722367659350217 und -0.24242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.30495935\n",
      "\u001b[31mIch lag um -0.06253511 daneben\u001b[0m\n",
      "neues weight1: -0.39772690 und neues bias1: -0.56218857\n",
      "neues weight2: -3.31076605 und neues bias2: -2.47672170\n",
      "Wir haben 0.6135907273413173 und -0.21212121212121215 als Trainings Daten\n",
      "Ich sage y vorraus: -0.26676765\n",
      "\u001b[31mIch lag um -0.05464644 daneben\u001b[0m\n",
      "neues weight1: -0.39895788 und neues bias1: -0.56419476\n",
      "neues weight2: -3.31149559 und neues bias2: -2.47562877\n",
      "Wir haben 0.6579332246575682 und -0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: -0.22814787\n",
      "\u001b[31mIch lag um -0.04632969 daneben\u001b[0m\n",
      "neues weight1: -0.40004678 und neues bias1: -0.56584980\n",
      "neues weight2: -3.31212446 und neues bias2: -2.47470217\n",
      "Wir haben 0.7054802310718645 und -0.15151515151515138 als Trainings Daten\n",
      "Ich sage y vorraus: -0.18912958\n",
      "\u001b[31mIch lag um -0.03761443 daneben\u001b[0m\n",
      "neues weight1: -0.40096756 und neues bias1: -0.56715497\n",
      "neues weight2: -3.31264358 und neues bias2: -2.47394989\n",
      "Wir haben 0.7564633275546291 und -0.12121212121212109 als Trainings Daten\n",
      "Ich sage y vorraus: -0.14975285\n",
      "\u001b[31mIch lag um -0.02854073 daneben\u001b[0m\n",
      "neues weight1: -0.40169383 und neues bias1: -0.56811505\n",
      "neues weight2: -3.31304408 und neues bias2: -2.47337907\n",
      "Wir haben 0.8111308307896873 und -0.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -0.11006890\n",
      "\u001b[31mIch lag um -0.01915981 daneben\u001b[0m\n",
      "neues weight1: -0.40219960 und neues bias1: -0.56873860\n",
      "neues weight2: -3.31331742 und neues bias2: -2.47299588\n",
      "Wir haben 0.8697490026177834 und -0.06060606060606055 als Trainings Daten\n",
      "Ich sage y vorraus: -0.07014047\n",
      "\u001b[31mIch lag um -0.00953441 daneben\u001b[0m\n",
      "neues weight1: -0.40246011 und neues bias1: -0.56903812\n",
      "neues weight2: -3.31345571 und neues bias2: -2.47280519\n",
      "Wir haben 0.9326033468832199 und -0.030303030303030297 als Trainings Daten\n",
      "Ich sage y vorraus: -0.03004175\n",
      "\u001b[31mIch lag um 0.00026128 daneben\u001b[0m\n",
      "neues weight1: -0.40245274 und neues bias1: -0.56903021\n",
      "neues weight2: -3.31345186 und neues bias2: -2.47281041\n",
      "Wir haben 1.0 und 0.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.01014208\n",
      "\u001b[31mIch lag um 0.01014208 daneben\u001b[0m\n",
      "neues weight1: -0.40215804 und neues bias1: -0.56873552\n",
      "neues weight2: -3.31329986 und neues bias2: -2.47301325\n",
      "Wir haben 1.072267222010323 und 0.030303030303030252 als Trainings Daten\n",
      "Ich sage y vorraus: 0.05031589\n",
      "\u001b[31mIch lag um 0.02001286 daneben\u001b[0m\n",
      "neues weight1: -0.40156079 und neues bias1: -0.56817852\n",
      "neues weight2: -3.31299503 und neues bias2: -2.47341351\n",
      "Wir haben 1.1497569953977356 und 0.060606060606060524 als Trainings Daten\n",
      "Ich sage y vorraus: 0.09037600\n",
      "\u001b[31mIch lag um 0.02976994 daneben\u001b[0m\n",
      "neues weight1: -0.40065102 und neues bias1: -0.56738725\n",
      "neues weight2: -3.31253428 und neues bias2: -2.47400891\n",
      "Wir haben 1.232846739442066 und 0.09090909090909083 als Trainings Daten\n",
      "Ich sage y vorraus: 0.13021215\n",
      "\u001b[31mIch lag um 0.03930306 daneben\u001b[0m\n",
      "neues weight1: -0.39942495 und neues bias1: -0.56639275\n",
      "neues weight2: -3.31191630 und neues bias2: -2.47479497\n",
      "Wir haben 1.3219411484660286 und 0.12121212121212108 als Trainings Daten\n",
      "Ich sage y vorraus: 0.16970968\n",
      "\u001b[31mIch lag um 0.04849756 daneben\u001b[0m\n",
      "neues weight1: -0.39788587 und neues bias1: -0.56522849\n",
      "neues weight2: -3.31114181 und neues bias2: -2.47576492\n",
      "Wir haben 1.4174741629268048 und 0.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: 0.20875200\n",
      "\u001b[31mIch lag um 0.05723685 daneben\u001b[0m\n",
      "neues weight1: -0.39604473 und neues bias1: -0.56392960\n",
      "neues weight2: -3.31021371 und neues bias2: -2.47690966\n",
      "Wir haben 1.5199110829529332 und 0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: 0.24722300\n",
      "\u001b[31mIch lag um 0.06540482 daneben\u001b[0m\n",
      "neues weight1: -0.39392055 und neues bias1: -0.56253203\n",
      "neues weight2: -3.30913722 und neues bias2: -2.47821776\n",
      "Wir haben 1.629750834620645 und 0.21212121212121235 als Trainings Daten\n",
      "Ich sage y vorraus: 0.28500924\n",
      "\u001b[31mIch lag um 0.07288803 daneben\u001b[0m\n",
      "neues weight1: -0.39154058 und neues bias1: -0.56107170\n",
      "neues weight2: -3.30791995 und neues bias2: -2.47967552\n",
      "Wir haben 1.7475284000076847 und 0.24242424242424265 als Trainings Daten\n",
      "Ich sage y vorraus: 0.32200176\n",
      "\u001b[31mIch lag um 0.07957751 daneben\u001b[0m\n",
      "neues weight1: -0.38894006 und neues bias1: -0.55958359\n",
      "neues weight2: -3.30657197 und neues bias2: -2.48126707\n",
      "Wir haben 1.873817422860385 und 0.27272727272727293 als Trainings Daten\n",
      "Ich sage y vorraus: 0.35809721\n",
      "\u001b[31mIch lag um 0.08536994 daneben\u001b[0m\n",
      "neues weight1: -0.38616174 und neues bias1: -0.55810088\n",
      "neues weight2: -3.30510582 und neues bias2: -2.48297447\n",
      "Wir haben 2.0092330025650478 und 0.30303030303030315 als Trainings Daten\n",
      "Ich sage y vorraus: 0.39319830\n",
      "\u001b[31mIch lag um 0.09016800 daneben\u001b[0m\n",
      "neues weight1: -0.38325507 und neues bias1: -0.55665423\n",
      "neues weight2: -3.30353649 und neues bias2: -2.48477783\n",
      "Wir haben 2.1544346900318843 und 0.3333333333333334 als Trainings Daten\n",
      "Ich sage y vorraus: 0.42721330\n",
      "\u001b[31mIch lag um 0.09387996 daneben\u001b[0m\n",
      "neues weight1: -0.38027506 und neues bias1: -0.55527103\n",
      "neues weight2: -3.30188143 und neues bias2: -2.48665542\n",
      "Wir haben 2.31012970008316 und 0.3636363636363637 als Trainings Daten\n",
      "Ich sage y vorraus: 0.46005482\n",
      "\u001b[31mIch lag um 0.09641846 daneben\u001b[0m\n",
      "neues weight1: -0.37728084 und neues bias1: -0.55397490\n",
      "neues weight2: -3.30016049 und neues bias2: -2.48858379\n",
      "Wir haben 2.4770763559917115 und 0.39393939393939403 als Trainings Daten\n",
      "Ich sage y vorraus: 0.49163799\n",
      "\u001b[31mIch lag um 0.09769859 daneben\u001b[0m\n",
      "neues weight1: -0.37433387 und neues bias1: -0.55278520\n",
      "neues weight2: -3.29839595 und neues bias2: -2.49053777\n",
      "Wir haben 2.656087782946687 und 0.42424242424242437 als Trainings Daten\n",
      "Ich sage y vorraus: 0.52187816\n",
      "\u001b[31mIch lag um 0.09763573 daneben\u001b[0m\n",
      "neues weight1: -0.37149595 und neues bias1: -0.55171675\n",
      "neues weight2: -3.29661254 und neues bias2: -2.49249048\n",
      "Wir haben 2.848035868435802 und 0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: 0.55068890\n",
      "\u001b[31mIch lag um 0.09614344 daneben\u001b[0m\n",
      "neues weight1: -0.36882685 und neues bias1: -0.55077957\n",
      "neues weight2: -3.29483750 und neues bias2: -2.49441335\n",
      "Wir haben 3.0538555088334154 und 0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.57798066\n",
      "\u001b[31mIch lag um 0.09313217 daneben\u001b[0m\n",
      "neues weight1: -0.36638165 und neues bias1: -0.54997888\n",
      "neues weight2: -3.29310061 und neues bias2: -2.49627599\n",
      "Wir haben 3.2745491628777286 und 0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: 0.60366099\n",
      "\u001b[31mIch lag um 0.08850948 daneben\u001b[0m\n",
      "neues weight1: -0.36420795 und neues bias1: -0.54931507\n",
      "neues weight2: -3.29143425 und neues bias2: -2.49804618\n",
      "Wir haben 3.511191734215131 und 0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: 0.62763713\n",
      "\u001b[31mIch lag um 0.08218258 daneben\u001b[0m\n",
      "neues weight1: -0.36234297 und neues bias1: -0.54878391\n",
      "neues weight2: -3.28987337 und neues bias2: -2.49968983\n",
      "Wir haben 3.7649358067924674 und 0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: 0.64982159\n",
      "\u001b[31mIch lag um 0.07406401 daneben\u001b[0m\n",
      "neues weight1: -0.36081079 und neues bias1: -0.54837695\n",
      "neues weight2: -3.28845529 und neues bias2: -2.50117111\n",
      "Wir haben 4.037017258596554 und 0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: 0.67014111\n",
      "\u001b[31mIch lag um 0.06408050 daneben\u001b[0m\n",
      "neues weight1: -0.35962021 und neues bias1: -0.54808204\n",
      "neues weight2: -3.28721933 und neues bias2: -2.50245272\n",
      "Wir haben 4.328761281083057 und 0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: 0.68854855\n",
      "\u001b[31mIch lag um 0.05218492 daneben\u001b[0m\n",
      "neues weight1: -0.35876352 und neues bias1: -0.54788413\n",
      "neues weight2: -3.28620618 und neues bias2: -2.50349642\n",
      "Wir haben 4.641588833612782 und 0.666666666666667 als Trainings Daten\n",
      "Ich sage y vorraus: 0.70503628\n",
      "\u001b[31mIch lag um 0.03836961 daneben\u001b[0m\n",
      "neues weight1: -0.35821673 und neues bias1: -0.54776633\n",
      "neues weight2: -3.28545693 und neues bias2: -2.50426381\n",
      "Wir haben 4.9770235643321135 und 0.6969696969696972 als Trainings Daten\n",
      "Ich sage y vorraus: 0.71964871\n",
      "\u001b[31mIch lag um 0.02267901 daneben\u001b[0m\n",
      "neues weight1: -0.35794146 und neues bias1: -0.54771102\n",
      "neues weight2: -3.28501185 und neues bias2: -2.50471739\n",
      "Wir haben 5.336699231206313 und 0.7272727272727275 als Trainings Daten\n",
      "Ich sage y vorraus: 0.73249108\n",
      "\u001b[31mIch lag um 0.00521835 daneben\u001b[0m\n",
      "neues weight1: -0.35788860 und neues bias1: -0.54770111\n",
      "neues weight2: -3.28490900 und neues bias2: -2.50482176\n",
      "Wir haben 5.72236765935022 und 0.7575757575757578 als Trainings Daten\n",
      "Ich sage y vorraus: 0.74373158\n",
      "\u001b[31mIch lag um -0.01384418 daneben\u001b[0m\n",
      "neues weight1: -0.35800316 und neues bias1: -0.54772113\n",
      "neues weight2: -3.28518282 und neues bias2: -2.50454488\n",
      "Wir haben 6.135907273413176 und 0.7878787878787881 als Trainings Daten\n",
      "Ich sage y vorraus: 0.75359544\n",
      "\u001b[31mIch lag um -0.03428335 daneben\u001b[0m\n",
      "neues weight1: -0.35822977 und neues bias1: -0.54775807\n",
      "neues weight2: -3.28586284 und neues bias2: -2.50385921\n",
      "Wir haben 6.5793322465756825 und 0.8181818181818183 als Trainings Daten\n",
      "Ich sage y vorraus: 0.76235131\n",
      "\u001b[31mIch lag um -0.05583051 daneben\u001b[0m\n",
      "neues weight1: -0.35851766 und neues bias1: -0.54780182\n",
      "neues weight2: -3.28697277 und neues bias2: -2.50274260\n",
      "Wir haben 7.054802310718645 und 0.8484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.77029242\n",
      "\u001b[31mIch lag um -0.07819243 daneben\u001b[0m\n",
      "neues weight1: -0.35882455 und neues bias1: -0.54784532\n",
      "neues weight2: -3.28852999 und neues bias2: -2.50117875\n",
      "Wir haben 7.56463327554629 und 0.8787878787878789 als Trainings Daten\n",
      "Ich sage y vorraus: 0.77771593\n",
      "\u001b[31mIch lag um -0.10107195 daneben\u001b[0m\n",
      "neues weight1: -0.35911880 und neues bias1: -0.54788422\n",
      "neues weight2: -3.29054551 und neues bias2: -2.49915731\n",
      "Wir haben 8.111308307896872 und 0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: 0.78490408\n",
      "\u001b[31mIch lag um -0.12418683 daneben\u001b[0m\n",
      "neues weight1: -0.35937980 und neues bias1: -0.54791640\n",
      "neues weight2: -3.29302435 und neues bias2: -2.49667358\n",
      "Wir haben 8.697490026177835 und 0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79210965\n",
      "\u001b[31mIch lag um -0.14728429 daneben\u001b[0m\n",
      "neues weight1: -0.35959698 und neues bias1: -0.54794137\n",
      "neues weight2: -3.29596624 und neues bias2: -2.49372789\n",
      "Wir haben 9.326033468832199 und 0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79954689\n",
      "\u001b[31mIch lag um -0.17015008 daneben\u001b[0m\n",
      "neues weight1: -0.35976774 und neues bias1: -0.54795968\n",
      "neues weight2: -3.29936646 und neues bias2: -2.49032489\n",
      "Wir haben 10.0 und 1.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.80738772\n",
      "\u001b[31mIch lag um -0.19261228 daneben\u001b[0m\n",
      "neues weight1: -0.35989513 und neues bias1: -0.54797242\n",
      "neues weight2: -3.30321678 und neues bias2: -2.48647264\n",
      "\n",
      "\n",
      "Ich trainiere in Epoche 8001\n",
      "Wir haben 0.01 und -2.0 als Trainings Daten\n",
      "Ich sage y vorraus: -0.82907894\n",
      "\u001b[31mIch lag um 1.17092106 daneben\u001b[0m\n",
      "neues weight1: -0.35907344 und neues bias1: -0.49116135\n",
      "neues weight2: -3.29675999 und neues bias2: -2.51510061\n",
      "Wir haben 0.010722672220103232 und -1.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.00457466\n",
      "\u001b[31mIch lag um 0.96512231 daneben\u001b[0m\n",
      "neues weight1: -0.35853435 und neues bias1: -0.44088504\n",
      "neues weight2: -3.28791590 und neues bias2: -2.53440305\n",
      "Wir haben 0.011497569953977356 und -1.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.16075655\n",
      "\u001b[31mIch lag um 0.77863739 daneben\u001b[0m\n",
      "neues weight1: -0.35804841 und neues bias1: -0.39862022\n",
      "neues weight2: -3.28140982 und neues bias2: -2.54997580\n",
      "Wir haben 0.012328467394420659 und -1.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.29469768\n",
      "\u001b[31mIch lag um 0.61439323 daneben\u001b[0m\n",
      "neues weight1: -0.35762405 und neues bias1: -0.36419929\n",
      "neues weight2: -3.27670919 und neues bias2: -2.56226367\n",
      "Wir haben 0.013219411484660288 und -1.878787878787879 als Trainings Daten\n",
      "Ich sage y vorraus: -1.40541263\n",
      "\u001b[31mIch lag um 0.47337525 daneben\u001b[0m\n",
      "neues weight1: -0.35726507 und neues bias1: -0.33704384\n",
      "neues weight2: -3.27336666 und neues bias2: -2.57173117\n",
      "Wir haben 0.014174741629268055 und -1.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -1.49362164\n",
      "\u001b[31mIch lag um 0.35486321 daneben\u001b[0m\n",
      "neues weight1: -0.35697148 und neues bias1: -0.31633202\n",
      "neues weight2: -3.27102912 und neues bias2: -2.57882843\n",
      "Wir haben 0.01519911082952934 und -1.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -1.56122650\n",
      "\u001b[31mIch lag um 0.25695532 daneben\u001b[0m\n",
      "neues weight1: -0.35674071 und neues bias1: -0.30114874\n",
      "neues weight2: -3.26943037 und neues bias2: -2.58396754\n",
      "Wir haben 0.016297508346206444 und -1.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -1.61075143\n",
      "\u001b[31mIch lag um 0.17712736 daneben\u001b[0m\n",
      "neues weight1: -0.35656868 und neues bias1: -0.29059290\n",
      "neues weight2: -3.26837586 und neues bias2: -2.58751009\n",
      "Wir haben 0.01747528400007684 und -1.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -1.64489907\n",
      "\u001b[31mIch lag um 0.11267668 daneben\u001b[0m\n",
      "neues weight1: -0.35645067 und neues bias1: -0.28384014\n",
      "neues weight2: -3.26772593 und neues bias2: -2.58976362\n",
      "Wir haben 0.01873817422860384 und -1.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -1.66626254\n",
      "\u001b[31mIch lag um 0.06101019 daneben\u001b[0m\n",
      "neues weight1: -0.35638192 und neues bias1: -0.28017131\n",
      "neues weight2: -3.26738108 und neues bias2: -2.59098383\n",
      "Wir haben 0.02009233002565047 und -1.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67717181\n",
      "\u001b[31mIch lag um 0.01979788 daneben\u001b[0m\n",
      "neues weight1: -0.35635796 und neues bias1: -0.27897876\n",
      "neues weight2: -3.26727034 und neues bias2: -2.59137978\n",
      "Wir haben 0.021544346900318846 und -1.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67963381\n",
      "\u001b[31mIch lag um -0.01296715 daneben\u001b[0m\n",
      "neues weight1: -0.35637480 und neues bias1: -0.27976012\n",
      "neues weight2: -3.26734271 und neues bias2: -2.59112044\n",
      "Wir haben 0.023101297000831605 und -1.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67532878\n",
      "\u001b[31mIch lag um -0.03896515 daneben\u001b[0m\n",
      "neues weight1: -0.35642900 und neues bias1: -0.28210634\n",
      "neues weight2: -3.26756114 und neues bias2: -2.59034114\n",
      "Wir haben 0.024770763559917114 und -1.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.66563609\n",
      "\u001b[31mIch lag um -0.05957548 daneben\u001b[0m\n",
      "neues weight1: -0.35651771 und neues bias1: -0.28568786\n",
      "neues weight2: -3.26789833 und neues bias2: -2.58914963\n",
      "Wir haben 0.026560877829466867 und -1.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -1.65167247\n",
      "\u001b[31mIch lag um -0.07591489 daneben\u001b[0m\n",
      "neues weight1: -0.35663865 und neues bias1: -0.29024118\n",
      "neues weight2: -3.26833390 und neues bias2: -2.58763133\n",
      "Wir haben 0.02848035868435802 und -1.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -1.63433306\n",
      "\u001b[31mIch lag um -0.08887851 daneben\u001b[0m\n",
      "neues weight1: -0.35679004 und neues bias1: -0.29555661\n",
      "neues weight2: -3.26885237 und neues bias2: -2.58585376\n",
      "Wir haben 0.030538555088334154 und -1.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -1.61433012\n",
      "\u001b[31mIch lag um -0.09917861 daneben\u001b[0m\n",
      "neues weight1: -0.35697056 und neues bias1: -0.30146787\n",
      "neues weight2: -3.26944190 und neues bias2: -2.58387019\n",
      "Wir haben 0.03274549162877728 und -1.4848484848484849 als Trainings Daten\n",
      "Ich sage y vorraus: -1.59222737\n",
      "\u001b[31mIch lag um -0.10737889 daneben\u001b[0m\n",
      "neues weight1: -0.35717933 und neues bias1: -0.30784332\n",
      "neues weight2: -3.27009328 und neues bias2: -2.58172261\n",
      "Wir haben 0.03511191734215131 und -1.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -1.56846913\n",
      "\u001b[31mIch lag um -0.11392368 daneben\u001b[0m\n",
      "neues weight1: -0.35741582 und neues bias1: -0.31457879\n",
      "neues weight2: -3.27079927 und neues bias2: -2.57944414\n",
      "Wir haben 0.037649358067924674 und -1.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -1.54340450\n",
      "\u001b[31mIch lag um -0.11916207 daneben\u001b[0m\n",
      "neues weight1: -0.35767986 und neues bias1: -0.32159178\n",
      "neues weight2: -3.27155417 und neues bias2: -2.57706090\n",
      "Wir haben 0.040370172585965536 und -1.393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.51730705\n",
      "\u001b[31mIch lag um -0.12336766 daneben\u001b[0m\n",
      "neues weight1: -0.35797154 und neues bias1: -0.32881686\n",
      "neues weight2: -3.27235342 und neues bias2: -2.57459354\n",
      "Wir haben 0.04328761281083057 und -1.3636363636363638 als Trainings Daten\n",
      "Ich sage y vorraus: -1.49039072\n",
      "\u001b[31mIch lag um -0.12675436 daneben\u001b[0m\n",
      "neues weight1: -0.35829122 und neues bias1: -0.33620190\n",
      "neues weight2: -3.27319335 und neues bias2: -2.57205845\n",
      "Wir haben 0.046415888336127795 und -1.3333333333333333 als Trainings Daten\n",
      "Ich sage y vorraus: -1.46282238\n",
      "\u001b[31mIch lag um -0.12948905 daneben\u001b[0m\n",
      "neues weight1: -0.35863949 und neues bias1: -0.34370525\n",
      "neues weight2: -3.27407099 und neues bias2: -2.56946867\n",
      "Wir haben 0.049770235643321115 und -1.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.43473189\n",
      "\u001b[31mIch lag um -0.13170159 daneben\u001b[0m\n",
      "neues weight1: -0.35901715 und neues bias1: -0.35129334\n",
      "neues weight2: -3.27498390 und neues bias2: -2.56683464\n",
      "Wir haben 0.0533669923120631 und -1.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -1.40622000\n",
      "\u001b[31mIch lag um -0.13349273 daneben\u001b[0m\n",
      "neues weight1: -0.35942518 und neues bias1: -0.35893894\n",
      "neues weight2: -3.27593007 und neues bias2: -2.56416479\n",
      "Wir haben 0.05722367659350217 und -1.2424242424242424 als Trainings Daten\n",
      "Ich sage y vorraus: -1.37736452\n",
      "\u001b[31mIch lag um -0.13494028 daneben\u001b[0m\n",
      "neues weight1: -0.35986470 und neues bias1: -0.36661968\n",
      "neues weight2: -3.27690779 und neues bias2: -2.56146598\n",
      "Wir haben 0.06135907273413173 und -1.2121212121212122 als Trainings Daten\n",
      "Ich sage y vorraus: -1.34822520\n",
      "\u001b[31mIch lag um -0.13610399 daneben\u001b[0m\n",
      "neues weight1: -0.36033699 und neues bias1: -0.37431696\n",
      "neues weight2: -3.27791561 und neues bias2: -2.55874390\n",
      "Wir haben 0.06579332246575682 und -1.1818181818181817 als Trainings Daten\n",
      "Ich sage y vorraus: -1.31884750\n",
      "\u001b[31mIch lag um -0.13702932 daneben\u001b[0m\n",
      "neues weight1: -0.36084348 und neues bias1: -0.38201503\n",
      "neues weight2: -3.27895225 und neues bias2: -2.55600332\n",
      "Wir haben 0.07054802310718646 und -1.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: -1.28926560\n",
      "\u001b[31mIch lag um -0.13775045 daneben\u001b[0m\n",
      "neues weight1: -0.36138566 und neues bias1: -0.38970035\n",
      "neues weight2: -3.28001658 und neues bias2: -2.55324831\n",
      "Wir haben 0.07564633275546291 und -1.121212121212121 als Trainings Daten\n",
      "Ich sage y vorraus: -1.25950464\n",
      "\u001b[31mIch lag um -0.13829252 daneben\u001b[0m\n",
      "neues weight1: -0.36196516 und neues bias1: -0.39736099\n",
      "neues weight2: -3.28110752 und neues bias2: -2.55048246\n",
      "Wir haben 0.08111308307896872 und -1.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -1.22958257\n",
      "\u001b[31mIch lag um -0.13867347 daneben\u001b[0m\n",
      "neues weight1: -0.36258366 und neues bias1: -0.40498621\n",
      "neues weight2: -3.28222406 und neues bias2: -2.54770899\n",
      "Wir haben 0.08697490026177834 und -1.0606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.19951150\n",
      "\u001b[31mIch lag um -0.13890544 daneben\u001b[0m\n",
      "neues weight1: -0.36324293 und neues bias1: -0.41256612\n",
      "neues weight2: -3.28336519 und neues bias2: -2.54493088\n",
      "Wir haben 0.093260334688322 und -1.0303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.16929882\n",
      "\u001b[31mIch lag um -0.13899579 daneben\u001b[0m\n",
      "neues weight1: -0.36394474 und neues bias1: -0.42009140\n",
      "neues weight2: -3.28452989 und neues bias2: -2.54215096\n",
      "Wir haben 0.1 und -1.0 als Trainings Daten\n",
      "Ich sage y vorraus: -1.13894798\n",
      "\u001b[31mIch lag um -0.13894798 daneben\u001b[0m\n",
      "neues weight1: -0.36469090 und neues bias1: -0.42755307\n",
      "neues weight2: -3.28571710 und neues bias2: -2.53937200\n",
      "Wir haben 0.10722672220103231 und -0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.10845919\n",
      "\u001b[31mIch lag um -0.13876222 daneben\u001b[0m\n",
      "neues weight1: -0.36548323 und neues bias1: -0.43494233\n",
      "neues weight2: -3.28692571 und neues bias2: -2.53659676\n",
      "Wir haben 0.11497569953977356 und -0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.07782987\n",
      "\u001b[31mIch lag um -0.13843593 daneben\u001b[0m\n",
      "neues weight1: -0.36632348 und neues bias1: -0.44225039\n",
      "neues weight2: -3.28815449 und neues bias2: -2.53382804\n",
      "Wir haben 0.12328467394420659 und -0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.04705503\n",
      "\u001b[31mIch lag um -0.13796412 daneben\u001b[0m\n",
      "neues weight1: -0.36721335 und neues bias1: -0.44946839\n",
      "neues weight2: -3.28940213 und neues bias2: -2.53106876\n",
      "Wir haben 0.13219411484660293 und -0.8787878787878787 als Trainings Daten\n",
      "Ich sage y vorraus: -1.01612760\n",
      "\u001b[31mIch lag um -0.13733972 daneben\u001b[0m\n",
      "neues weight1: -0.36815442 und neues bias1: -0.45658723\n",
      "neues weight2: -3.29066717 und neues bias2: -2.52832196\n",
      "Wir haben 0.14174741629268056 und -0.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -0.98503861\n",
      "\u001b[31mIch lag um -0.13655376 daneben\u001b[0m\n",
      "neues weight1: -0.36914812 und neues bias1: -0.46359759\n",
      "neues weight2: -3.29194801 und neues bias2: -2.52559089\n",
      "Wir haben 0.1519911082952934 und -0.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -0.95377735\n",
      "\u001b[31mIch lag um -0.13559554 daneben\u001b[0m\n",
      "neues weight1: -0.37019567 und neues bias1: -0.47048978\n",
      "neues weight2: -3.29324287 und neues bias2: -2.52287898\n",
      "Wir haben 0.16297508346206444 und -0.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -0.92233158\n",
      "\u001b[31mIch lag um -0.13445279 daneben\u001b[0m\n",
      "neues weight1: -0.37129802 und neues bias1: -0.47725373\n",
      "neues weight2: -3.29454978 und neues bias2: -2.52018992\n",
      "Wir haben 0.17475284000076838 und -0.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -0.89068753\n",
      "\u001b[31mIch lag um -0.13311177 daneben\u001b[0m\n",
      "neues weight1: -0.37245580 und neues bias1: -0.48387894\n",
      "neues weight2: -3.29586654 und neues bias2: -2.51752769\n",
      "Wir haben 0.1873817422860384 und -0.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -0.85883009\n",
      "\u001b[31mIch lag um -0.13155736 daneben\u001b[0m\n",
      "neues weight1: -0.37366919 und neues bias1: -0.49035445\n",
      "neues weight2: -3.29719070 und neues bias2: -2.51489654\n",
      "Wir haben 0.20092330025650468 und -0.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -0.82674286\n",
      "\u001b[31mIch lag um -0.12977317 daneben\u001b[0m\n",
      "neues weight1: -0.37493790 und neues bias1: -0.49666885\n",
      "neues weight2: -3.29851957 und neues bias2: -2.51230108\n",
      "Wir haben 0.21544346900318845 und -0.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -0.79440828\n",
      "\u001b[31mIch lag um -0.12774161 daneben\u001b[0m\n",
      "neues weight1: -0.37626102 und neues bias1: -0.50281023\n",
      "neues weight2: -3.29985015 und neues bias2: -2.50974624\n",
      "Wir haben 0.23101297000831605 und -0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -0.76180768\n",
      "\u001b[31mIch lag um -0.12544404 daneben\u001b[0m\n",
      "neues weight1: -0.37763693 und neues bias1: -0.50876622\n",
      "neues weight2: -3.30117911 und neues bias2: -2.50723736\n",
      "Wir haben 0.24770763559917114 und -0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -0.72892152\n",
      "\u001b[31mIch lag um -0.12286091 daneben\u001b[0m\n",
      "neues weight1: -0.37906318 und neues bias1: -0.51452402\n",
      "neues weight2: -3.30250279 und neues bias2: -2.50478014\n",
      "Wir haben 0.26560877829466867 und -0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -0.69572949\n",
      "\u001b[31mIch lag um -0.11997191 daneben\u001b[0m\n",
      "neues weight1: -0.38053635 und neues bias1: -0.52007041\n",
      "neues weight2: -3.30381716 und neues bias2: -2.50238071\n",
      "Wir haben 0.2848035868435802 und -0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -0.66221078\n",
      "\u001b[31mIch lag um -0.11675623 daneben\u001b[0m\n",
      "neues weight1: -0.38205192 und neues bias1: -0.52539186\n",
      "neues weight2: -3.30511778 und neues bias2: -2.50004558\n",
      "Wir haben 0.30538555088334157 und -0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -0.62834434\n",
      "\u001b[31mIch lag um -0.11319282 daneben\u001b[0m\n",
      "neues weight1: -0.38360411 und neues bias1: -0.53047460\n",
      "neues weight2: -3.30639981 und neues bias2: -2.49778172\n",
      "Wir haben 0.32745491628777285 und -0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: -0.59410924\n",
      "\u001b[31mIch lag um -0.10926075 daneben\u001b[0m\n",
      "neues weight1: -0.38518575 und neues bias1: -0.53530469\n",
      "neues weight2: -3.30765796 und neues bias2: -2.49559651\n",
      "Wir haben 0.3511191734215131 und -0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -0.55948512\n",
      "\u001b[31mIch lag um -0.10493967 daneben\u001b[0m\n",
      "neues weight1: -0.38678810 und neues bias1: -0.53986825\n",
      "neues weight2: -3.30888647 und neues bias2: -2.49349772\n",
      "Wir haben 0.37649358067924676 und -0.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.52445269\n",
      "\u001b[31mIch lag um -0.10021027 daneben\u001b[0m\n",
      "neues weight1: -0.38840074 und neues bias1: -0.54415154\n",
      "neues weight2: -3.31007913 und neues bias2: -2.49149351\n",
      "Wir haben 0.4037017258596556 und -0.3939393939393938 als Trainings Daten\n",
      "Ich sage y vorraus: -0.48899437\n",
      "\u001b[31mIch lag um -0.09505498 daneben\u001b[0m\n",
      "neues weight1: -0.39001138 und neues bias1: -0.54814124\n",
      "neues weight2: -3.31122924 und neues bias2: -2.48959241\n",
      "Wir haben 0.43287612810830595 und -0.36363636363636354 als Trainings Daten\n",
      "Ich sage y vorraus: -0.45309501\n",
      "\u001b[31mIch lag um -0.08945865 daneben\u001b[0m\n",
      "neues weight1: -0.39160585 und neues bias1: -0.55182466\n",
      "neues weight2: -3.31232963 und neues bias2: -2.48780324\n",
      "Wir haben 0.464158883361278 und -0.33333333333333326 als Trainings Daten\n",
      "Ich sage y vorraus: -0.41674272\n",
      "\u001b[31mIch lag um -0.08340939 daneben\u001b[0m\n",
      "neues weight1: -0.39316792 und neues bias1: -0.55519003\n",
      "neues weight2: -3.31337268 und neues bias2: -2.48613505\n",
      "Wir haben 0.49770235643321115 und -0.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -0.37992984\n",
      "\u001b[31mIch lag um -0.07689954 daneben\u001b[0m\n",
      "neues weight1: -0.39467934 und neues bias1: -0.55822683\n",
      "neues weight2: -3.31435033 und neues bias2: -2.48459706\n",
      "Wir haben 0.533669923120631 und -0.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -0.34265392\n",
      "\u001b[31mIch lag um -0.06992665 daneben\u001b[0m\n",
      "neues weight1: -0.39611987 und neues bias1: -0.56092613\n",
      "neues weight2: -3.31525415 und neues bias2: -2.48319853\n",
      "Wir haben 0.5722367659350217 und -0.24242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.30491886\n",
      "\u001b[31mIch lag um -0.06249462 daneben\u001b[0m\n",
      "neues weight1: -0.39746739 und neues bias1: -0.56328095\n",
      "neues weight2: -3.31607539 und neues bias2: -2.48194863\n",
      "Wir haben 0.6135907273413173 und -0.21212121212121215 als Trainings Daten\n",
      "Ich sage y vorraus: -0.26673600\n",
      "\u001b[31mIch lag um -0.05461479 daneben\u001b[0m\n",
      "neues weight1: -0.39869809 und neues bias1: -0.56528669\n",
      "neues weight2: -3.31680507 und neues bias2: -2.48085634\n",
      "Wir haben 0.6579332246575682 und -0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: -0.22812528\n",
      "\u001b[31mIch lag um -0.04630709 daneben\u001b[0m\n",
      "neues weight1: -0.39978685 und neues bias1: -0.56694150\n",
      "neues weight2: -3.31743409 und neues bias2: -2.47993020\n",
      "Wir haben 0.7054802310718645 und -0.15151515151515138 als Trainings Daten\n",
      "Ich sage y vorraus: -0.18911620\n",
      "\u001b[31mIch lag um -0.03760105 daneben\u001b[0m\n",
      "neues weight1: -0.40070762 und neues bias1: -0.56824667\n",
      "neues weight2: -3.31795339 und neues bias2: -2.47917818\n",
      "Wir haben 0.7564633275546291 und -0.12121212121212109 als Trainings Daten\n",
      "Ich sage y vorraus: -0.14974879\n",
      "\u001b[31mIch lag um -0.02853667 daneben\u001b[0m\n",
      "neues weight1: -0.40143403 und neues bias1: -0.56920694\n",
      "neues weight2: -3.31835408 und neues bias2: -2.47860744\n",
      "Wir haben 0.8111308307896873 und -0.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -0.11007422\n",
      "\u001b[31mIch lag um -0.01916513 daneben\u001b[0m\n",
      "neues weight1: -0.40194012 und neues bias1: -0.56983087\n",
      "neues weight2: -3.31862767 und neues bias2: -2.47822414\n",
      "Wir haben 0.8697490026177834 und -0.06060606060606055 als Trainings Daten\n",
      "Ich sage y vorraus: -0.07015516\n",
      "\u001b[31mIch lag um -0.00954910 daneben\u001b[0m\n",
      "neues weight1: -0.40220112 und neues bias1: -0.57013096\n",
      "neues weight2: -3.31876625 und neues bias2: -2.47803316\n",
      "Wir haben 0.9326033468832199 und -0.030303030303030297 als Trainings Daten\n",
      "Ich sage y vorraus: -0.03006572\n",
      "\u001b[31mIch lag um 0.00023731 daneben\u001b[0m\n",
      "neues weight1: -0.40219442 und neues bias1: -0.57012378\n",
      "neues weight2: -3.31876275 und neues bias2: -2.47803790\n",
      "Wir haben 1.0 und 0.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.01010900\n",
      "\u001b[31mIch lag um 0.01010900 daneben\u001b[0m\n",
      "neues weight1: -0.40190058 und neues bias1: -0.56982994\n",
      "neues weight2: -3.31861117 und neues bias2: -2.47824008\n",
      "Wir haben 1.072267222010323 und 0.030303030303030252 als Trainings Daten\n",
      "Ich sage y vorraus: 0.05027397\n",
      "\u001b[31mIch lag um 0.01997094 daneben\u001b[0m\n",
      "neues weight1: -0.40130438 und neues bias1: -0.56927392\n",
      "neues weight2: -3.31830685 und neues bias2: -2.47863950\n",
      "Wir haben 1.1497569953977356 und 0.060606060606060524 als Trainings Daten\n",
      "Ich sage y vorraus: 0.09032560\n",
      "\u001b[31mIch lag um 0.02971954 daneben\u001b[0m\n",
      "neues weight1: -0.40039581 und neues bias1: -0.56848369\n",
      "neues weight2: -3.31784668 und neues bias2: -2.47923389\n",
      "Wir haben 1.232846739442066 und 0.09090909090909083 als Trainings Daten\n",
      "Ich sage y vorraus: 0.13015373\n",
      "\u001b[31mIch lag um 0.03924464 daneben\u001b[0m\n",
      "neues weight1: -0.39917111 und neues bias1: -0.56749030\n",
      "neues weight2: -3.31722939 und neues bias2: -2.48001879\n",
      "Wir haben 1.3219411484660286 und 0.12121212121212108 als Trainings Daten\n",
      "Ich sage y vorraus: 0.16964382\n",
      "\u001b[31mIch lag um 0.04843170 daneben\u001b[0m\n",
      "neues weight1: -0.39763353 und neues bias1: -0.56632717\n",
      "neues weight2: -3.31645568 und neues bias2: -2.48098742\n",
      "Wir haben 1.4174741629268048 und 0.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: 0.20867937\n",
      "\u001b[31mIch lag um 0.05716422 daneben\u001b[0m\n",
      "neues weight1: -0.39579398 und neues bias1: -0.56502941\n",
      "neues weight2: -3.31552847 und neues bias2: -2.48213070\n",
      "Wir haben 1.5199110829529332 und 0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: 0.24714436\n",
      "\u001b[31mIch lag um 0.06532618 daneben\u001b[0m\n",
      "neues weight1: -0.39367147 und neues bias1: -0.56363294\n",
      "neues weight2: -3.31445297 und neues bias2: -2.48343723\n",
      "Wir haben 1.629750834620645 und 0.21212121212121235 als Trainings Daten\n",
      "Ich sage y vorraus: 0.28492547\n",
      "\u001b[31mIch lag um 0.07280426 daneben\u001b[0m\n",
      "neues weight1: -0.39129317 und neues bias1: -0.56217364\n",
      "neues weight2: -3.31323679 und neues bias2: -2.48489331\n",
      "Wir haben 1.7475284000076847 und 0.24242424242424265 als Trainings Daten\n",
      "Ich sage y vorraus: 0.32191383\n",
      "\u001b[31mIch lag um 0.07948959 daneben\u001b[0m\n",
      "neues weight1: -0.38869430 und neues bias1: -0.56068647\n",
      "neues weight2: -3.31189000 und neues bias2: -2.48648311\n",
      "Wir haben 1.873817422860385 und 0.27272727272727293 als Trainings Daten\n",
      "Ich sage y vorraus: 0.35800621\n",
      "\u001b[31mIch lag um 0.08527894 daneben\u001b[0m\n",
      "neues weight1: -0.38591754 und neues bias1: -0.55920460\n",
      "neues weight2: -3.31042512 und neues bias2: -2.48818868\n",
      "Wir haben 2.0092330025650478 und 0.30303030303030315 als Trainings Daten\n",
      "Ich sage y vorraus: 0.39310539\n",
      "\u001b[31mIch lag um 0.09007509 daneben\u001b[0m\n",
      "neues weight1: -0.38301230 und neues bias1: -0.55775865\n",
      "neues weight2: -3.30885715 und neues bias2: -2.48999019\n",
      "Wir haben 2.1544346900318843 und 0.3333333333333334 als Trainings Daten\n",
      "Ich sage y vorraus: 0.42711974\n",
      "\u001b[31mIch lag um 0.09378641 daneben\u001b[0m\n",
      "neues weight1: -0.38003352 und neues bias1: -0.55637602\n",
      "neues weight2: -3.30720350 und neues bias2: -2.49186591\n",
      "Wir haben 2.31012970008316 und 0.3636363636363637 als Trainings Daten\n",
      "Ich sage y vorraus: 0.45996200\n",
      "\u001b[31mIch lag um 0.09632563 daneben\u001b[0m\n",
      "neues weight1: -0.37704028 und neues bias1: -0.55508032\n",
      "neues weight2: -3.30548400 und neues bias2: -2.49379243\n",
      "Wir haben 2.4770763559917115 und 0.39393939393939403 als Trainings Daten\n",
      "Ich sage y vorraus: 0.49154736\n",
      "\u001b[31mIch lag um 0.09760797 daneben\u001b[0m\n",
      "neues weight1: -0.37409401 und neues bias1: -0.55389091\n",
      "neues weight2: -3.30372091 und neues bias2: -2.49574459\n",
      "Wir haben 2.656087782946687 und 0.42424242424242437 als Trainings Daten\n",
      "Ich sage y vorraus: 0.52179132\n",
      "\u001b[31mIch lag um 0.09754889 daneben\u001b[0m\n",
      "neues weight1: -0.37125647 und neues bias1: -0.55282259\n",
      "neues weight2: -3.30193894 und neues bias2: -2.49769556\n",
      "Wir haben 2.848035868435802 und 0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: 0.55060753\n",
      "\u001b[31mIch lag um 0.09606208 daneben\u001b[0m\n",
      "neues weight1: -0.36858740 und neues bias1: -0.55188543\n",
      "neues weight2: -3.30016527 und neues bias2: -2.49961681\n",
      "Wir haben 3.0538555088334154 und 0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.57790653\n",
      "\u001b[31mIch lag um 0.09305805 daneben\u001b[0m\n",
      "neues weight1: -0.36614191 und neues bias1: -0.55108464\n",
      "neues weight2: -3.29842967 und neues bias2: -2.50147797\n",
      "Wir haben 3.2745491628777286 und 0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: 0.60359592\n",
      "\u001b[31mIch lag um 0.08844441 daneben\u001b[0m\n",
      "neues weight1: -0.36396761 und neues bias1: -0.55042064\n",
      "neues weight2: -3.29676448 und neues bias2: -2.50324685\n",
      "Wir haben 3.511191734215131 und 0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: 0.62758294\n",
      "\u001b[31mIch lag um 0.08212840 daneben\u001b[0m\n",
      "neues weight1: -0.36210176 und neues bias1: -0.54988924\n",
      "neues weight2: -3.29520459 und neues bias2: -2.50488942\n",
      "Wir haben 3.7649358067924674 und 0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: 0.64978001\n",
      "\u001b[31mIch lag um 0.07402244 daneben\u001b[0m\n",
      "neues weight1: -0.36056854 und neues bias1: -0.54948200\n",
      "neues weight2: -3.29378728 und neues bias2: -2.50636987\n",
      "Wir haben 4.037017258596554 und 0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: 0.67011369\n",
      "\u001b[31mIch lag um 0.06405309 daneben\u001b[0m\n",
      "neues weight1: -0.35937683 und neues bias1: -0.54918681\n",
      "neues weight2: -3.29255184 und neues bias2: -2.50765093\n",
      "Wir haben 4.328761281083057 und 0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: 0.68853650\n",
      "\u001b[31mIch lag um 0.05217286 daneben\u001b[0m\n",
      "neues weight1: -0.35851904 und neues bias1: -0.54898865\n",
      "neues weight2: -3.29153892 und neues bias2: -2.50869439\n",
      "Wir haben 4.641588833612782 und 0.666666666666667 als Trainings Daten\n",
      "Ich sage y vorraus: 0.70504034\n",
      "\u001b[31mIch lag um 0.03837368 daneben\u001b[0m\n",
      "neues weight1: -0.35797127 und neues bias1: -0.54887063\n",
      "neues weight2: -3.29078959 und neues bias2: -2.50946186\n",
      "Wir haben 4.9770235643321135 und 0.6969696969696972 als Trainings Daten\n",
      "Ich sage y vorraus: 0.71966909\n",
      "\u001b[31mIch lag um 0.02269939 daneben\u001b[0m\n",
      "neues weight1: -0.35769524 und neues bias1: -0.54881517\n",
      "neues weight2: -3.29034411 und neues bias2: -2.50991585\n",
      "Wir haben 5.336699231206313 und 0.7272727272727275 als Trainings Daten\n",
      "Ich sage y vorraus: 0.73252733\n",
      "\u001b[31mIch lag um 0.00525460 daneben\u001b[0m\n",
      "neues weight1: -0.35764190 und neues bias1: -0.54880518\n",
      "neues weight2: -3.29024054 und neues bias2: -2.51002094\n",
      "Wir haben 5.72236765935022 und 0.7575757575757578 als Trainings Daten\n",
      "Ich sage y vorraus: 0.74378265\n",
      "\u001b[31mIch lag um -0.01379311 daneben\u001b[0m\n",
      "neues weight1: -0.35775630 und neues bias1: -0.54882517\n",
      "neues weight2: -3.29051335 und neues bias2: -2.50974508\n",
      "Wir haben 6.135907273413176 und 0.7878787878787881 als Trainings Daten\n",
      "Ich sage y vorraus: 0.75365972\n",
      "\u001b[31mIch lag um -0.03421906 daneben\u001b[0m\n",
      "neues weight1: -0.35798304 und neues bias1: -0.54886212\n",
      "neues weight2: -3.29119209 und neues bias2: -2.50906070\n",
      "Wir haben 6.5793322465756825 und 0.8181818181818183 als Trainings Daten\n",
      "Ich sage y vorraus: 0.76242681\n",
      "\u001b[31mIch lag um -0.05575501 daneben\u001b[0m\n",
      "neues weight1: -0.35827130 und neues bias1: -0.54890594\n",
      "neues weight2: -3.29230052 und neues bias2: -2.50794560\n",
      "Wir haben 7.054802310718645 und 0.8484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.77037690\n",
      "\u001b[31mIch lag um -0.07810794 daneben\u001b[0m\n",
      "neues weight1: -0.35857875 und neues bias1: -0.54894952\n",
      "neues weight2: -3.29385604 und neues bias2: -2.50638344\n",
      "Wir haben 7.56463327554629 und 0.8787878787878789 als Trainings Daten\n",
      "Ich sage y vorraus: 0.77780712\n",
      "\u001b[31mIch lag um -0.10098076 daneben\u001b[0m\n",
      "neues weight1: -0.35887364 und neues bias1: -0.54898850\n",
      "neues weight2: -3.29586973 und neues bias2: -2.50436383\n",
      "Wir haben 8.111308307896872 und 0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: 0.78499982\n",
      "\u001b[31mIch lag um -0.12409109 daneben\u001b[0m\n",
      "neues weight1: -0.35913533 und neues bias1: -0.54902076\n",
      "neues weight2: -3.29834666 und neues bias2: -2.50188200\n",
      "Wir haben 8.697490026177835 und 0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79220799\n",
      "\u001b[31mIch lag um -0.14718595 daneben\u001b[0m\n",
      "neues weight1: -0.35935316 und neues bias1: -0.54904581\n",
      "neues weight2: -3.30128658 und neues bias2: -2.49893829\n",
      "Wir haben 9.326033468832199 und 0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79964618\n",
      "\u001b[31mIch lag um -0.17005079 daneben\u001b[0m\n",
      "neues weight1: -0.35952450 und neues bias1: -0.54906418\n",
      "neues weight2: -3.30468481 und neues bias2: -2.49553727\n",
      "Wir haben 10.0 und 1.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.80748661\n",
      "\u001b[31mIch lag um -0.19251339 daneben\u001b[0m\n",
      "neues weight1: -0.35965237 und neues bias1: -0.54907697\n",
      "neues weight2: -3.30853314 und neues bias2: -2.49168700\n",
      "\n",
      "\n",
      "Ich trainiere in Epoche 9001\n",
      "Wir haben 0.01 und -2.0 als Trainings Daten\n",
      "Ich sage y vorraus: -0.82892580\n",
      "\u001b[31mIch lag um 1.17107420 daneben\u001b[0m\n",
      "neues weight1: -0.35886651 und neues bias1: -0.49207145\n",
      "neues weight2: -3.30128769 und neues bias2: -2.51956248\n",
      "Wir haben 0.010722672220103232 und -1.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.00459502\n",
      "\u001b[31mIch lag um 0.96510195 daneben\u001b[0m\n",
      "neues weight1: -0.35832714 und neues bias1: -0.44176906\n",
      "neues weight2: -3.29242994 und neues bias2: -2.53886452\n",
      "Wir haben 0.011497569953977356 und -1.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.16093693\n",
      "\u001b[31mIch lag um 0.77845701 daneben\u001b[0m\n",
      "neues weight1: -0.35784100 und neues bias1: -0.39948720\n",
      "neues weight2: -3.28591404 und neues bias2: -2.55443366\n",
      "Wir haben 0.012328467394420659 und -1.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.29500852\n",
      "\u001b[31mIch lag um 0.61408239 daneben\u001b[0m\n",
      "neues weight1: -0.35741655 und neues bias1: -0.36505926\n",
      "neues weight2: -3.28120673 und neues bias2: -2.56671530\n",
      "Wir haben 0.013219411484660288 und -1.878787878787879 als Trainings Daten\n",
      "Ich sage y vorraus: -1.40581500\n",
      "\u001b[31mIch lag um 0.47297288 daneben\u001b[0m\n",
      "neues weight1: -0.35705761 und neues bias1: -0.33790611\n",
      "neues weight2: -3.27785995 und neues bias2: -2.57617476\n",
      "Wir haben 0.014174741629268055 und -1.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -1.49407483\n",
      "\u001b[31mIch lag um 0.35441002 daneben\u001b[0m\n",
      "neues weight1: -0.35676416 und neues bias1: -0.31720408\n",
      "neues weight2: -3.27551996 und neues bias2: -2.58326296\n",
      "Wir haben 0.01519911082952934 und -1.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -1.56169395\n",
      "\u001b[31mIch lag um 0.25648786 daneben\u001b[0m\n",
      "neues weight1: -0.35653362 und neues bias1: -0.30203583\n",
      "neues weight2: -3.27392009 und neues bias2: -2.58839272\n",
      "Wir haben 0.016297508346206444 und -1.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -1.61120399\n",
      "\u001b[31mIch lag um 0.17667480 daneben\u001b[0m\n",
      "neues weight1: -0.35636188 und neues bias1: -0.29149806\n",
      "neues weight2: -3.27286543 und neues bias2: -2.59192621\n",
      "Wir haben 0.01747528400007684 und -1.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -1.64531590\n",
      "\u001b[31mIch lag um 0.11225986 daneben\u001b[0m\n",
      "neues weight1: -0.35624421 und neues bias1: -0.28476454\n",
      "neues weight2: -3.27221605 und neues bias2: -2.59417141\n",
      "Wir haben 0.01873817422860384 und -1.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -1.66663049\n",
      "\u001b[31mIch lag um 0.06064224 daneben\u001b[0m\n",
      "neues weight1: -0.35617582 und neues bias1: -0.28111473\n",
      "neues weight2: -3.27187226 und neues bias2: -2.59538426\n",
      "Wir haben 0.02009233002565047 und -1.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67748410\n",
      "\u001b[31mIch lag um 0.01948560 daneben\u001b[0m\n",
      "neues weight1: -0.35615221 und neues bias1: -0.27994000\n",
      "neues weight2: -3.27176293 und neues bias2: -2.59577397\n",
      "Wir haben 0.021544346900318846 und -1.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67988844\n",
      "\u001b[31mIch lag um -0.01322177 daneben\u001b[0m\n",
      "neues weight1: -0.35616939 und neues bias1: -0.28073737\n",
      "neues weight2: -3.27183695 und neues bias2: -2.59550953\n",
      "Wir haben 0.023101297000831605 und -1.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -1.67552711\n",
      "\u001b[31mIch lag um -0.03916347 daneben\u001b[0m\n",
      "neues weight1: -0.35622391 und neues bias1: -0.28309748\n",
      "neues weight2: -3.27205719 und neues bias2: -2.59472626\n",
      "Wir haben 0.024770763559917114 und -1.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.66578162\n",
      "\u001b[31mIch lag um -0.05972102 daneben\u001b[0m\n",
      "neues weight1: -0.35631292 und neues bias1: -0.28669069\n",
      "neues weight2: -3.27239629 und neues bias2: -2.59353184\n",
      "Wir haben 0.026560877829466867 und -1.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -1.65176996\n",
      "\u001b[31mIch lag um -0.07601239 daneben\u001b[0m\n",
      "neues weight1: -0.35643411 und neues bias1: -0.29125351\n",
      "neues weight2: -3.27283380 und neues bias2: -2.59201160\n",
      "Wir haben 0.02848035868435802 und -1.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -1.63438782\n",
      "\u001b[31mIch lag um -0.08893327 daneben\u001b[0m\n",
      "neues weight1: -0.35658571 und neues bias1: -0.29657640\n",
      "neues weight2: -3.27335424 und neues bias2: -2.59023293\n",
      "Wir haben 0.030538555088334154 und -1.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -1.61434755\n",
      "\u001b[31mIch lag um -0.09919604 daneben\u001b[0m\n",
      "neues weight1: -0.35676640 und neues bias1: -0.30249328\n",
      "neues weight2: -3.27394570 und neues bias2: -2.58824901\n",
      "Wir haben 0.03274549162877728 und -1.4848484848484849 als Trainings Daten\n",
      "Ich sage y vorraus: -1.59221269\n",
      "\u001b[31mIch lag um -0.10736420 daneben\u001b[0m\n",
      "neues weight1: -0.35697530 und neues bias1: -0.30887269\n",
      "neues weight2: -3.27459897 und neues bias2: -2.58610173\n",
      "Wir haben 0.03511191734215131 und -1.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -1.56842717\n",
      "\u001b[31mIch lag um -0.11388171 daneben\u001b[0m\n",
      "neues weight1: -0.35721188 und neues bias1: -0.31561067\n",
      "neues weight2: -3.27530681 und neues bias2: -2.58382409\n",
      "Wir haben 0.037649358067924674 und -1.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -1.54333963\n",
      "\u001b[31mIch lag um -0.11909721 daneben\u001b[0m\n",
      "neues weight1: -0.35747597 und neues bias1: -0.32262495\n",
      "neues weight2: -3.27606350 und neues bias2: -2.58144215\n",
      "Wir haben 0.040370172585965536 und -1.393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.51722317\n",
      "\u001b[31mIch lag um -0.12328378 daneben\u001b[0m\n",
      "neues weight1: -0.35776765 und neues bias1: -0.32985026\n",
      "neues weight2: -3.27686447 und neues bias2: -2.57897647\n",
      "Wir haben 0.04328761281083057 und -1.3636363636363638 als Trainings Daten\n",
      "Ich sage y vorraus: -1.49029122\n",
      "\u001b[31mIch lag um -0.12665485 daneben\u001b[0m\n",
      "neues weight1: -0.35808731 und neues bias1: -0.33723466\n",
      "neues weight2: -3.27770605 und neues bias2: -2.57644337\n",
      "Wir haben 0.046415888336127795 und -1.3333333333333333 als Trainings Daten\n",
      "Ich sage y vorraus: -1.46271019\n",
      "\u001b[31mIch lag um -0.12937686 daneben\u001b[0m\n",
      "neues weight1: -0.35843552 und neues bias1: -0.34473663\n",
      "neues weight2: -3.27858527 und neues bias2: -2.57385584\n",
      "Wir haben 0.049770235643321115 und -1.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.43460954\n",
      "\u001b[31mIch lag um -0.13157923 daneben\u001b[0m\n",
      "neues weight1: -0.35881308 und neues bias1: -0.35232275\n",
      "neues weight2: -3.27949969 und neues bias2: -2.57122425\n",
      "Wir haben 0.0533669923120631 und -1.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -1.40608962\n",
      "\u001b[31mIch lag um -0.13336235 daneben\u001b[0m\n",
      "neues weight1: -0.35922097 und neues bias1: -0.35996588\n",
      "neues weight2: -3.28044731 und neues bias2: -2.56855701\n",
      "Wir haben 0.05722367659350217 und -1.2424242424242424 als Trainings Daten\n",
      "Ich sage y vorraus: -1.37722793\n",
      "\u001b[31mIch lag um -0.13480369 daneben\u001b[0m\n",
      "neues weight1: -0.35966033 und neues bias1: -0.36764377\n",
      "neues weight2: -3.28142642 und neues bias2: -2.56586093\n",
      "Wir haben 0.06135907273413173 und -1.2121212121212122 als Trainings Daten\n",
      "Ich sage y vorraus: -1.34808393\n",
      "\u001b[31mIch lag um -0.13596272 daneben\u001b[0m\n",
      "neues weight1: -0.36013243 und neues bias1: -0.37533789\n",
      "neues weight2: -3.28243556 und neues bias2: -2.56314168\n",
      "Wir haben 0.06579332246575682 und -1.1818181818181817 als Trainings Daten\n",
      "Ich sage y vorraus: -1.31870285\n",
      "\u001b[31mIch lag um -0.13688467 daneben\u001b[0m\n",
      "neues weight1: -0.36063869 und neues bias1: -0.38303257\n",
      "neues weight2: -3.28347348 und neues bias2: -2.56040398\n",
      "Wir haben 0.07054802310718646 und -1.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: -1.28911865\n",
      "\u001b[31mIch lag um -0.13760349 daneben\u001b[0m\n",
      "neues weight1: -0.36118062 und neues bias1: -0.39071431\n",
      "neues weight2: -3.28453902 und neues bias2: -2.55765191\n",
      "Wir haben 0.07564633275546291 und -1.121212121212121 als Trainings Daten\n",
      "Ich sage y vorraus: -1.25935631\n",
      "\u001b[31mIch lag um -0.13814418 daneben\u001b[0m\n",
      "neues weight1: -0.36175984 und neues bias1: -0.39837124\n",
      "neues weight2: -3.28563112 und neues bias2: -2.55488903\n",
      "Wir haben 0.08111308307896872 und -1.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -1.22943363\n",
      "\u001b[31mIch lag um -0.13852454 daneben\u001b[0m\n",
      "neues weight1: -0.36237804 und neues bias1: -0.40599267\n",
      "neues weight2: -3.28674876 und neues bias2: -2.55211854\n",
      "Wir haben 0.08697490026177834 und -1.0606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -1.19936261\n",
      "\u001b[31mIch lag um -0.13875655 daneben\u001b[0m\n",
      "neues weight1: -0.36303697 und neues bias1: -0.41356873\n",
      "neues weight2: -3.28789094 und neues bias2: -2.54934341\n",
      "Wir haben 0.093260334688322 und -1.0303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -1.16915053\n",
      "\u001b[31mIch lag um -0.13884750 daneben\u001b[0m\n",
      "neues weight1: -0.36373841 und neues bias1: -0.42109013\n",
      "neues weight2: -3.28905665 und neues bias2: -2.54656646\n",
      "Wir haben 0.1 und -1.0 als Trainings Daten\n",
      "Ich sage y vorraus: -1.13880078\n",
      "\u001b[31mIch lag um -0.13880078 daneben\u001b[0m\n",
      "neues weight1: -0.36448419 und neues bias1: -0.42854793\n",
      "neues weight2: -3.29024483 und neues bias2: -2.54379044\n",
      "Wir haben 0.10722672220103231 und -0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -1.10831349\n",
      "\u001b[31mIch lag um -0.13861652 daneben\u001b[0m\n",
      "neues weight1: -0.36527611 und neues bias1: -0.43593334\n",
      "neues weight2: -3.29145435 und neues bias2: -2.54101811\n",
      "Wir haben 0.11497569953977356 und -0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: -1.07768602\n",
      "\u001b[31mIch lag um -0.13829208 daneben\u001b[0m\n",
      "neues weight1: -0.36611592 und neues bias1: -0.44323759\n",
      "neues weight2: -3.29268400 und neues bias2: -2.53825227\n",
      "Wir haben 0.12328467394420659 und -0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: -1.04691336\n",
      "\u001b[31mIch lag um -0.13782245 daneben\u001b[0m\n",
      "neues weight1: -0.36700532 und neues bias1: -0.45045182\n",
      "neues weight2: -3.29393247 und neues bias2: -2.53549582\n",
      "Wir haben 0.13219411484660293 und -0.8787878787878787 als Trainings Daten\n",
      "Ich sage y vorraus: -1.01598840\n",
      "\u001b[31mIch lag um -0.13720052 daneben\u001b[0m\n",
      "neues weight1: -0.36794590 und neues bias1: -0.45756698\n",
      "neues weight2: -3.29519829 und neues bias2: -2.53275181\n",
      "Wir haben 0.14174741629268056 und -0.8484848484848484 als Trainings Daten\n",
      "Ich sage y vorraus: -0.98490212\n",
      "\u001b[31mIch lag um -0.13641727 daneben\u001b[0m\n",
      "neues weight1: -0.36893909 und neues bias1: -0.46457372\n",
      "neues weight2: -3.29647987 und neues bias2: -2.53002347\n",
      "Wir haben 0.1519911082952934 und -0.8181818181818181 als Trainings Daten\n",
      "Ich sage y vorraus: -0.95364384\n",
      "\u001b[31mIch lag um -0.13546202 daneben\u001b[0m\n",
      "neues weight1: -0.36998611 und neues bias1: -0.47146238\n",
      "neues weight2: -3.29777544 und neues bias2: -2.52731423\n",
      "Wir haben 0.16297508346206444 und -0.7878787878787878 als Trainings Daten\n",
      "Ich sage y vorraus: -0.92220125\n",
      "\u001b[31mIch lag um -0.13432247 daneben\u001b[0m\n",
      "neues weight1: -0.37108790 und neues bias1: -0.47822290\n",
      "neues weight2: -3.29908300 und neues bias2: -2.52462778\n",
      "Wir haben 0.17475284000076838 und -0.7575757575757576 als Trainings Daten\n",
      "Ich sage y vorraus: -0.89056062\n",
      "\u001b[31mIch lag um -0.13298486 daneben\u001b[0m\n",
      "neues weight1: -0.37224510 und neues bias1: -0.48484478\n",
      "neues weight2: -3.30040037 und neues bias2: -2.52196808\n",
      "Wir haben 0.1873817422860384 und -0.7272727272727273 als Trainings Daten\n",
      "Ich sage y vorraus: -0.85870681\n",
      "\u001b[31mIch lag um -0.13143408 daneben\u001b[0m\n",
      "neues weight1: -0.37345789 und neues bias1: -0.49131709\n",
      "neues weight2: -3.30172512 und neues bias2: -2.51933940\n",
      "Wir haben 0.20092330025650468 und -0.696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: -0.82662342\n",
      "\u001b[31mIch lag um -0.12965373 daneben\u001b[0m\n",
      "neues weight1: -0.37472598 und neues bias1: -0.49762840\n",
      "neues weight2: -3.30305453 und neues bias2: -2.51674632\n",
      "Wir haben 0.21544346900318845 und -0.6666666666666665 als Trainings Daten\n",
      "Ich sage y vorraus: -0.79429289\n",
      "\u001b[31mIch lag um -0.12762622 daneben\u001b[0m\n",
      "neues weight1: -0.37604846 und neues bias1: -0.50376682\n",
      "neues weight2: -3.30438560 und neues bias2: -2.51419380\n",
      "Wir haben 0.23101297000831605 und -0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: -0.76169655\n",
      "\u001b[31mIch lag um -0.12533292 daneben\u001b[0m\n",
      "neues weight1: -0.37742372 und neues bias1: -0.50971998\n",
      "neues weight2: -3.30571502 und neues bias2: -2.51168714\n",
      "Wir haben 0.24770763559917114 und -0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: -0.72881487\n",
      "\u001b[31mIch lag um -0.12275426 daneben\u001b[0m\n",
      "neues weight1: -0.37884930 und neues bias1: -0.51547509\n",
      "neues weight2: -3.30703912 und neues bias2: -2.50923206\n",
      "Wir haben 0.26560877829466867 und -0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: -0.69562754\n",
      "\u001b[31mIch lag um -0.11986996 daneben\u001b[0m\n",
      "neues weight1: -0.38032180 und neues bias1: -0.52101894\n",
      "neues weight2: -3.30835387 und neues bias2: -2.50683466\n",
      "Wir haben 0.2848035868435802 und -0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: -0.66211375\n",
      "\u001b[31mIch lag um -0.11665920 daneben\u001b[0m\n",
      "neues weight1: -0.38183669 und neues bias1: -0.52633801\n",
      "neues weight2: -3.30965484 und neues bias2: -2.50450147\n",
      "Wir haben 0.30538555088334157 und -0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: -0.62825246\n",
      "\u001b[31mIch lag um -0.11310095 daneben\u001b[0m\n",
      "neues weight1: -0.38338820 und neues bias1: -0.53141852\n",
      "neues weight2: -3.31093718 und neues bias2: -2.50223945\n",
      "Wir haben 0.32745491628777285 und -0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: -0.59402276\n",
      "\u001b[31mIch lag um -0.10917427 daneben\u001b[0m\n",
      "neues weight1: -0.38496917 und neues bias1: -0.53624656\n",
      "neues weight2: -3.31219561 und neues bias2: -2.50005597\n",
      "Wir haben 0.3511191734215131 und -0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: -0.55940428\n",
      "\u001b[31mIch lag um -0.10485882 daneben\u001b[0m\n",
      "neues weight1: -0.38657085 und neues bias1: -0.54080822\n",
      "neues weight2: -3.31342437 und neues bias2: -2.49795879\n",
      "Wir haben 0.37649358067924676 und -0.4242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.52437773\n",
      "\u001b[31mIch lag um -0.10013531 daneben\u001b[0m\n",
      "neues weight1: -0.38818284 und neues bias1: -0.54508980\n",
      "neues weight2: -3.31461724 und neues bias2: -2.49595609\n",
      "Wir haben 0.4037017258596556 und -0.3939393939393938 als Trainings Daten\n",
      "Ich sage y vorraus: -0.48892554\n",
      "\u001b[31mIch lag um -0.09498615 daneben\u001b[0m\n",
      "neues weight1: -0.38979287 und neues bias1: -0.54907797\n",
      "neues weight2: -3.31576754 und neues bias2: -2.49405636\n",
      "Wir haben 0.43287612810830595 und -0.36363636363636354 als Trainings Daten\n",
      "Ich sage y vorraus: -0.45303256\n",
      "\u001b[31mIch lag um -0.08939620 daneben\u001b[0m\n",
      "neues weight1: -0.39138675 und neues bias1: -0.55276004\n",
      "neues weight2: -3.31686810 und neues bias2: -2.49226844\n",
      "Wir haben 0.464158883361278 und -0.33333333333333326 als Trainings Daten\n",
      "Ich sage y vorraus: -0.41668690\n",
      "\u001b[31mIch lag um -0.08335357 daneben\u001b[0m\n",
      "neues weight1: -0.39294829 und neues bias1: -0.55612426\n",
      "neues weight2: -3.31791129 und neues bias2: -2.49060137\n",
      "Wir haben 0.49770235643321115 und -0.303030303030303 als Trainings Daten\n",
      "Ich sage y vorraus: -0.37988090\n",
      "\u001b[31mIch lag um -0.07685059 daneben\u001b[0m\n",
      "neues weight1: -0.39445923 und neues bias1: -0.55916010\n",
      "neues weight2: -3.31888908 und neues bias2: -2.48906436\n",
      "Wir haben 0.533669923120631 und -0.2727272727272727 als Trainings Daten\n",
      "Ich sage y vorraus: -0.34261208\n",
      "\u001b[31mIch lag um -0.06988481 daneben\u001b[0m\n",
      "neues weight1: -0.39589935 und neues bias1: -0.56185863\n",
      "neues weight2: -3.31979302 und neues bias2: -2.48766666\n",
      "Wir haben 0.5722367659350217 und -0.24242424242424243 als Trainings Daten\n",
      "Ich sage y vorraus: -0.30488435\n",
      "\u001b[31mIch lag um -0.06246011 daneben\u001b[0m\n",
      "neues weight1: -0.39724654 und neues bias1: -0.56421288\n",
      "neues weight2: -3.32061438 und neues bias2: -2.48641746\n",
      "Wir haben 0.6135907273413173 und -0.21212121212121215 als Trainings Daten\n",
      "Ich sage y vorraus: -0.26670902\n",
      "\u001b[31mIch lag um -0.05458781 daneben\u001b[0m\n",
      "neues weight1: -0.39847701 und neues bias1: -0.56621824\n",
      "neues weight2: -3.32134418 und neues bias2: -2.48532570\n",
      "Wir haben 0.6579332246575682 und -0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: -0.22810600\n",
      "\u001b[31mIch lag um -0.04628781 daneben\u001b[0m\n",
      "neues weight1: -0.39956564 und neues bias1: -0.56787285\n",
      "neues weight2: -3.32197333 und neues bias2: -2.48439994\n",
      "Wir haben 0.7054802310718645 und -0.15151515151515138 als Trainings Daten\n",
      "Ich sage y vorraus: -0.18910476\n",
      "\u001b[31mIch lag um -0.03758961 daneben\u001b[0m\n",
      "neues weight1: -0.40048640 und neues bias1: -0.56917801\n",
      "neues weight2: -3.32249278 und neues bias2: -2.48364815\n",
      "Wir haben 0.7564633275546291 und -0.12121212121212109 als Trainings Daten\n",
      "Ich sage y vorraus: -0.14974529\n",
      "\u001b[31mIch lag um -0.02853316 daneben\u001b[0m\n",
      "neues weight1: -0.40121294 und neues bias1: -0.57013845\n",
      "neues weight2: -3.32289364 und neues bias2: -2.48307749\n",
      "Wir haben 0.8111308307896873 und -0.0909090909090908 als Trainings Daten\n",
      "Ich sage y vorraus: -0.11007870\n",
      "\u001b[31mIch lag um -0.01916961 daneben\u001b[0m\n",
      "neues weight1: -0.40171929 und neues bias1: -0.57076271\n",
      "neues weight2: -3.32316744 und neues bias2: -2.48269410\n",
      "Wir haben 0.8697490026177834 und -0.06060606060606055 als Trainings Daten\n",
      "Ich sage y vorraus: -0.07016761\n",
      "\u001b[31mIch lag um -0.00956155 daneben\u001b[0m\n",
      "neues weight1: -0.40198071 und neues bias1: -0.57106327\n",
      "neues weight2: -3.32330627 und neues bias2: -2.48250287\n",
      "Wir haben 0.9326033468832199 und -0.030303030303030297 als Trainings Daten\n",
      "Ich sage y vorraus: -0.03008607\n",
      "\u001b[31mIch lag um 0.00021696 daneben\u001b[0m\n",
      "neues weight1: -0.40197458 und neues bias1: -0.57105671\n",
      "neues weight2: -3.32330306 und neues bias2: -2.48250721\n",
      "Wir haben 1.0 und 0.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.01008090\n",
      "\u001b[31mIch lag um 0.01008090 daneben\u001b[0m\n",
      "neues weight1: -0.40168148 und neues bias1: -0.57076360\n",
      "neues weight2: -3.32315184 und neues bias2: -2.48270882\n",
      "Wir haben 1.072267222010323 und 0.030303030303030252 als Trainings Daten\n",
      "Ich sage y vorraus: 0.05023834\n",
      "\u001b[31mIch lag um 0.01993531 daneben\u001b[0m\n",
      "neues weight1: -0.40108615 und neues bias1: -0.57020840\n",
      "neues weight2: -3.32284795 und neues bias2: -2.48310753\n",
      "Wir haben 1.1497569953977356 und 0.060606060606060524 als Trainings Daten\n",
      "Ich sage y vorraus: 0.09028276\n",
      "\u001b[31mIch lag um 0.02967669 daneben\u001b[0m\n",
      "neues weight1: -0.40017862 und neues bias1: -0.56941907\n",
      "neues weight2: -3.32238828 und neues bias2: -2.48370106\n",
      "Wir haben 1.232846739442066 und 0.09090909090909083 als Trainings Daten\n",
      "Ich sage y vorraus: 0.13010406\n",
      "\u001b[31mIch lag um 0.03919497 daneben\u001b[0m\n",
      "neues weight1: -0.39895508 und neues bias1: -0.56842662\n",
      "neues weight2: -3.32177157 und neues bias2: -2.48448496\n",
      "Wir haben 1.3219411484660286 und 0.12121212121212108 als Trainings Daten\n",
      "Ich sage y vorraus: 0.16958781\n",
      "\u001b[31mIch lag um 0.04837569 daneben\u001b[0m\n",
      "neues weight1: -0.39741877 und neues bias1: -0.56726446\n",
      "neues weight2: -3.32099853 und neues bias2: -2.48545248\n",
      "Wir haben 1.4174741629268048 und 0.1515151515151514 als Trainings Daten\n",
      "Ich sage y vorraus: 0.20861758\n",
      "\u001b[31mIch lag um 0.05710243 daneben\u001b[0m\n",
      "neues weight1: -0.39558058 und neues bias1: -0.56596765\n",
      "neues weight2: -3.32007207 und neues bias2: -2.48659453\n",
      "Wir haben 1.5199110829529332 und 0.18181818181818168 als Trainings Daten\n",
      "Ich sage y vorraus: 0.24707746\n",
      "\u001b[31mIch lag um 0.06525928 daneben\u001b[0m\n",
      "neues weight1: -0.39345948 und neues bias1: -0.56457211\n",
      "neues weight2: -3.31899741 und neues bias2: -2.48789971\n",
      "Wir haben 1.629750834620645 und 0.21212121212121235 als Trainings Daten\n",
      "Ich sage y vorraus: 0.28485421\n",
      "\u001b[31mIch lag um 0.07273299 daneben\u001b[0m\n",
      "neues weight1: -0.39108262 und neues bias1: -0.56311369\n",
      "neues weight2: -3.31778216 und neues bias2: -2.48935437\n",
      "Wir haben 1.7475284000076847 und 0.24242424242424265 als Trainings Daten\n",
      "Ich sage y vorraus: 0.32183903\n",
      "\u001b[31mIch lag um 0.07941479 daneben\u001b[0m\n",
      "neues weight1: -0.38848514 und neues bias1: -0.56162732\n",
      "neues weight2: -3.31643638 und neues bias2: -2.49094267\n",
      "Wir haben 1.873817422860385 und 0.27272727272727293 als Trainings Daten\n",
      "Ich sage y vorraus: 0.35792878\n",
      "\u001b[31mIch lag um 0.08520150 daneben\u001b[0m\n",
      "neues weight1: -0.38570972 und neues bias1: -0.56014616\n",
      "neues weight2: -3.31497259 und neues bias2: -2.49264670\n",
      "Wir haben 2.0092330025650478 und 0.30303030303030315 als Trainings Daten\n",
      "Ich sage y vorraus: 0.39302632\n",
      "\u001b[31mIch lag um 0.08999602 daneben\u001b[0m\n",
      "neues weight1: -0.38280569 und neues bias1: -0.55870081\n",
      "neues weight2: -3.31340577 und neues bias2: -2.49444662\n",
      "Wir haben 2.1544346900318843 und 0.3333333333333334 als Trainings Daten\n",
      "Ich sage y vorraus: 0.42704012\n",
      "\u001b[31mIch lag um 0.09370678 daneben\u001b[0m\n",
      "neues weight1: -0.37982796 und neues bias1: -0.55731868\n",
      "neues weight2: -3.31175331 und neues bias2: -2.49632075\n",
      "Wir haben 2.31012970008316 und 0.3636363636363637 als Trainings Daten\n",
      "Ich sage y vorraus: 0.45988298\n",
      "\u001b[31mIch lag um 0.09624662 daneben\u001b[0m\n",
      "neues weight1: -0.37683556 und neues bias1: -0.55602334\n",
      "neues weight2: -3.31003504 und neues bias2: -2.49824568\n",
      "Wir haben 2.4770763559917115 und 0.39393939393939403 als Trainings Daten\n",
      "Ich sage y vorraus: 0.49147022\n",
      "\u001b[31mIch lag um 0.09753082 daneben\u001b[0m\n",
      "neues weight1: -0.37388988 und neues bias1: -0.55483416\n",
      "neues weight2: -3.30827319 und neues bias2: -2.50019630\n",
      "Wir haben 2.656087782946687 und 0.42424242424242437 als Trainings Daten\n",
      "Ich sage y vorraus: 0.52171739\n",
      "\u001b[31mIch lag um 0.09747496 daneben\u001b[0m\n",
      "neues weight1: -0.37105267 und neues bias1: -0.55376597\n",
      "neues weight2: -3.30649244 und neues bias2: -2.50214580\n",
      "Wir haben 2.848035868435802 und 0.4545454545454546 als Trainings Daten\n",
      "Ich sage y vorraus: 0.55053824\n",
      "\u001b[31mIch lag um 0.09599279 daneben\u001b[0m\n",
      "neues weight1: -0.36838364 und neues bias1: -0.55282882\n",
      "neues weight2: -3.30471995 und neues bias2: -2.50406566\n",
      "Wir haben 3.0538555088334154 und 0.48484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.57784339\n",
      "\u001b[31mIch lag um 0.09299491 daneben\u001b[0m\n",
      "neues weight1: -0.36593789 und neues bias1: -0.55202795\n",
      "neues weight2: -3.30298545 und neues bias2: -2.50592555\n",
      "Wir haben 3.2745491628777286 und 0.5151515151515151 als Trainings Daten\n",
      "Ich sage y vorraus: 0.60354049\n",
      "\u001b[31mIch lag um 0.08838898 daneben\u001b[0m\n",
      "neues weight1: -0.36376308 und neues bias1: -0.55136379\n",
      "neues weight2: -3.30132124 und neues bias2: -2.50769333\n",
      "Wir haben 3.511191734215131 und 0.5454545454545454 als Trainings Daten\n",
      "Ich sage y vorraus: 0.62753676\n",
      "\u001b[31mIch lag um 0.08208221 daneben\u001b[0m\n",
      "neues weight1: -0.36189651 und neues bias1: -0.55083219\n",
      "neues weight2: -3.29976219 und neues bias2: -2.50933498\n",
      "Wir haben 3.7649358067924674 und 0.5757575757575757 als Trainings Daten\n",
      "Ich sage y vorraus: 0.64974456\n",
      "\u001b[31mIch lag um 0.07398698 daneben\u001b[0m\n",
      "neues weight1: -0.36036240 und neues bias1: -0.55042471\n",
      "neues weight2: -3.29834554 und neues bias2: -2.51081472\n",
      "Wir haben 4.037017258596554 und 0.606060606060606 als Trainings Daten\n",
      "Ich sage y vorraus: 0.67009027\n",
      "\u001b[31mIch lag um 0.06402966 daneben\u001b[0m\n",
      "neues weight1: -0.35916973 und neues bias1: -0.55012928\n",
      "neues weight2: -3.29711054 und neues bias2: -2.51209531\n",
      "Wir haben 4.328761281083057 und 0.6363636363636362 als Trainings Daten\n",
      "Ich sage y vorraus: 0.68852615\n",
      "\u001b[31mIch lag um 0.05216252 daneben\u001b[0m\n",
      "neues weight1: -0.35831100 und neues bias1: -0.54993090\n",
      "neues weight2: -3.29609782 und neues bias2: -2.51313856\n",
      "Wir haben 4.641588833612782 und 0.666666666666667 als Trainings Daten\n",
      "Ich sage y vorraus: 0.70504371\n",
      "\u001b[31mIch lag um 0.03837705 daneben\u001b[0m\n",
      "neues weight1: -0.35776240 und neues bias1: -0.54981271\n",
      "neues weight2: -3.29534842 und neues bias2: -2.51390610\n",
      "Wir haben 4.9770235643321135 und 0.6969696969696972 als Trainings Daten\n",
      "Ich sage y vorraus: 0.71968634\n",
      "\u001b[31mIch lag um 0.02271664 daneben\u001b[0m\n",
      "neues weight1: -0.35748573 und neues bias1: -0.54975712\n",
      "neues weight2: -3.29490261 und neues bias2: -2.51436044\n",
      "Wir haben 5.336699231206313 und 0.7272727272727275 als Trainings Daten\n",
      "Ich sage y vorraus: 0.73255810\n",
      "\u001b[31mIch lag um 0.00528538 daneben\u001b[0m\n",
      "neues weight1: -0.35743198 und neues bias1: -0.54974705\n",
      "neues weight2: -3.29479844 und neues bias2: -2.51446614\n",
      "Wir haben 5.72236765935022 und 0.7575757575757578 als Trainings Daten\n",
      "Ich sage y vorraus: 0.74382604\n",
      "\u001b[31mIch lag um -0.01374971 daneben\u001b[0m\n",
      "neues weight1: -0.35754624 und neues bias1: -0.54976702\n",
      "neues weight2: -3.29507039 und neues bias2: -2.51419115\n",
      "Wir haben 6.135907273413176 und 0.7878787878787881 als Trainings Daten\n",
      "Ich sage y vorraus: 0.75371438\n",
      "\u001b[31mIch lag um -0.03416441 daneben\u001b[0m\n",
      "neues weight1: -0.35777308 und neues bias1: -0.54980399\n",
      "neues weight2: -3.29574804 und neues bias2: -2.51350786\n",
      "Wir haben 6.5793322465756825 und 0.8181818181818183 als Trainings Daten\n",
      "Ich sage y vorraus: 0.76249102\n",
      "\u001b[31mIch lag um -0.05569080 daneben\u001b[0m\n",
      "neues weight1: -0.35806166 und neues bias1: -0.54984785\n",
      "neues weight2: -3.29685518 und neues bias2: -2.51239404\n",
      "Wir haben 7.054802310718645 und 0.8484848484848486 als Trainings Daten\n",
      "Ich sage y vorraus: 0.77044878\n",
      "\u001b[31mIch lag um -0.07803607 daneben\u001b[0m\n",
      "neues weight1: -0.35836958 und neues bias1: -0.54989149\n",
      "neues weight2: -3.29840927 und neues bias2: -2.51083332\n",
      "Wir haben 7.56463327554629 und 0.8787878787878789 als Trainings Daten\n",
      "Ich sage y vorraus: 0.77788472\n",
      "\u001b[31mIch lag um -0.10090316 daneben\u001b[0m\n",
      "neues weight1: -0.35866503 und neues bias1: -0.54993055\n",
      "neues weight2: -3.30042140 und neues bias2: -2.50881526\n",
      "Wir haben 8.111308307896872 und 0.9090909090909092 als Trainings Daten\n",
      "Ich sage y vorraus: 0.78508130\n",
      "\u001b[31mIch lag um -0.12400961 daneben\u001b[0m\n",
      "neues weight1: -0.35892730 und neues bias1: -0.54996289\n",
      "neues weight2: -3.30289669 und neues bias2: -2.50633507\n",
      "Wir haben 8.697490026177835 und 0.9393939393939394 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79229170\n",
      "\u001b[31mIch lag um -0.14710224 daneben\u001b[0m\n",
      "neues weight1: -0.35914568 und neues bias1: -0.54998799\n",
      "neues weight2: -3.30583493 und neues bias2: -2.50339302\n",
      "Wir haben 9.326033468832199 und 0.9696969696969697 als Trainings Daten\n",
      "Ich sage y vorraus: 0.79973070\n",
      "\u001b[31mIch lag um -0.16996627 daneben\u001b[0m\n",
      "neues weight1: -0.35931751 und neues bias1: -0.55000642\n",
      "neues weight2: -3.30923147 und neues bias2: -2.49999370\n",
      "Wir haben 10.0 und 1.0 als Trainings Daten\n",
      "Ich sage y vorraus: 0.80757081\n",
      "\u001b[31mIch lag um -0.19242919 daneben\u001b[0m\n",
      "neues weight1: -0.35944579 und neues bias1: -0.55001925\n",
      "neues weight2: -3.31307812 und neues bias2: -2.49614511\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(\"\\n\")\n",
    "        print(f\"Ich trainiere in Epoche {epoch+1}\")\n",
    "    package = zip(x_inputs, y_train_inputs)\n",
    "    \n",
    "    for x, y_true in package:\n",
    "\n",
    "        hidden = np.tanh(weight1 * x + bias1)\n",
    "        y_pred = weight2 * hidden + bias2\n",
    "\n",
    "        diff = y_pred - y_true\n",
    "\n",
    "        # hidden layer gradients (chain rule)\n",
    "        d_hidden = (1 - hidden**2) * weight2 * 2.0 * diff   # tanh' = 1 - tanh^2\n",
    "        grad_w1 = d_hidden * x\n",
    "        grad_b1 = d_hidden\n",
    "        \n",
    "        # output layer gradients\n",
    "        grad_w2 = 2.0 * diff * hidden\n",
    "        grad_b2 = 2.0 * diff\n",
    "        \n",
    "\n",
    "        # wir passen weight und bias an\n",
    "        weight1 -= learning_rate * grad_w1\n",
    "        bias1   -= learning_rate * grad_b1\n",
    "        weight2 -= learning_rate * grad_w2\n",
    "        bias2   -= learning_rate * grad_b2\n",
    "        \n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Wir haben {x} und {y_true} als Trainings Daten\")\n",
    "            print(f\"Ich sage y vorraus: {y_pred:.8f}\")\n",
    "            print(f\"\\033[31mIch lag um {diff:.8f} daneben\\033[0m\")\n",
    "            print(f\"neues weight1: {weight1:.8f} und neues bias1: {bias1:.8f}\")\n",
    "            print(f\"neues weight2: {weight2:.8f} und neues bias2: {bias2:.8f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a5251d-1713-4b44-b305-ce62b33f11bf",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cf23f1f-bca0-4a44-a952-bd1dc116256e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 3, y_pred = 0.57110938\n",
      "3, 0.5711093810948786\n"
     ]
    }
   ],
   "source": [
    "x_input = 3\n",
    "\n",
    "hidden = np.tanh(weight1 * x_input + bias1)\n",
    "y_model_pred = weight2 * hidden + bias2\n",
    "\n",
    "print(f\"x = {x_input}, y_pred = {y_model_pred:.8f}\")\n",
    "print(f\"{x_input}, {y_model_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4a3116b-874d-474c-9398-34a0d843833f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize(x_inputs + [x_input], y_train_inputs + [y_model_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bde80b54-5584-4e8e-bea8-d529a2d8a43d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'historical_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m plt.plot(\u001b[43mhistorical_loss\u001b[49m, label=\u001b[33m\"\u001b[39m\u001b[33mTraining Loss\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mEpoch\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m plt.ylabel(\u001b[33m\"\u001b[39m\u001b[33mLoss (MSE)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'historical_loss' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(historical_loss, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.title(\"Training Loss over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9e79a-a4e6-4a95-a51f-72d24f5dc40c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dcf259-be43-42f6-803d-356e7ef7d3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
